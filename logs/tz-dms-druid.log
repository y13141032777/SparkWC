[druid] 2019-06-11 09:21:31,649 [main           ] INFO  org.apache.spark.SparkContext  {1} - Running Spark version 2.2.0
   [druid] 2019-06-11 09:21:32,161 [main           ] INFO  org.apache.spark.SparkContext  {1} - Submitted application: trans
   [druid] 2019-06-11 09:21:32,185 [main           ] INFO  g.apache.spark.SecurityManager {1} - Changing view acls to: 于羊,ÓÚÑò
   [druid] 2019-06-11 09:21:32,186 [main           ] INFO  g.apache.spark.SecurityManager {1} - Changing modify acls to: 于羊,ÓÚÑò
   [druid] 2019-06-11 09:21:32,187 [main           ] INFO  g.apache.spark.SecurityManager {1} - Changing view acls groups to: 
   [druid] 2019-06-11 09:21:32,187 [main           ] INFO  g.apache.spark.SecurityManager {1} - Changing modify acls groups to: 
   [druid] 2019-06-11 09:21:32,188 [main           ] INFO  g.apache.spark.SecurityManager {1} - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(于羊, ÓÚÑò); groups with view permissions: Set(); users  with modify permissions: Set(于羊, ÓÚÑò); groups with modify permissions: Set()
   [druid] 2019-06-11 09:21:32,914 [main           ] INFO  org.apache.spark.util.Utils    {1} - Successfully started service 'sparkDriver' on port 63501.
   [druid] 2019-06-11 09:21:32,930 [main           ] INFO  org.apache.spark.SparkEnv      {1} - Registering MapOutputTracker
   [druid] 2019-06-11 09:21:32,943 [main           ] INFO  org.apache.spark.SparkEnv      {1} - Registering BlockManagerMaster
   [druid] 2019-06-11 09:21:32,947 [main           ] INFO  age.BlockManagerMasterEndpoint {1} - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
   [druid] 2019-06-11 09:21:32,947 [main           ] INFO  age.BlockManagerMasterEndpoint {1} - BlockManagerMasterEndpoint up
   [druid] 2019-06-11 09:21:32,981 [main           ] INFO  spark.storage.DiskBlockManager {1} - Created local directory at C:\Users\于羊\AppData\Local\Temp\blockmgr-df510867-e106-4dad-b6b1-96aaf41c0281
   [druid] 2019-06-11 09:21:32,996 [main           ] INFO  ark.storage.memory.MemoryStore {1} - MemoryStore started with capacity 1989.6 MB
   [druid] 2019-06-11 09:21:33,035 [main           ] INFO  org.apache.spark.SparkEnv      {1} - Registering OutputCommitCoordinator
   [druid] 2019-06-11 09:21:33,103 [main           ] INFO  g.spark_project.jetty.util.log {1} - Logging initialized @2319ms
   [druid] 2019-06-11 09:21:33,159 [main           ] INFO  rk_project.jetty.server.Server {1} - jetty-9.3.z-SNAPSHOT
   [druid] 2019-06-11 09:21:33,172 [main           ] INFO  rk_project.jetty.server.Server {1} - Started @2390ms
   [druid] 2019-06-11 09:21:33,188 [main           ] INFO  jetty.server.AbstractConnector {1} - Started ServerConnector@58e6d4b8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
   [druid] 2019-06-11 09:21:33,191 [main           ] INFO  org.apache.spark.util.Utils    {1} - Successfully started service 'SparkUI' on port 4040.
   [druid] 2019-06-11 09:21:33,210 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@3383649e{/jobs,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,211 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@1cd201a8{/jobs/json,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,211 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@1992eaf4{/jobs/job,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,212 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@3f28bd56{/jobs/job/json,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,213 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@19fe4644{/stages,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,213 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@5be067de{/stages/json,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,213 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@18245eb0{/stages/stage,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,214 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@72a85671{/stages/stage/json,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,215 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@18f20260{/stages/pool,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,215 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@7a48e6e2{/stages/pool/json,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,216 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@3a94964{/storage,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,216 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@6d0b5baf{/storage/json,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,216 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@2a3591c5{/storage/rdd,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,217 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@346a361{/storage/rdd/json,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,217 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@1643d68f{/environment,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,218 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@2e029d61{/environment/json,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,218 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@4052274f{/executors,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,219 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@297ea53a{/executors/json,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,219 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@5bf22f18{/executors/threadDump,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,219 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@7a7471ce{/executors/threadDump/json,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,225 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@62e70ea3{/static,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,226 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@29ef6856{/,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,227 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@3faf2e7d{/api,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,227 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@eb6449b{/jobs/job/kill,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,228 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@180e6ac4{/stages/stage/kill,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,229 [main           ] INFO  org.apache.spark.ui.SparkUI    {1} - Bound SparkUI to 0.0.0.0, and started at http://192.168.91.1:4040
   [druid] 2019-06-11 09:21:33,304 [main           ] INFO  apache.spark.executor.Executor {1} - Starting executor ID driver on host localhost
   [druid] 2019-06-11 09:21:33,329 [main           ] INFO  org.apache.spark.util.Utils    {1} - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63518.
   [druid] 2019-06-11 09:21:33,329 [main           ] INFO  etty.NettyBlockTransferService {1} - Server created on 192.168.91.1:63518
   [druid] 2019-06-11 09:21:33,331 [main           ] INFO  che.spark.storage.BlockManager {1} - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
   [druid] 2019-06-11 09:21:33,333 [main           ] INFO  ark.storage.BlockManagerMaster {1} - Registering BlockManager BlockManagerId(driver, 192.168.91.1, 63518, None)
   [druid] 2019-06-11 09:21:33,336 [er-event-loop-2] INFO  age.BlockManagerMasterEndpoint {1} - Registering block manager 192.168.91.1:63518 with 1989.6 MB RAM, BlockManagerId(driver, 192.168.91.1, 63518, None)
   [druid] 2019-06-11 09:21:33,338 [main           ] INFO  ark.storage.BlockManagerMaster {1} - Registered BlockManager BlockManagerId(driver, 192.168.91.1, 63518, None)
   [druid] 2019-06-11 09:21:33,338 [main           ] INFO  che.spark.storage.BlockManager {1} - Initialized BlockManager: BlockManagerId(driver, 192.168.91.1, 63518, None)
   [druid] 2019-06-11 09:21:33,500 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@9257031{/metrics/json,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,819 [streaming-start] INFO  ming.scheduler.ReceiverTracker {1} - Starting 1 receivers
   [druid] 2019-06-11 09:21:33,821 [streaming-start] INFO  ming.scheduler.ReceiverTracker {1} - ReceiverTracker started
   [druid] 2019-06-11 09:21:33,828 [streaming-start] INFO  ing.dstream.SocketInputDStream {1} - Slide time = 5000 ms
   [druid] 2019-06-11 09:21:33,829 [streaming-start] INFO  ing.dstream.SocketInputDStream {1} - Storage level = Serialized 1x Replicated
   [druid] 2019-06-11 09:21:33,829 [streaming-start] INFO  ing.dstream.SocketInputDStream {1} - Checkpoint interval = null
   [druid] 2019-06-11 09:21:33,830 [streaming-start] INFO  ing.dstream.SocketInputDStream {1} - Remember interval = 5000 ms
   [druid] 2019-06-11 09:21:33,831 [streaming-start] INFO  ing.dstream.SocketInputDStream {1} - Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@1cf575d
   [druid] 2019-06-11 09:21:33,831 [streaming-start] INFO  treaming.dstream.MappedDStream {1} - Slide time = 5000 ms
   [druid] 2019-06-11 09:21:33,831 [streaming-start] INFO  treaming.dstream.MappedDStream {1} - Storage level = Serialized 1x Replicated
   [druid] 2019-06-11 09:21:33,831 [streaming-start] INFO  treaming.dstream.MappedDStream {1} - Checkpoint interval = null
   [druid] 2019-06-11 09:21:33,831 [streaming-start] INFO  treaming.dstream.MappedDStream {1} - Remember interval = 5000 ms
   [druid] 2019-06-11 09:21:33,831 [streaming-start] INFO  treaming.dstream.MappedDStream {1} - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@2ba4013c
   [druid] 2019-06-11 09:21:33,831 [streaming-start] INFO  ing.dstream.TransformedDStream {1} - Slide time = 5000 ms
   [druid] 2019-06-11 09:21:33,832 [streaming-start] INFO  ing.dstream.TransformedDStream {1} - Storage level = Serialized 1x Replicated
   [druid] 2019-06-11 09:21:33,832 [streaming-start] INFO  ing.dstream.TransformedDStream {1} - Checkpoint interval = null
   [druid] 2019-06-11 09:21:33,832 [streaming-start] INFO  ing.dstream.TransformedDStream {1} - Remember interval = 5000 ms
   [druid] 2019-06-11 09:21:33,832 [streaming-start] INFO  ing.dstream.TransformedDStream {1} - Initialized and validated org.apache.spark.streaming.dstream.TransformedDStream@4a90d051
   [druid] 2019-06-11 09:21:33,832 [streaming-start] INFO  reaming.dstream.ForEachDStream {1} - Slide time = 5000 ms
   [druid] 2019-06-11 09:21:33,832 [streaming-start] INFO  reaming.dstream.ForEachDStream {1} - Storage level = Serialized 1x Replicated
   [druid] 2019-06-11 09:21:33,832 [streaming-start] INFO  reaming.dstream.ForEachDStream {1} - Checkpoint interval = null
   [druid] 2019-06-11 09:21:33,832 [streaming-start] INFO  reaming.dstream.ForEachDStream {1} - Remember interval = 5000 ms
   [druid] 2019-06-11 09:21:33,832 [streaming-start] INFO  reaming.dstream.ForEachDStream {1} - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@61529349
   [druid] 2019-06-11 09:21:33,883 [er-event-loop-5] INFO  ming.scheduler.ReceiverTracker {1} - Receiver 0 started
   [druid] 2019-06-11 09:21:33,888 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 0 (start at Transform.scala:37) with 1 output partitions
   [druid] 2019-06-11 09:21:33,889 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 0 (start at Transform.scala:37)
   [druid] 2019-06-11 09:21:33,889 [streaming-start] INFO  .streaming.util.RecurringTimer {1} - Started timer for JobGenerator at time 1560216095000
   [druid] 2019-06-11 09:21:33,889 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List()
   [druid] 2019-06-11 09:21:33,890 [streaming-start] INFO  reaming.scheduler.JobGenerator {1} - Started JobGenerator at 1560216095000 ms
   [druid] 2019-06-11 09:21:33,890 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:21:33,890 [streaming-start] INFO  reaming.scheduler.JobScheduler {1} - Started JobScheduler
   [druid] 2019-06-11 09:21:33,894 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@5cd61783{/streaming,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,895 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@57aa341b{/streaming/json,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,895 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@769d513{/streaming/batch,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,896 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@588cd519{/streaming/batch/json,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,897 [main           ] INFO  .server.handler.ContextHandler {1} - Started o.s.j.s.ServletContextHandler@6be25526{/static/streaming,null,AVAILABLE,@Spark}
   [druid] 2019-06-11 09:21:33,897 [main           ] INFO  ark.streaming.StreamingContext {1} - StreamingContext started
   [druid] 2019-06-11 09:21:33,898 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[1] at makeRDD at ReceiverTracker.scala:620), which has no missing parents
   [druid] 2019-06-11 09:21:33,996 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_0 stored as values in memory (estimated size 50.7 KB, free 1989.6 MB)
   [druid] 2019-06-11 09:21:34,062 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.9 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:34,066 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_0_piece0 in memory on 192.168.91.1:63518 (size: 16.9 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:34,068 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:34,086 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[1] at makeRDD at ReceiverTracker.scala:620) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:21:34,087 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 0.0 with 1 tasks
   [druid] 2019-06-11 09:21:34,122 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5413 bytes)
   [druid] 2019-06-11 09:21:34,133 [rker for task 0] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 0.0 (TID 0)
   [druid] 2019-06-11 09:21:34,188 [rker for task 0] INFO  .streaming.util.RecurringTimer {1} - Started timer for BlockGenerator at time 1560216094200
   [druid] 2019-06-11 09:21:34,189 [rker for task 0] INFO  eaming.receiver.BlockGenerator {1} - Started BlockGenerator
   [druid] 2019-06-11 09:21:34,189 [Thread-12      ] INFO  eaming.receiver.BlockGenerator {1} - Started block pushing thread
   [druid] 2019-06-11 09:21:34,195 [er-event-loop-3] INFO  ming.scheduler.ReceiverTracker {1} - Registered receiver for stream 0 from 192.168.91.1:63501
   [druid] 2019-06-11 09:21:34,195 [rker for task 0] INFO  eceiver.ReceiverSupervisorImpl {1} - Starting receiver 0
   [druid] 2019-06-11 09:21:34,196 [rker for task 0] INFO  reaming.dstream.SocketReceiver {1} - Connecting to hadoop01:8989
   [druid] 2019-06-11 09:21:34,200 [rker for task 0] INFO  reaming.dstream.SocketReceiver {1} - Connected to hadoop01:8989
   [druid] 2019-06-11 09:21:34,201 [rker for task 0] INFO  eceiver.ReceiverSupervisorImpl {1} - Called receiver 0 onStart
   [druid] 2019-06-11 09:21:34,201 [rker for task 0] INFO  eceiver.ReceiverSupervisorImpl {1} - Waiting for receiver to be stopped
   [druid] 2019-06-11 09:21:35,056 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216095000 ms
   [druid] 2019-06-11 09:21:35,059 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216095000 ms.0 from job set of time 1560216095000 ms
   [druid] 2019-06-11 09:21:35,090 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:21:35,092 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:21:35,093 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 3 (map at Transform.scala:20)
   [druid] 2019-06-11 09:21:35,093 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 1 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:21:35,093 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 3 (print at Transform.scala:36)
   [druid] 2019-06-11 09:21:35,093 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1, ShuffleMapStage 2)
   [druid] 2019-06-11 09:21:35,094 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1)
   [druid] 2019-06-11 09:21:35,095 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:21:35,099 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_1 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:35,103 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1308.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:21:35,104 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_1_piece0 in memory on 192.168.91.1:63518 (size: 1308.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:35,104 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:35,107 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:21:35,107 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1.0 with 2 tasks
   [druid] 2019-06-11 09:21:35,110 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:21:35,111 [rker for task 1] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1.0 (TID 1)
   [druid] 2019-06-11 09:21:35,156 [rker for task 1] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1.0 (TID 1). 896 bytes result sent to driver
   [druid] 2019-06-11 09:21:35,157 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:21:35,158 [rker for task 2] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1.0 (TID 2)
   [druid] 2019-06-11 09:21:35,164 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1.0 (TID 1) in 55 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:21:35,176 [rker for task 2] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1.0 (TID 2). 853 bytes result sent to driver
   [druid] 2019-06-11 09:21:35,177 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1.0 (TID 2) in 20 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:21:35,178 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:35,179 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1 (parallelize at Transform.scala:16) finished in 0.071 s
   [druid] 2019-06-11 09:21:35,179 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:21:35,179 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:21:35,180 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 3)
   [druid] 2019-06-11 09:21:35,180 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:21:35,183 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 3 (MapPartitionsRDD[8] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:21:35,186 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:35,189 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:35,190 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_2_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:35,191 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:35,191 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:21:35,191 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 3.0 with 1 tasks
   [druid] 2019-06-11 09:21:35,194 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:21:35,194 [rker for task 3] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 3.0 (TID 3)
   [druid] 2019-06-11 09:21:35,206 [rker for task 3] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:21:35,207 [rker for task 3] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 4 ms
   [druid] 2019-06-11 09:21:35,210 [rker for task 3] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:21:35,210 [rker for task 3] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:21:35,232 [rker for task 3] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 3.0 (TID 3). 1095 bytes result sent to driver
   [druid] 2019-06-11 09:21:35,234 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 3.0 (TID 3) in 42 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:21:35,234 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 3.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:35,234 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 3 (print at Transform.scala:36) finished in 0.042 s
   [druid] 2019-06-11 09:21:35,239 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 1 finished: print at Transform.scala:36, took 0.149756 s
   [druid] 2019-06-11 09:21:35,246 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:21:35,248 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 1 is 160 bytes
   [druid] 2019-06-11 09:21:35,249 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 0 is 83 bytes
   [druid] 2019-06-11 09:21:35,250 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 2 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:21:35,250 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 6 (print at Transform.scala:36)
   [druid] 2019-06-11 09:21:35,250 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 5, ShuffleMapStage 4)
   [druid] 2019-06-11 09:21:35,250 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:21:35,250 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 6 (MapPartitionsRDD[8] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:21:35,253 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_3 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:35,255 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:35,255 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_3_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:35,256 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:35,257 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[8] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:21:35,257 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 6.0 with 1 tasks
   [druid] 2019-06-11 09:21:35,258 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 6.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:21:35,258 [rker for task 4] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 6.0 (TID 4)
   [druid] 2019-06-11 09:21:35,261 [rker for task 4] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:21:35,261 [rker for task 4] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:35,261 [rker for task 4] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:21:35,261 [rker for task 4] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:35,264 [rker for task 4] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 6.0 (TID 4). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:21:35,266 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 6.0 (TID 4) in 8 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:21:35,266 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 6.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:35,267 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 6 (print at Transform.scala:36) finished in 0.010 s
   [druid] 2019-06-11 09:21:35,269 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 2 finished: print at Transform.scala:36, took 0.021998 s
   [druid] 2019-06-11 09:21:35,274 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216095000 ms.0 from job set of time 1560216095000 ms
   [druid] 2019-06-11 09:21:35,276 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.273 s for time 1560216095000 ms (execution: 0.215 s)
   [druid] 2019-06-11 09:21:35,281 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 
   [druid] 2019-06-11 09:21:35,285 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 
   [druid] 2019-06-11 09:21:40,015 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216100000 ms
   [druid] 2019-06-11 09:21:40,016 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216100000 ms.0 from job set of time 1560216100000 ms
   [druid] 2019-06-11 09:21:40,020 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:21:40,021 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 10 (map at Transform.scala:20)
   [druid] 2019-06-11 09:21:40,021 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:21:40,022 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 3 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:21:40,022 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 9 (print at Transform.scala:36)
   [druid] 2019-06-11 09:21:40,022 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 7, ShuffleMapStage 8)
   [druid] 2019-06-11 09:21:40,022 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 8)
   [druid] 2019-06-11 09:21:40,022 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 8 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:21:40,023 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_4 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:40,025 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:21:40,026 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_4_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:40,027 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:40,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 8 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:21:40,027 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 8.0 with 2 tasks
   [druid] 2019-06-11 09:21:40,028 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 8.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:21:40,028 [rker for task 5] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 8.0 (TID 5)
   [druid] 2019-06-11 09:21:40,041 [rker for task 5] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 8.0 (TID 5). 853 bytes result sent to driver
   [druid] 2019-06-11 09:21:40,042 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 8.0 (TID 6, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:21:40,042 [rker for task 6] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 8.0 (TID 6)
   [druid] 2019-06-11 09:21:40,042 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 8.0 (TID 5) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:21:40,056 [rker for task 6] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 8.0 (TID 6). 853 bytes result sent to driver
   [druid] 2019-06-11 09:21:40,057 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 8.0 (TID 6) in 15 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:21:40,057 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 8.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:40,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 8 (parallelize at Transform.scala:16) finished in 0.031 s
   [druid] 2019-06-11 09:21:40,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:21:40,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:21:40,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 9)
   [druid] 2019-06-11 09:21:40,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:21:40,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 9 (MapPartitionsRDD[15] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:21:40,061 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_5 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:40,064 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:40,065 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_5_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:40,066 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:40,066 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[15] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:21:40,066 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 9.0 with 1 tasks
   [druid] 2019-06-11 09:21:40,067 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 9.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:21:40,067 [rker for task 7] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 9.0 (TID 7)
   [druid] 2019-06-11 09:21:40,071 [rker for task 7] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:21:40,071 [rker for task 7] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:40,071 [rker for task 7] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:21:40,071 [rker for task 7] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:40,073 [rker for task 7] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 9.0 (TID 7). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:21:40,074 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 9.0 (TID 7) in 7 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:21:40,074 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 9.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:40,075 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 9 (print at Transform.scala:36) finished in 0.007 s
   [druid] 2019-06-11 09:21:40,075 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 3 finished: print at Transform.scala:36, took 0.054677 s
   [druid] 2019-06-11 09:21:40,080 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:21:40,081 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 2 is 83 bytes
   [druid] 2019-06-11 09:21:40,082 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 3 is 160 bytes
   [druid] 2019-06-11 09:21:40,082 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 4 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:21:40,082 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 12 (print at Transform.scala:36)
   [druid] 2019-06-11 09:21:40,082 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 10, ShuffleMapStage 11)
   [druid] 2019-06-11 09:21:40,083 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:21:40,083 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 12 (MapPartitionsRDD[15] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:21:40,084 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:40,086 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:40,087 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_6_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:40,088 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:40,088 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[15] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:21:40,088 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 12.0 with 1 tasks
   [druid] 2019-06-11 09:21:40,089 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 12.0 (TID 8, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:21:40,089 [rker for task 8] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 12.0 (TID 8)
   [druid] 2019-06-11 09:21:40,092 [rker for task 8] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:21:40,092 [rker for task 8] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:40,092 [rker for task 8] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:21:40,092 [rker for task 8] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:40,095 [rker for task 8] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 12.0 (TID 8). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:21:40,096 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 12.0 (TID 8) in 6 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:21:40,096 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 12.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:40,096 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 12 (print at Transform.scala:36) finished in 0.008 s
   [druid] 2019-06-11 09:21:40,097 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 4 finished: print at Transform.scala:36, took 0.016436 s
   [druid] 2019-06-11 09:21:40,097 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216100000 ms.0 from job set of time 1560216100000 ms
   [druid] 2019-06-11 09:21:40,097 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.097 s for time 1560216100000 ms (execution: 0.081 s)
   [druid] 2019-06-11 09:21:40,097 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 8 from persistence list
   [druid] 2019-06-11 09:21:40,104 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 3 from persistence list
   [druid] 2019-06-11 09:21:40,105 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 2 from persistence list
   [druid] 2019-06-11 09:21:40,105 [c-thread-pool-0] INFO  che.spark.storage.BlockManager {1} - Removing RDD 8
   [druid] 2019-06-11 09:21:40,106 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[2] at socketTextStream at Transform.scala:18 of time 1560216100000 ms
   [druid] 2019-06-11 09:21:40,106 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 3
   [druid] 2019-06-11 09:21:40,107 [c-thread-pool-2] INFO  che.spark.storage.BlockManager {1} - Removing RDD 2
   [druid] 2019-06-11 09:21:40,107 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 
   [druid] 2019-06-11 09:21:40,107 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 
   [druid] 2019-06-11 09:21:45,012 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216105000 ms
   [druid] 2019-06-11 09:21:45,013 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216105000 ms.0 from job set of time 1560216105000 ms
   [druid] 2019-06-11 09:21:45,017 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:21:45,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:21:45,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 17 (map at Transform.scala:20)
   [druid] 2019-06-11 09:21:45,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 5 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:21:45,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 15 (print at Transform.scala:36)
   [druid] 2019-06-11 09:21:45,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 13, ShuffleMapStage 14)
   [druid] 2019-06-11 09:21:45,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 13)
   [druid] 2019-06-11 09:21:45,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 13 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:21:45,021 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_7 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:45,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_7_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:21:45,034 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_7_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:45,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 13 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:21:45,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 13.0 with 2 tasks
   [druid] 2019-06-11 09:21:45,036 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 13.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:21:45,036 [rker for task 9] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 13.0 (TID 9)
   [druid] 2019-06-11 09:21:45,044 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_1_piece0 on 192.168.91.1:63518 in memory (size: 1308.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:45,048 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 0
   [druid] 2019-06-11 09:21:45,049 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_5_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:45,051 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_4_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:45,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 1
   [druid] 2019-06-11 09:21:45,052 [rker for task 9] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 13.0 (TID 9). 896 bytes result sent to driver
   [druid] 2019-06-11 09:21:45,053 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 13.0 (TID 10, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:21:45,053 [ker for task 10] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 13.0 (TID 10)
   [druid] 2019-06-11 09:21:45,053 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 13.0 (TID 9) in 17 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:21:45,053 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_6_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:45,055 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_2_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:45,057 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_3_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:45,066 [ker for task 10] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 13.0 (TID 10). 810 bytes result sent to driver
   [druid] 2019-06-11 09:21:45,067 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 13.0 (TID 10) in 14 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:21:45,067 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 13.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:45,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 13 (parallelize at Transform.scala:16) finished in 0.032 s
   [druid] 2019-06-11 09:21:45,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:21:45,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:21:45,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 15)
   [druid] 2019-06-11 09:21:45,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:21:45,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 15 (MapPartitionsRDD[22] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:21:45,070 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_8 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:45,071 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:45,072 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_8_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:45,072 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:45,073 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[22] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:21:45,073 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 15.0 with 1 tasks
   [druid] 2019-06-11 09:21:45,073 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 15.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:21:45,073 [ker for task 11] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 15.0 (TID 11)
   [druid] 2019-06-11 09:21:45,075 [ker for task 11] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:21:45,075 [ker for task 11] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:45,075 [ker for task 11] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:21:45,075 [ker for task 11] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:45,077 [ker for task 11] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 15.0 (TID 11). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:21:45,078 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 15.0 (TID 11) in 5 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:21:45,078 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 15.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:45,079 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 15 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:21:45,079 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 5 finished: print at Transform.scala:36, took 0.061433 s
   [druid] 2019-06-11 09:21:45,084 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:21:45,085 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 5 is 160 bytes
   [druid] 2019-06-11 09:21:45,085 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 4 is 83 bytes
   [druid] 2019-06-11 09:21:45,086 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 6 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:21:45,086 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 18 (print at Transform.scala:36)
   [druid] 2019-06-11 09:21:45,086 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 16, ShuffleMapStage 17)
   [druid] 2019-06-11 09:21:45,086 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:21:45,086 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 18 (MapPartitionsRDD[22] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:21:45,088 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_9 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:45,090 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:45,092 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_9_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:45,092 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 9 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:45,093 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[22] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:21:45,093 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 18.0 with 1 tasks
   [druid] 2019-06-11 09:21:45,094 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 18.0 (TID 12, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:21:45,094 [ker for task 12] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 18.0 (TID 12)
   [druid] 2019-06-11 09:21:45,096 [ker for task 12] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:21:45,096 [ker for task 12] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:45,096 [ker for task 12] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:21:45,096 [ker for task 12] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:45,098 [ker for task 12] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 18.0 (TID 12). 966 bytes result sent to driver
   [druid] 2019-06-11 09:21:45,099 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 18.0 (TID 12) in 5 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:21:45,099 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 18.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:45,099 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 18 (print at Transform.scala:36) finished in 0.006 s
   [druid] 2019-06-11 09:21:45,100 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 6 finished: print at Transform.scala:36, took 0.015199 s
   [druid] 2019-06-11 09:21:45,100 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216105000 ms.0 from job set of time 1560216105000 ms
   [druid] 2019-06-11 09:21:45,101 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 15 from persistence list
   [druid] 2019-06-11 09:21:45,101 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.100 s for time 1560216105000 ms (execution: 0.087 s)
   [druid] 2019-06-11 09:21:45,101 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 15
   [druid] 2019-06-11 09:21:45,101 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 10 from persistence list
   [druid] 2019-06-11 09:21:45,101 [c-thread-pool-8] INFO  che.spark.storage.BlockManager {1} - Removing RDD 10
   [druid] 2019-06-11 09:21:45,102 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 9 from persistence list
   [druid] 2019-06-11 09:21:45,102 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 9
   [druid] 2019-06-11 09:21:45,102 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[9] at socketTextStream at Transform.scala:18 of time 1560216105000 ms
   [druid] 2019-06-11 09:21:45,102 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216095000 ms
   [druid] 2019-06-11 09:21:45,102 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216095000 ms
   [druid] 2019-06-11 09:21:50,011 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216110000 ms
   [druid] 2019-06-11 09:21:50,011 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216110000 ms.0 from job set of time 1560216110000 ms
   [druid] 2019-06-11 09:21:50,014 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:21:50,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 24 (map at Transform.scala:20)
   [druid] 2019-06-11 09:21:50,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:21:50,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 7 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:21:50,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 21 (print at Transform.scala:36)
   [druid] 2019-06-11 09:21:50,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 19, ShuffleMapStage 20)
   [druid] 2019-06-11 09:21:50,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 20)
   [druid] 2019-06-11 09:21:50,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 20 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:21:50,017 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_10 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:50,020 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_10_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:21:50,020 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_10_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:50,021 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 10 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:50,021 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 20 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:21:50,021 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 20.0 with 2 tasks
   [druid] 2019-06-11 09:21:50,022 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 20.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:21:50,022 [ker for task 13] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 20.0 (TID 13)
   [druid] 2019-06-11 09:21:50,033 [ker for task 13] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 20.0 (TID 13). 810 bytes result sent to driver
   [druid] 2019-06-11 09:21:50,034 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 20.0 (TID 14, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:21:50,034 [ker for task 14] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 20.0 (TID 14)
   [druid] 2019-06-11 09:21:50,034 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 20.0 (TID 13) in 12 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:21:50,050 [ker for task 14] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 20.0 (TID 14). 853 bytes result sent to driver
   [druid] 2019-06-11 09:21:50,051 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 20.0 (TID 14) in 16 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:21:50,051 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 20.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:50,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 20 (parallelize at Transform.scala:16) finished in 0.029 s
   [druid] 2019-06-11 09:21:50,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:21:50,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:21:50,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 21)
   [druid] 2019-06-11 09:21:50,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:21:50,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 21 (MapPartitionsRDD[29] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:21:50,054 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_11 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:50,056 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:50,058 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_11_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:50,058 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 11 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:50,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[29] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:21:50,059 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 21.0 with 1 tasks
   [druid] 2019-06-11 09:21:50,060 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 21.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:21:50,060 [ker for task 15] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 21.0 (TID 15)
   [druid] 2019-06-11 09:21:50,062 [ker for task 15] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:21:50,062 [ker for task 15] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:50,062 [ker for task 15] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:21:50,062 [ker for task 15] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:50,063 [ker for task 15] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 21.0 (TID 15). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:21:50,064 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 21.0 (TID 15) in 5 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:21:50,064 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 21.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:50,065 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 21 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:21:50,065 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 7 finished: print at Transform.scala:36, took 0.050792 s
   [druid] 2019-06-11 09:21:50,070 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:21:50,070 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 6 is 83 bytes
   [druid] 2019-06-11 09:21:50,071 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 7 is 160 bytes
   [druid] 2019-06-11 09:21:50,071 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 8 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:21:50,071 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 24 (print at Transform.scala:36)
   [druid] 2019-06-11 09:21:50,071 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 22, ShuffleMapStage 23)
   [druid] 2019-06-11 09:21:50,072 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:21:50,072 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 24 (MapPartitionsRDD[29] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:21:50,073 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_12 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:50,076 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:50,077 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_12_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:50,077 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 12 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:50,078 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[29] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:21:50,078 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 24.0 with 1 tasks
   [druid] 2019-06-11 09:21:50,079 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 24.0 (TID 16, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:21:50,079 [ker for task 16] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 24.0 (TID 16)
   [druid] 2019-06-11 09:21:50,082 [ker for task 16] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:21:50,082 [ker for task 16] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:50,082 [ker for task 16] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:21:50,082 [ker for task 16] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:50,084 [ker for task 16] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 24.0 (TID 16). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:21:50,084 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 24.0 (TID 16) in 5 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:21:50,084 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 24.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:50,085 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 24 (print at Transform.scala:36) finished in 0.006 s
   [druid] 2019-06-11 09:21:50,085 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 8 finished: print at Transform.scala:36, took 0.015394 s
   [druid] 2019-06-11 09:21:50,086 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216110000 ms.0 from job set of time 1560216110000 ms
   [druid] 2019-06-11 09:21:50,086 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.086 s for time 1560216110000 ms (execution: 0.075 s)
   [druid] 2019-06-11 09:21:50,086 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 22 from persistence list
   [druid] 2019-06-11 09:21:50,086 [c-thread-pool-8] INFO  che.spark.storage.BlockManager {1} - Removing RDD 22
   [druid] 2019-06-11 09:21:50,086 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 17 from persistence list
   [druid] 2019-06-11 09:21:50,087 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 17
   [druid] 2019-06-11 09:21:50,087 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 16 from persistence list
   [druid] 2019-06-11 09:21:50,087 [c-thread-pool-8] INFO  che.spark.storage.BlockManager {1} - Removing RDD 16
   [druid] 2019-06-11 09:21:50,087 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[16] at socketTextStream at Transform.scala:18 of time 1560216110000 ms
   [druid] 2019-06-11 09:21:50,087 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216100000 ms
   [druid] 2019-06-11 09:21:50,087 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216100000 ms
   [druid] 2019-06-11 09:21:55,011 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216115000 ms
   [druid] 2019-06-11 09:21:55,011 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216115000 ms.0 from job set of time 1560216115000 ms
   [druid] 2019-06-11 09:21:55,015 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:21:55,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 31 (map at Transform.scala:20)
   [druid] 2019-06-11 09:21:55,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:21:55,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 9 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:21:55,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 27 (print at Transform.scala:36)
   [druid] 2019-06-11 09:21:55,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 25, ShuffleMapStage 26)
   [druid] 2019-06-11 09:21:55,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 26)
   [druid] 2019-06-11 09:21:55,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 26 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:21:55,018 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_13 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:55,020 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_13_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:21:55,021 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_13_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:55,021 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 13 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:55,022 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 26 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:21:55,022 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 26.0 with 2 tasks
   [druid] 2019-06-11 09:21:55,023 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 26.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:21:55,023 [ker for task 17] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 26.0 (TID 17)
   [druid] 2019-06-11 09:21:55,033 [ker for task 17] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 26.0 (TID 17). 810 bytes result sent to driver
   [druid] 2019-06-11 09:21:55,034 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 26.0 (TID 18, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:21:55,034 [ker for task 18] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 26.0 (TID 18)
   [druid] 2019-06-11 09:21:55,034 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 26.0 (TID 17) in 12 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:21:55,046 [ker for task 18] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 26.0 (TID 18). 853 bytes result sent to driver
   [druid] 2019-06-11 09:21:55,047 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 26.0 (TID 18) in 12 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:21:55,047 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 26.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:55,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 26 (parallelize at Transform.scala:16) finished in 0.025 s
   [druid] 2019-06-11 09:21:55,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:21:55,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:21:55,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 27)
   [druid] 2019-06-11 09:21:55,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:21:55,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 27 (MapPartitionsRDD[36] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:21:55,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_14 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:55,052 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:55,053 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_14_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:55,053 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 14 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:55,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[36] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:21:55,054 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 27.0 with 1 tasks
   [druid] 2019-06-11 09:21:55,054 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 27.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:21:55,054 [ker for task 19] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 27.0 (TID 19)
   [druid] 2019-06-11 09:21:55,056 [ker for task 19] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:21:55,056 [ker for task 19] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:55,056 [ker for task 19] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:21:55,056 [ker for task 19] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:55,058 [ker for task 19] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 27.0 (TID 19). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:21:55,058 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 27.0 (TID 19) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:21:55,058 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 27.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:55,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 27 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:21:55,058 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 9 finished: print at Transform.scala:36, took 0.043639 s
   [druid] 2019-06-11 09:21:55,062 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:21:55,063 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 8 is 83 bytes
   [druid] 2019-06-11 09:21:55,063 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 9 is 160 bytes
   [druid] 2019-06-11 09:21:55,063 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 10 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:21:55,063 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 30 (print at Transform.scala:36)
   [druid] 2019-06-11 09:21:55,063 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 28, ShuffleMapStage 29)
   [druid] 2019-06-11 09:21:55,063 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:21:55,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 30 (MapPartitionsRDD[36] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:21:55,065 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_15 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:55,067 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:21:55,067 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_15_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:21:55,068 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 15 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:21:55,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[36] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:21:55,069 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 30.0 with 1 tasks
   [druid] 2019-06-11 09:21:55,069 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 30.0 (TID 20, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:21:55,070 [ker for task 20] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 30.0 (TID 20)
   [druid] 2019-06-11 09:21:55,071 [ker for task 20] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:21:55,071 [ker for task 20] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:55,071 [ker for task 20] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:21:55,071 [ker for task 20] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:21:55,073 [ker for task 20] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 30.0 (TID 20). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:21:55,073 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 30.0 (TID 20) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:21:55,073 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 30.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:21:55,074 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 30 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:21:55,074 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 10 finished: print at Transform.scala:36, took 0.012096 s
   [druid] 2019-06-11 09:21:55,075 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216115000 ms.0 from job set of time 1560216115000 ms
   [druid] 2019-06-11 09:21:55,075 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.075 s for time 1560216115000 ms (execution: 0.064 s)
   [druid] 2019-06-11 09:21:55,075 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 29 from persistence list
   [druid] 2019-06-11 09:21:55,075 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 29
   [druid] 2019-06-11 09:21:55,076 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 24 from persistence list
   [druid] 2019-06-11 09:21:55,076 [c-thread-pool-8] INFO  che.spark.storage.BlockManager {1} - Removing RDD 24
   [druid] 2019-06-11 09:21:55,076 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 23 from persistence list
   [druid] 2019-06-11 09:21:55,076 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 23
   [druid] 2019-06-11 09:21:55,077 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[23] at socketTextStream at Transform.scala:18 of time 1560216115000 ms
   [druid] 2019-06-11 09:21:55,077 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216105000 ms
   [druid] 2019-06-11 09:21:55,077 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216105000 ms
   [druid] 2019-06-11 09:22:00,011 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216120000 ms
   [druid] 2019-06-11 09:22:00,011 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216120000 ms.0 from job set of time 1560216120000 ms
   [druid] 2019-06-11 09:22:00,015 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:00,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:22:00,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 38 (map at Transform.scala:20)
   [druid] 2019-06-11 09:22:00,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 11 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:00,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 33 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:00,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 31, ShuffleMapStage 32)
   [druid] 2019-06-11 09:22:00,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 31)
   [druid] 2019-06-11 09:22:00,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 31 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:22:00,018 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_16 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:00,020 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_16_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:22:00,020 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_16_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:00,021 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 16 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:00,021 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 31 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:22:00,021 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 31.0 with 2 tasks
   [druid] 2019-06-11 09:22:00,022 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 31.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:00,022 [ker for task 21] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 31.0 (TID 21)
   [druid] 2019-06-11 09:22:00,041 [ker for task 21] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 31.0 (TID 21). 853 bytes result sent to driver
   [druid] 2019-06-11 09:22:00,042 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 31.0 (TID 22, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:00,042 [ker for task 22] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 31.0 (TID 22)
   [druid] 2019-06-11 09:22:00,042 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 31.0 (TID 21) in 20 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:22:00,053 [ker for task 22] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 31.0 (TID 22). 853 bytes result sent to driver
   [druid] 2019-06-11 09:22:00,054 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 31.0 (TID 22) in 12 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:22:00,054 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 31.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:00,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 31 (parallelize at Transform.scala:16) finished in 0.032 s
   [druid] 2019-06-11 09:22:00,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:22:00,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:22:00,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 33)
   [druid] 2019-06-11 09:22:00,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:22:00,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 33 (MapPartitionsRDD[43] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:00,058 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_17 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:00,059 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:00,060 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_17_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:00,060 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 17 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:00,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[43] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:22:00,060 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 33.0 with 1 tasks
   [druid] 2019-06-11 09:22:00,061 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 33.0 (TID 23, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:00,061 [ker for task 23] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 33.0 (TID 23)
   [druid] 2019-06-11 09:22:00,063 [ker for task 23] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:00,063 [ker for task 23] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:00,063 [ker for task 23] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:00,063 [ker for task 23] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:00,064 [ker for task 23] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 33.0 (TID 23). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:22:00,065 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 33.0 (TID 23) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:00,065 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 33.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:00,065 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 33 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:22:00,065 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 11 finished: print at Transform.scala:36, took 0.050326 s
   [druid] 2019-06-11 09:22:00,069 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:00,070 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 11 is 160 bytes
   [druid] 2019-06-11 09:22:00,071 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 10 is 83 bytes
   [druid] 2019-06-11 09:22:00,071 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 12 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:00,071 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 36 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:00,071 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 34, ShuffleMapStage 35)
   [druid] 2019-06-11 09:22:00,071 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:22:00,072 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 36 (MapPartitionsRDD[43] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:00,073 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_18 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:00,074 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:00,075 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_18_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:00,075 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 18 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:00,075 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[43] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:22:00,075 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 36.0 with 1 tasks
   [druid] 2019-06-11 09:22:00,076 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 36.0 (TID 24, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:00,076 [ker for task 24] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 36.0 (TID 24)
   [druid] 2019-06-11 09:22:00,078 [ker for task 24] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:00,078 [ker for task 24] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:00,078 [ker for task 24] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:00,078 [ker for task 24] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:00,079 [ker for task 24] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 36.0 (TID 24). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:22:00,080 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 36.0 (TID 24) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:00,080 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 36.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:00,081 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 36 (print at Transform.scala:36) finished in 0.006 s
   [druid] 2019-06-11 09:22:00,081 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 12 finished: print at Transform.scala:36, took 0.011628 s
   [druid] 2019-06-11 09:22:00,082 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216120000 ms.0 from job set of time 1560216120000 ms
   [druid] 2019-06-11 09:22:00,082 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.082 s for time 1560216120000 ms (execution: 0.071 s)
   [druid] 2019-06-11 09:22:00,082 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 36 from persistence list
   [druid] 2019-06-11 09:22:00,082 [c-thread-pool-8] INFO  che.spark.storage.BlockManager {1} - Removing RDD 36
   [druid] 2019-06-11 09:22:00,082 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 31 from persistence list
   [druid] 2019-06-11 09:22:00,082 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 31
   [druid] 2019-06-11 09:22:00,083 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 30 from persistence list
   [druid] 2019-06-11 09:22:00,083 [c-thread-pool-0] INFO  che.spark.storage.BlockManager {1} - Removing RDD 30
   [druid] 2019-06-11 09:22:00,083 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[30] at socketTextStream at Transform.scala:18 of time 1560216120000 ms
   [druid] 2019-06-11 09:22:00,083 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216110000 ms
   [druid] 2019-06-11 09:22:00,083 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216110000 ms
   [druid] 2019-06-11 09:22:05,010 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216125000 ms
   [druid] 2019-06-11 09:22:05,011 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216125000 ms.0 from job set of time 1560216125000 ms
   [druid] 2019-06-11 09:22:05,014 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:05,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:22:05,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 45 (map at Transform.scala:20)
   [druid] 2019-06-11 09:22:05,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 13 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:05,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 39 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:05,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 37, ShuffleMapStage 38)
   [druid] 2019-06-11 09:22:05,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 37)
   [druid] 2019-06-11 09:22:05,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 37 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:22:05,017 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_19 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:05,020 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_19_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:22:05,021 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_19_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:05,022 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 19 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:05,022 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 37 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:22:05,022 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 37.0 with 2 tasks
   [druid] 2019-06-11 09:22:05,023 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 37.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:05,023 [ker for task 25] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 37.0 (TID 25)
   [druid] 2019-06-11 09:22:05,035 [ker for task 25] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 37.0 (TID 25). 853 bytes result sent to driver
   [druid] 2019-06-11 09:22:05,035 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 37.0 (TID 26, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:05,036 [ker for task 26] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 37.0 (TID 26)
   [druid] 2019-06-11 09:22:05,036 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 37.0 (TID 25) in 13 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:22:05,047 [ker for task 26] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 37.0 (TID 26). 896 bytes result sent to driver
   [druid] 2019-06-11 09:22:05,049 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 37.0 (TID 26) in 14 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:22:05,049 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 37.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:05,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 37 (parallelize at Transform.scala:16) finished in 0.027 s
   [druid] 2019-06-11 09:22:05,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:22:05,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:22:05,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 39)
   [druid] 2019-06-11 09:22:05,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:22:05,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 39 (MapPartitionsRDD[50] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:05,053 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_20 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:05,056 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:05,056 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_20_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:05,056 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 20 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:05,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[50] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:22:05,057 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 39.0 with 1 tasks
   [druid] 2019-06-11 09:22:05,057 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 39.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:05,058 [ker for task 27] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 39.0 (TID 27)
   [druid] 2019-06-11 09:22:05,059 [ker for task 27] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:05,059 [ker for task 27] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:05,060 [ker for task 27] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:05,060 [ker for task 27] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:22:05,061 [ker for task 27] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 39.0 (TID 27). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:22:05,061 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 39.0 (TID 27) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:05,061 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 39.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:05,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 39 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:22:05,062 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 13 finished: print at Transform.scala:36, took 0.048328 s
   [druid] 2019-06-11 09:22:05,068 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:05,069 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 13 is 160 bytes
   [druid] 2019-06-11 09:22:05,070 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 12 is 83 bytes
   [druid] 2019-06-11 09:22:05,070 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 14 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:05,070 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 42 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:05,070 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 40, ShuffleMapStage 41)
   [druid] 2019-06-11 09:22:05,070 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:22:05,071 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 42 (MapPartitionsRDD[50] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:05,072 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_21 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:05,075 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_21_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:05,076 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_21_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:05,076 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 21 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:05,077 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[50] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:22:05,078 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 42.0 with 1 tasks
   [druid] 2019-06-11 09:22:05,078 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 42.0 (TID 28, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:05,078 [ker for task 28] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 42.0 (TID 28)
   [druid] 2019-06-11 09:22:05,081 [ker for task 28] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:05,081 [ker for task 28] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:05,081 [ker for task 28] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:05,082 [ker for task 28] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:22:05,083 [ker for task 28] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 42.0 (TID 28). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:22:05,084 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 42.0 (TID 28) in 6 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:05,084 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 42.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:05,084 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 42 (print at Transform.scala:36) finished in 0.006 s
   [druid] 2019-06-11 09:22:05,085 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 14 finished: print at Transform.scala:36, took 0.016085 s
   [druid] 2019-06-11 09:22:05,085 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216125000 ms.0 from job set of time 1560216125000 ms
   [druid] 2019-06-11 09:22:05,086 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.085 s for time 1560216125000 ms (execution: 0.074 s)
   [druid] 2019-06-11 09:22:05,086 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 43 from persistence list
   [druid] 2019-06-11 09:22:05,086 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 43
   [druid] 2019-06-11 09:22:05,086 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 38 from persistence list
   [druid] 2019-06-11 09:22:05,086 [c-thread-pool-0] INFO  che.spark.storage.BlockManager {1} - Removing RDD 38
   [druid] 2019-06-11 09:22:05,087 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 37 from persistence list
   [druid] 2019-06-11 09:22:05,087 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 37
   [druid] 2019-06-11 09:22:05,087 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[37] at socketTextStream at Transform.scala:18 of time 1560216125000 ms
   [druid] 2019-06-11 09:22:05,087 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216115000 ms
   [druid] 2019-06-11 09:22:05,087 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216115000 ms
   [druid] 2019-06-11 09:22:10,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216130000 ms
   [druid] 2019-06-11 09:22:10,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216130000 ms.0 from job set of time 1560216130000 ms
   [druid] 2019-06-11 09:22:10,011 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 52 (map at Transform.scala:20)
   [druid] 2019-06-11 09:22:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:22:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 15 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 45 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 43, ShuffleMapStage 44)
   [druid] 2019-06-11 09:22:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 44)
   [druid] 2019-06-11 09:22:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 44 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:22:10,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_22 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:10,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_22_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:22:10,016 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_22_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:10,016 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 22 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:10,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 44 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:22:10,017 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 44.0 with 2 tasks
   [druid] 2019-06-11 09:22:10,017 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 44.0 (TID 29, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:10,018 [ker for task 29] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 44.0 (TID 29)
   [druid] 2019-06-11 09:22:10,028 [ker for task 29] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 44.0 (TID 29). 810 bytes result sent to driver
   [druid] 2019-06-11 09:22:10,028 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 44.0 (TID 30, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:10,028 [ker for task 30] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 44.0 (TID 30)
   [druid] 2019-06-11 09:22:10,028 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 44.0 (TID 29) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:22:10,041 [ker for task 30] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 44.0 (TID 30). 853 bytes result sent to driver
   [druid] 2019-06-11 09:22:10,041 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 44.0 (TID 30) in 13 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:22:10,041 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 44.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 44 (parallelize at Transform.scala:16) finished in 0.025 s
   [druid] 2019-06-11 09:22:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:22:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:22:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 45)
   [druid] 2019-06-11 09:22:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:22:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 45 (MapPartitionsRDD[57] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:10,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_23 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:10,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_23_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:10,045 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_23_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:10,045 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 23 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:10,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[57] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:22:10,045 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 45.0 with 1 tasks
   [druid] 2019-06-11 09:22:10,046 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 45.0 (TID 31, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:10,046 [ker for task 31] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 45.0 (TID 31)
   [druid] 2019-06-11 09:22:10,047 [ker for task 31] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:10,048 [ker for task 31] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:22:10,048 [ker for task 31] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:10,048 [ker for task 31] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:10,050 [ker for task 31] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 45.0 (TID 31). 966 bytes result sent to driver
   [druid] 2019-06-11 09:22:10,050 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 45.0 (TID 31) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:10,050 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 45.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:10,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 45 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:22:10,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 15 finished: print at Transform.scala:36, took 0.040061 s
   [druid] 2019-06-11 09:22:10,055 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:10,056 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 14 is 83 bytes
   [druid] 2019-06-11 09:22:10,056 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 15 is 160 bytes
   [druid] 2019-06-11 09:22:10,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 16 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:10,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 48 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:10,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 46, ShuffleMapStage 47)
   [druid] 2019-06-11 09:22:10,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:22:10,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 48 (MapPartitionsRDD[57] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:10,058 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_24 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:10,060 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:10,061 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_24_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:10,061 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 24 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:10,061 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[57] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:22:10,061 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 48.0 with 1 tasks
   [druid] 2019-06-11 09:22:10,062 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 48.0 (TID 32, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:10,062 [ker for task 32] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 48.0 (TID 32)
   [druid] 2019-06-11 09:22:10,064 [ker for task 32] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:10,064 [ker for task 32] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:10,065 [ker for task 32] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:10,065 [ker for task 32] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:22:10,066 [ker for task 32] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 48.0 (TID 32). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:22:10,067 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 48.0 (TID 32) in 5 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:10,067 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 48.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:10,067 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 48 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:22:10,067 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 16 finished: print at Transform.scala:36, took 0.011901 s
   [druid] 2019-06-11 09:22:10,068 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216130000 ms.0 from job set of time 1560216130000 ms
   [druid] 2019-06-11 09:22:10,068 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.068 s for time 1560216130000 ms (execution: 0.060 s)
   [druid] 2019-06-11 09:22:10,068 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 50 from persistence list
   [druid] 2019-06-11 09:22:10,068 [c-thread-pool-0] INFO  che.spark.storage.BlockManager {1} - Removing RDD 50
   [druid] 2019-06-11 09:22:10,068 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 45 from persistence list
   [druid] 2019-06-11 09:22:10,069 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 45
   [druid] 2019-06-11 09:22:10,069 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 44 from persistence list
   [druid] 2019-06-11 09:22:10,069 [c-thread-pool-0] INFO  che.spark.storage.BlockManager {1} - Removing RDD 44
   [druid] 2019-06-11 09:22:10,069 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[44] at socketTextStream at Transform.scala:18 of time 1560216130000 ms
   [druid] 2019-06-11 09:22:10,069 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216120000 ms
   [druid] 2019-06-11 09:22:10,069 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216120000 ms
   [druid] 2019-06-11 09:22:15,010 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216135000 ms
   [druid] 2019-06-11 09:22:15,010 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216135000 ms.0 from job set of time 1560216135000 ms
   [druid] 2019-06-11 09:22:15,012 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 59 (map at Transform.scala:20)
   [druid] 2019-06-11 09:22:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:22:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 17 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 51 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 49, ShuffleMapStage 50)
   [druid] 2019-06-11 09:22:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 50)
   [druid] 2019-06-11 09:22:15,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 50 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:22:15,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_25 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:15,018 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_25_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:22:15,019 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_25_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,019 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 25 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:15,020 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 50 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:22:15,020 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 50.0 with 2 tasks
   [druid] 2019-06-11 09:22:15,020 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 50.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:15,020 [ker for task 33] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 50.0 (TID 33)
   [druid] 2019-06-11 09:22:15,029 [ker for task 33] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 50.0 (TID 33). 810 bytes result sent to driver
   [druid] 2019-06-11 09:22:15,030 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 50.0 (TID 34, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:15,030 [ker for task 34] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 50.0 (TID 34)
   [druid] 2019-06-11 09:22:15,030 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 50.0 (TID 33) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:22:15,041 [ker for task 34] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 50.0 (TID 34). 853 bytes result sent to driver
   [druid] 2019-06-11 09:22:15,041 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 50.0 (TID 34) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:22:15,041 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 50.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 50 (parallelize at Transform.scala:16) finished in 0.022 s
   [druid] 2019-06-11 09:22:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:22:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:22:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 51)
   [druid] 2019-06-11 09:22:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:22:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 51 (MapPartitionsRDD[64] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:15,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_26 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:15,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_26_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:15,046 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_26_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:22:15,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 26 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:15,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[64] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:22:15,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 51.0 with 1 tasks
   [druid] 2019-06-11 09:22:15,047 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 51.0 (TID 35, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:15,047 [ker for task 35] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 51.0 (TID 35)
   [druid] 2019-06-11 09:22:15,048 [ker for task 35] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:15,049 [ker for task 35] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:22:15,049 [ker for task 35] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:15,049 [ker for task 35] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:15,050 [ker for task 35] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 51.0 (TID 35). 966 bytes result sent to driver
   [druid] 2019-06-11 09:22:15,051 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 51.0 (TID 35) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:15,051 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 51.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:15,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 51 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:22:15,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 17 finished: print at Transform.scala:36, took 0.038965 s
   [druid] 2019-06-11 09:22:15,055 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:15,055 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 16 is 83 bytes
   [druid] 2019-06-11 09:22:15,056 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 17 is 160 bytes
   [druid] 2019-06-11 09:22:15,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 18 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:15,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 54 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:15,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 52, ShuffleMapStage 53)
   [druid] 2019-06-11 09:22:15,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:22:15,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 54 (MapPartitionsRDD[64] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:15,057 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_27 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:15,070 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_27_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:15,070 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_24_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,071 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_27_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:22:15,071 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 27 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:15,071 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[64] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:22:15,071 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 54.0 with 1 tasks
   [druid] 2019-06-11 09:22:15,071 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_19_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,072 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 54.0 (TID 36, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:15,072 [ker for task 36] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 54.0 (TID 36)
   [druid] 2019-06-11 09:22:15,072 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 6
   [druid] 2019-06-11 09:22:15,073 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_20_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,074 [ker for task 36] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:15,074 [ker for task 36] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:22:15,074 [ker for task 36] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:15,074 [ker for task 36] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:15,074 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_7_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,075 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 8
   [druid] 2019-06-11 09:22:15,076 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 7
   [druid] 2019-06-11 09:22:15,076 [ker for task 36] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 54.0 (TID 36). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:22:15,077 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 54.0 (TID 36) in 5 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:15,077 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 54.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:15,077 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_23_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,077 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 54 (print at Transform.scala:36) finished in 0.006 s
   [druid] 2019-06-11 09:22:15,078 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 18 finished: print at Transform.scala:36, took 0.022912 s
   [druid] 2019-06-11 09:22:15,078 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216135000 ms.0 from job set of time 1560216135000 ms
   [druid] 2019-06-11 09:22:15,078 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.078 s for time 1560216135000 ms (execution: 0.068 s)
   [druid] 2019-06-11 09:22:15,078 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 57 from persistence list
   [druid] 2019-06-11 09:22:15,078 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_26_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,079 [c-thread-pool-0] INFO  che.spark.storage.BlockManager {1} - Removing RDD 57
   [druid] 2019-06-11 09:22:15,079 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 52 from persistence list
   [druid] 2019-06-11 09:22:15,079 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 52
   [druid] 2019-06-11 09:22:15,079 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 51 from persistence list
   [druid] 2019-06-11 09:22:15,079 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 51
   [druid] 2019-06-11 09:22:15,080 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[51] at socketTextStream at Transform.scala:18 of time 1560216135000 ms
   [druid] 2019-06-11 09:22:15,080 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216125000 ms
   [druid] 2019-06-11 09:22:15,080 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_18_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,080 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216125000 ms
   [druid] 2019-06-11 09:22:15,081 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_10_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,082 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_25_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,083 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 12
   [druid] 2019-06-11 09:22:15,084 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_17_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,084 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 4
   [druid] 2019-06-11 09:22:15,084 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 11
   [druid] 2019-06-11 09:22:15,085 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_16_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,086 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 5
   [druid] 2019-06-11 09:22:15,087 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_9_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,088 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 10
   [druid] 2019-06-11 09:22:15,089 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_21_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,090 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_8_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,091 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_15_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,093 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_12_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,093 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 2
   [druid] 2019-06-11 09:22:15,094 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_14_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,095 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_22_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,096 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 3
   [druid] 2019-06-11 09:22:15,097 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_11_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,098 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 13
   [druid] 2019-06-11 09:22:15,099 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_13_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:15,100 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 9
   [druid] 2019-06-11 09:22:20,012 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216140000 ms
   [druid] 2019-06-11 09:22:20,012 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216140000 ms.0 from job set of time 1560216140000 ms
   [druid] 2019-06-11 09:22:20,014 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:20,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 66 (map at Transform.scala:20)
   [druid] 2019-06-11 09:22:20,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:22:20,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 19 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:20,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 57 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:20,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 56, ShuffleMapStage 55)
   [druid] 2019-06-11 09:22:20,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 56)
   [druid] 2019-06-11 09:22:20,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 56 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:22:20,017 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_28 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:20,018 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_28_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:22:20,018 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_28_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:20,019 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 28 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:20,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 56 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:22:20,019 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 56.0 with 2 tasks
   [druid] 2019-06-11 09:22:20,019 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 56.0 (TID 37, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:20,020 [ker for task 37] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 56.0 (TID 37)
   [druid] 2019-06-11 09:22:20,029 [ker for task 37] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 56.0 (TID 37). 810 bytes result sent to driver
   [druid] 2019-06-11 09:22:20,029 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 56.0 (TID 38, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:20,029 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 56.0 (TID 37) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:22:20,029 [ker for task 38] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 56.0 (TID 38)
   [druid] 2019-06-11 09:22:20,040 [ker for task 38] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 56.0 (TID 38). 853 bytes result sent to driver
   [druid] 2019-06-11 09:22:20,040 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 56.0 (TID 38) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:22:20,040 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 56.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 56 (parallelize at Transform.scala:16) finished in 0.021 s
   [druid] 2019-06-11 09:22:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:22:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:22:20,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 57)
   [druid] 2019-06-11 09:22:20,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:22:20,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 57 (MapPartitionsRDD[71] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:20,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_29 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:20,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:20,043 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_29_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:20,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 29 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:20,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[71] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:22:20,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 57.0 with 1 tasks
   [druid] 2019-06-11 09:22:20,045 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 57.0 (TID 39, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:20,045 [ker for task 39] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 57.0 (TID 39)
   [druid] 2019-06-11 09:22:20,046 [ker for task 39] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:20,046 [ker for task 39] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:20,046 [ker for task 39] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:20,046 [ker for task 39] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:20,048 [ker for task 39] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 57.0 (TID 39). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:22:20,049 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 57.0 (TID 39) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:20,049 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 57.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:20,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 57 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:22:20,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 19 finished: print at Transform.scala:36, took 0.034513 s
   [druid] 2019-06-11 09:22:20,053 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:20,054 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 18 is 83 bytes
   [druid] 2019-06-11 09:22:20,054 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 19 is 160 bytes
   [druid] 2019-06-11 09:22:20,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 20 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:20,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 60 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:20,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 58, ShuffleMapStage 59)
   [druid] 2019-06-11 09:22:20,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:22:20,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 60 (MapPartitionsRDD[71] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:20,056 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_30 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:20,057 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:20,058 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_30_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:20,058 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 30 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:20,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[71] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:22:20,058 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 60.0 with 1 tasks
   [druid] 2019-06-11 09:22:20,059 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 60.0 (TID 40, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:20,059 [ker for task 40] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 60.0 (TID 40)
   [druid] 2019-06-11 09:22:20,060 [ker for task 40] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:20,060 [ker for task 40] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:20,060 [ker for task 40] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:20,060 [ker for task 40] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:20,061 [ker for task 40] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 60.0 (TID 40). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:22:20,062 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 60.0 (TID 40) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:20,062 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 60.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:20,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 60 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:22:20,062 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 20 finished: print at Transform.scala:36, took 0.009139 s
   [druid] 2019-06-11 09:22:20,063 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216140000 ms.0 from job set of time 1560216140000 ms
   [druid] 2019-06-11 09:22:20,063 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.063 s for time 1560216140000 ms (execution: 0.051 s)
   [druid] 2019-06-11 09:22:20,063 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 64 from persistence list
   [druid] 2019-06-11 09:22:20,063 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 64
   [druid] 2019-06-11 09:22:20,063 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 59 from persistence list
   [druid] 2019-06-11 09:22:20,064 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 59
   [druid] 2019-06-11 09:22:20,064 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 58 from persistence list
   [druid] 2019-06-11 09:22:20,064 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 58
   [druid] 2019-06-11 09:22:20,064 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[58] at socketTextStream at Transform.scala:18 of time 1560216140000 ms
   [druid] 2019-06-11 09:22:20,064 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216130000 ms
   [druid] 2019-06-11 09:22:20,064 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216130000 ms
   [druid] 2019-06-11 09:22:25,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216145000 ms
   [druid] 2019-06-11 09:22:25,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216145000 ms.0 from job set of time 1560216145000 ms
   [druid] 2019-06-11 09:22:25,011 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 73 (map at Transform.scala:20)
   [druid] 2019-06-11 09:22:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:22:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 21 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 63 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 61, ShuffleMapStage 62)
   [druid] 2019-06-11 09:22:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 62)
   [druid] 2019-06-11 09:22:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 62 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:22:25,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_31 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:25,018 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_31_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:22:25,018 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_31_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:25,018 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 31 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:25,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 62 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:22:25,019 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 62.0 with 2 tasks
   [druid] 2019-06-11 09:22:25,019 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 62.0 (TID 41, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:25,019 [ker for task 41] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 62.0 (TID 41)
   [druid] 2019-06-11 09:22:25,028 [ker for task 41] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 62.0 (TID 41). 810 bytes result sent to driver
   [druid] 2019-06-11 09:22:25,029 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 62.0 (TID 42, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:25,029 [ker for task 42] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 62.0 (TID 42)
   [druid] 2019-06-11 09:22:25,030 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 62.0 (TID 41) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:22:25,040 [ker for task 42] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 62.0 (TID 42). 853 bytes result sent to driver
   [druid] 2019-06-11 09:22:25,040 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 62.0 (TID 42) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:22:25,040 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 62.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:25,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 62 (parallelize at Transform.scala:16) finished in 0.022 s
   [druid] 2019-06-11 09:22:25,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:22:25,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:22:25,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 63)
   [druid] 2019-06-11 09:22:25,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:22:25,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 63 (MapPartitionsRDD[78] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:25,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_32 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:25,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_32_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:25,043 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_32_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:25,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 32 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[78] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:22:25,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 63.0 with 1 tasks
   [druid] 2019-06-11 09:22:25,044 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 63.0 (TID 43, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:25,045 [ker for task 43] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 63.0 (TID 43)
   [druid] 2019-06-11 09:22:25,046 [ker for task 43] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:25,046 [ker for task 43] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:25,046 [ker for task 43] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:25,046 [ker for task 43] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:25,047 [ker for task 43] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 63.0 (TID 43). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:22:25,048 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 63.0 (TID 43) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:25,048 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 63.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:25,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 63 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:22:25,048 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 21 finished: print at Transform.scala:36, took 0.036676 s
   [druid] 2019-06-11 09:22:25,052 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:25,052 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 20 is 83 bytes
   [druid] 2019-06-11 09:22:25,053 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 21 is 160 bytes
   [druid] 2019-06-11 09:22:25,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 22 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:25,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 66 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:25,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 64, ShuffleMapStage 65)
   [druid] 2019-06-11 09:22:25,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:22:25,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 66 (MapPartitionsRDD[78] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:25,054 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_33 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:25,056 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_33_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:25,056 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_33_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:25,056 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 33 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:25,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[78] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:22:25,056 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 66.0 with 1 tasks
   [druid] 2019-06-11 09:22:25,057 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 66.0 (TID 44, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:25,057 [ker for task 44] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 66.0 (TID 44)
   [druid] 2019-06-11 09:22:25,058 [ker for task 44] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:25,058 [ker for task 44] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:25,058 [ker for task 44] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:25,058 [ker for task 44] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:25,060 [ker for task 44] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 66.0 (TID 44). 966 bytes result sent to driver
   [druid] 2019-06-11 09:22:25,060 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 66.0 (TID 44) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:25,060 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 66.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:25,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 66 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:22:25,061 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 22 finished: print at Transform.scala:36, took 0.008554 s
   [druid] 2019-06-11 09:22:25,061 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216145000 ms.0 from job set of time 1560216145000 ms
   [druid] 2019-06-11 09:22:25,061 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.061 s for time 1560216145000 ms (execution: 0.052 s)
   [druid] 2019-06-11 09:22:25,061 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 71 from persistence list
   [druid] 2019-06-11 09:22:25,061 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 71
   [druid] 2019-06-11 09:22:25,062 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 66 from persistence list
   [druid] 2019-06-11 09:22:25,062 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 66
   [druid] 2019-06-11 09:22:25,062 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 65 from persistence list
   [druid] 2019-06-11 09:22:25,062 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 65
   [druid] 2019-06-11 09:22:25,062 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[65] at socketTextStream at Transform.scala:18 of time 1560216145000 ms
   [druid] 2019-06-11 09:22:25,062 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216135000 ms
   [druid] 2019-06-11 09:22:25,062 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216135000 ms
   [druid] 2019-06-11 09:22:30,009 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216150000 ms
   [druid] 2019-06-11 09:22:30,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216150000 ms.0 from job set of time 1560216150000 ms
   [druid] 2019-06-11 09:22:30,012 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:30,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:22:30,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 80 (map at Transform.scala:20)
   [druid] 2019-06-11 09:22:30,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 23 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:30,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 69 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:30,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 67, ShuffleMapStage 68)
   [druid] 2019-06-11 09:22:30,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 67)
   [druid] 2019-06-11 09:22:30,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 67 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:22:30,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_34 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:30,016 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_34_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:22:30,017 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_34_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:30,017 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 34 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:30,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 67 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:22:30,018 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 67.0 with 2 tasks
   [druid] 2019-06-11 09:22:30,018 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 67.0 (TID 45, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:30,018 [ker for task 45] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 67.0 (TID 45)
   [druid] 2019-06-11 09:22:30,031 [ker for task 45] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 67.0 (TID 45). 810 bytes result sent to driver
   [druid] 2019-06-11 09:22:30,031 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 67.0 (TID 46, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:30,031 [ker for task 46] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 67.0 (TID 46)
   [druid] 2019-06-11 09:22:30,031 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 67.0 (TID 45) in 13 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:22:30,042 [ker for task 46] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 67.0 (TID 46). 810 bytes result sent to driver
   [druid] 2019-06-11 09:22:30,043 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 67.0 (TID 46) in 12 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:22:30,043 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 67.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 67 (parallelize at Transform.scala:16) finished in 0.025 s
   [druid] 2019-06-11 09:22:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:22:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:22:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 69)
   [druid] 2019-06-11 09:22:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:22:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 69 (MapPartitionsRDD[85] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:30,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_35 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:30,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_35_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:30,046 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_35_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:30,047 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 35 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:30,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[85] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:22:30,047 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 69.0 with 1 tasks
   [druid] 2019-06-11 09:22:30,048 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 69.0 (TID 47, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:30,048 [ker for task 47] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 69.0 (TID 47)
   [druid] 2019-06-11 09:22:30,050 [ker for task 47] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:30,050 [ker for task 47] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:30,050 [ker for task 47] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:30,050 [ker for task 47] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:30,051 [ker for task 47] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 69.0 (TID 47). 966 bytes result sent to driver
   [druid] 2019-06-11 09:22:30,052 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 69.0 (TID 47) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:30,052 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 69.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:30,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 69 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:22:30,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 23 finished: print at Transform.scala:36, took 0.039737 s
   [druid] 2019-06-11 09:22:30,056 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:30,057 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 23 is 160 bytes
   [druid] 2019-06-11 09:22:30,057 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 22 is 83 bytes
   [druid] 2019-06-11 09:22:30,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 24 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:30,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 72 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:30,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 70, ShuffleMapStage 71)
   [druid] 2019-06-11 09:22:30,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:22:30,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 72 (MapPartitionsRDD[85] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:30,059 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_36 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:30,060 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_36_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:30,060 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_36_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:30,061 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 36 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:30,061 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 72 (MapPartitionsRDD[85] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:22:30,061 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 72.0 with 1 tasks
   [druid] 2019-06-11 09:22:30,062 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 72.0 (TID 48, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:30,062 [ker for task 48] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 72.0 (TID 48)
   [druid] 2019-06-11 09:22:30,063 [ker for task 48] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:30,063 [ker for task 48] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:30,063 [ker for task 48] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:30,063 [ker for task 48] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:30,065 [ker for task 48] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 72.0 (TID 48). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:22:30,065 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 72.0 (TID 48) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:30,065 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 72.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:30,065 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 72 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:22:30,066 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 24 finished: print at Transform.scala:36, took 0.009030 s
   [druid] 2019-06-11 09:22:30,066 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216150000 ms.0 from job set of time 1560216150000 ms
   [druid] 2019-06-11 09:22:30,066 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 78 from persistence list
   [druid] 2019-06-11 09:22:30,066 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.066 s for time 1560216150000 ms (execution: 0.057 s)
   [druid] 2019-06-11 09:22:30,066 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 78
   [druid] 2019-06-11 09:22:30,067 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 73 from persistence list
   [druid] 2019-06-11 09:22:30,067 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 73
   [druid] 2019-06-11 09:22:30,067 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 72 from persistence list
   [druid] 2019-06-11 09:22:30,067 [c-thread-pool-0] INFO  che.spark.storage.BlockManager {1} - Removing RDD 72
   [druid] 2019-06-11 09:22:30,067 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[72] at socketTextStream at Transform.scala:18 of time 1560216150000 ms
   [druid] 2019-06-11 09:22:30,068 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216140000 ms
   [druid] 2019-06-11 09:22:30,068 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216140000 ms
   [druid] 2019-06-11 09:22:35,010 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216155000 ms
   [druid] 2019-06-11 09:22:35,011 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216155000 ms.0 from job set of time 1560216155000 ms
   [druid] 2019-06-11 09:22:35,013 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:35,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 87 (map at Transform.scala:20)
   [druid] 2019-06-11 09:22:35,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:22:35,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 25 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:35,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 75 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:35,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 74, ShuffleMapStage 73)
   [druid] 2019-06-11 09:22:35,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 74)
   [druid] 2019-06-11 09:22:35,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 74 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:22:35,016 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_37 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:35,018 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_37_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:22:35,018 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_37_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:35,019 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 37 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:35,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 74 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:22:35,019 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 74.0 with 2 tasks
   [druid] 2019-06-11 09:22:35,020 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 74.0 (TID 49, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:35,020 [ker for task 49] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 74.0 (TID 49)
   [druid] 2019-06-11 09:22:35,029 [ker for task 49] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 74.0 (TID 49). 767 bytes result sent to driver
   [druid] 2019-06-11 09:22:35,030 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 74.0 (TID 50, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:35,030 [ker for task 50] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 74.0 (TID 50)
   [druid] 2019-06-11 09:22:35,030 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 74.0 (TID 49) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:22:35,045 [ker for task 50] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 74.0 (TID 50). 810 bytes result sent to driver
   [druid] 2019-06-11 09:22:35,046 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 74.0 (TID 50) in 16 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:22:35,046 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 74.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 74 (parallelize at Transform.scala:16) finished in 0.027 s
   [druid] 2019-06-11 09:22:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:22:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:22:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 75)
   [druid] 2019-06-11 09:22:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:22:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 75 (MapPartitionsRDD[92] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:35,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_38 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:35,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_38_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:35,049 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_38_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:35,049 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 38 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:35,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[92] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:22:35,050 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 75.0 with 1 tasks
   [druid] 2019-06-11 09:22:35,051 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 75.0 (TID 51, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:35,051 [ker for task 51] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 75.0 (TID 51)
   [druid] 2019-06-11 09:22:35,053 [ker for task 51] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:35,053 [ker for task 51] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:35,053 [ker for task 51] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:35,053 [ker for task 51] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:35,056 [ker for task 51] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 75.0 (TID 51). 966 bytes result sent to driver
   [druid] 2019-06-11 09:22:35,056 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 75.0 (TID 51) in 6 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:35,057 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 75.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:35,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 75 (print at Transform.scala:36) finished in 0.007 s
   [druid] 2019-06-11 09:22:35,057 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 25 finished: print at Transform.scala:36, took 0.044092 s
   [druid] 2019-06-11 09:22:35,061 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:35,062 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 24 is 83 bytes
   [druid] 2019-06-11 09:22:35,062 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 25 is 160 bytes
   [druid] 2019-06-11 09:22:35,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 26 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:35,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 78 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:35,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 76, ShuffleMapStage 77)
   [druid] 2019-06-11 09:22:35,063 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:22:35,063 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 78 (MapPartitionsRDD[92] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:35,064 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_39 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:35,066 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_39_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:35,066 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_39_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:35,067 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 39 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:35,067 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 78 (MapPartitionsRDD[92] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:22:35,067 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 78.0 with 1 tasks
   [druid] 2019-06-11 09:22:35,068 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 78.0 (TID 52, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:35,068 [ker for task 52] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 78.0 (TID 52)
   [druid] 2019-06-11 09:22:35,069 [ker for task 52] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:35,070 [ker for task 52] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:22:35,070 [ker for task 52] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:35,070 [ker for task 52] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:35,071 [ker for task 52] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 78.0 (TID 52). 966 bytes result sent to driver
   [druid] 2019-06-11 09:22:35,071 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 78.0 (TID 52) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:35,071 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 78.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:35,071 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 78 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:22:35,072 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 26 finished: print at Transform.scala:36, took 0.010490 s
   [druid] 2019-06-11 09:22:35,072 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216155000 ms.0 from job set of time 1560216155000 ms
   [druid] 2019-06-11 09:22:35,072 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.072 s for time 1560216155000 ms (execution: 0.061 s)
   [druid] 2019-06-11 09:22:35,072 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 85 from persistence list
   [druid] 2019-06-11 09:22:35,073 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 85
   [druid] 2019-06-11 09:22:35,073 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 80 from persistence list
   [druid] 2019-06-11 09:22:35,073 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 80
   [druid] 2019-06-11 09:22:35,073 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 79 from persistence list
   [druid] 2019-06-11 09:22:35,073 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 79
   [druid] 2019-06-11 09:22:35,074 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[79] at socketTextStream at Transform.scala:18 of time 1560216155000 ms
   [druid] 2019-06-11 09:22:35,074 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216145000 ms
   [druid] 2019-06-11 09:22:35,074 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216145000 ms
   [druid] 2019-06-11 09:22:40,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216160000 ms
   [druid] 2019-06-11 09:22:40,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216160000 ms.0 from job set of time 1560216160000 ms
   [druid] 2019-06-11 09:22:40,011 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:40,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 94 (map at Transform.scala:20)
   [druid] 2019-06-11 09:22:40,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:22:40,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 27 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:40,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 81 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:40,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 79, ShuffleMapStage 80)
   [druid] 2019-06-11 09:22:40,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 80)
   [druid] 2019-06-11 09:22:40,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 80 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:22:40,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_40 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:40,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_40_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:22:40,014 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_40_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:40,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 40 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:40,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 80 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:22:40,015 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 80.0 with 2 tasks
   [druid] 2019-06-11 09:22:40,015 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 80.0 (TID 53, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:40,015 [ker for task 53] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 80.0 (TID 53)
   [druid] 2019-06-11 09:22:40,026 [ker for task 53] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 80.0 (TID 53). 810 bytes result sent to driver
   [druid] 2019-06-11 09:22:40,026 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 80.0 (TID 54, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:40,026 [ker for task 54] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 80.0 (TID 54)
   [druid] 2019-06-11 09:22:40,026 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 80.0 (TID 53) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:22:40,038 [ker for task 54] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 80.0 (TID 54). 853 bytes result sent to driver
   [druid] 2019-06-11 09:22:40,038 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 80.0 (TID 54) in 12 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:22:40,038 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 80.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 80 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:22:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:22:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:22:40,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 81)
   [druid] 2019-06-11 09:22:40,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:22:40,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 81 (MapPartitionsRDD[99] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:40,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_41 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:40,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_41_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:40,041 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_41_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:40,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 41 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:40,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 81 (MapPartitionsRDD[99] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:22:40,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 81.0 with 1 tasks
   [druid] 2019-06-11 09:22:40,042 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 81.0 (TID 55, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:40,042 [ker for task 55] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 81.0 (TID 55)
   [druid] 2019-06-11 09:22:40,043 [ker for task 55] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:40,043 [ker for task 55] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:40,044 [ker for task 55] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:40,044 [ker for task 55] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:22:40,045 [ker for task 55] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 81.0 (TID 55). 966 bytes result sent to driver
   [druid] 2019-06-11 09:22:40,045 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 81.0 (TID 55) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:40,045 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 81.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:40,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 81 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:22:40,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 27 finished: print at Transform.scala:36, took 0.034915 s
   [druid] 2019-06-11 09:22:40,051 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:40,051 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 26 is 83 bytes
   [druid] 2019-06-11 09:22:40,052 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 27 is 160 bytes
   [druid] 2019-06-11 09:22:40,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 28 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:40,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 84 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:40,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 82, ShuffleMapStage 83)
   [druid] 2019-06-11 09:22:40,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:22:40,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 84 (MapPartitionsRDD[99] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:40,053 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_42 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:40,054 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_42_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:40,055 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_42_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:40,055 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 42 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:40,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 84 (MapPartitionsRDD[99] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:22:40,055 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 84.0 with 1 tasks
   [druid] 2019-06-11 09:22:40,056 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 84.0 (TID 56, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:40,056 [ker for task 56] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 84.0 (TID 56)
   [druid] 2019-06-11 09:22:40,057 [ker for task 56] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:40,057 [ker for task 56] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:40,057 [ker for task 56] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:40,057 [ker for task 56] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:40,058 [ker for task 56] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 84.0 (TID 56). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:22:40,059 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 84.0 (TID 56) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:40,059 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 84.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:40,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 84 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:22:40,059 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 28 finished: print at Transform.scala:36, took 0.008253 s
   [druid] 2019-06-11 09:22:40,060 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216160000 ms.0 from job set of time 1560216160000 ms
   [druid] 2019-06-11 09:22:40,060 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.060 s for time 1560216160000 ms (execution: 0.051 s)
   [druid] 2019-06-11 09:22:40,060 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 92 from persistence list
   [druid] 2019-06-11 09:22:40,060 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 92
   [druid] 2019-06-11 09:22:40,060 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 87 from persistence list
   [druid] 2019-06-11 09:22:40,060 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 87
   [druid] 2019-06-11 09:22:40,060 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 86 from persistence list
   [druid] 2019-06-11 09:22:40,060 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 86
   [druid] 2019-06-11 09:22:40,061 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[86] at socketTextStream at Transform.scala:18 of time 1560216160000 ms
   [druid] 2019-06-11 09:22:40,061 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216150000 ms
   [druid] 2019-06-11 09:22:40,061 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216150000 ms
   [druid] 2019-06-11 09:22:45,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216165000 ms
   [druid] 2019-06-11 09:22:45,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216165000 ms.0 from job set of time 1560216165000 ms
   [druid] 2019-06-11 09:22:45,013 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:45,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:22:45,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 101 (map at Transform.scala:20)
   [druid] 2019-06-11 09:22:45,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 29 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:45,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 87 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:45,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 85, ShuffleMapStage 86)
   [druid] 2019-06-11 09:22:45,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 85)
   [druid] 2019-06-11 09:22:45,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 85 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:22:45,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_43 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:45,016 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_43_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:22:45,017 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_43_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:45,017 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 43 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:45,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 85 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:22:45,018 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 85.0 with 2 tasks
   [druid] 2019-06-11 09:22:45,018 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 85.0 (TID 57, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:45,018 [ker for task 57] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 85.0 (TID 57)
   [druid] 2019-06-11 09:22:45,028 [ker for task 57] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 85.0 (TID 57). 810 bytes result sent to driver
   [druid] 2019-06-11 09:22:45,028 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 85.0 (TID 58, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:45,028 [ker for task 58] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 85.0 (TID 58)
   [druid] 2019-06-11 09:22:45,028 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 85.0 (TID 57) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:22:45,039 [ker for task 58] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 85.0 (TID 58). 853 bytes result sent to driver
   [druid] 2019-06-11 09:22:45,039 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 85.0 (TID 58) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:22:45,039 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 85.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:45,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 85 (parallelize at Transform.scala:16) finished in 0.021 s
   [druid] 2019-06-11 09:22:45,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:22:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:22:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 87)
   [druid] 2019-06-11 09:22:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:22:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 87 (MapPartitionsRDD[106] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:45,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_44 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:45,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_44_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:45,042 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_44_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:45,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 44 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:45,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 87 (MapPartitionsRDD[106] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:22:45,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 87.0 with 1 tasks
   [druid] 2019-06-11 09:22:45,043 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 87.0 (TID 59, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:45,043 [ker for task 59] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 87.0 (TID 59)
   [druid] 2019-06-11 09:22:45,044 [ker for task 59] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:45,044 [ker for task 59] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:45,044 [ker for task 59] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:45,044 [ker for task 59] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:45,045 [ker for task 59] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 87.0 (TID 59). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:22:45,046 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 87.0 (TID 59) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:45,046 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 87.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:45,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 87 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:22:45,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 29 finished: print at Transform.scala:36, took 0.032996 s
   [druid] 2019-06-11 09:22:45,051 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:45,051 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 29 is 160 bytes
   [druid] 2019-06-11 09:22:45,051 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 28 is 83 bytes
   [druid] 2019-06-11 09:22:45,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 30 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 90 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 88, ShuffleMapStage 89)
   [druid] 2019-06-11 09:22:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:22:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 90 (MapPartitionsRDD[106] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:45,053 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_45 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:45,054 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_45_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:45,054 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_45_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:45,054 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 45 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:45,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 90 (MapPartitionsRDD[106] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:22:45,054 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 90.0 with 1 tasks
   [druid] 2019-06-11 09:22:45,055 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 90.0 (TID 60, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:45,055 [ker for task 60] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 90.0 (TID 60)
   [druid] 2019-06-11 09:22:45,056 [ker for task 60] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:45,056 [ker for task 60] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:45,057 [ker for task 60] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:45,057 [ker for task 60] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:45,057 [ker for task 60] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 90.0 (TID 60). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:22:45,057 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 90.0 (TID 60) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:45,057 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 90.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:45,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 90 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:22:45,058 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 30 finished: print at Transform.scala:36, took 0.007788 s
   [druid] 2019-06-11 09:22:45,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216165000 ms.0 from job set of time 1560216165000 ms
   [druid] 2019-06-11 09:22:45,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.058 s for time 1560216165000 ms (execution: 0.050 s)
   [druid] 2019-06-11 09:22:45,058 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 99 from persistence list
   [druid] 2019-06-11 09:22:45,059 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 99
   [druid] 2019-06-11 09:22:45,059 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 94 from persistence list
   [druid] 2019-06-11 09:22:45,059 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 94
   [druid] 2019-06-11 09:22:45,059 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 93 from persistence list
   [druid] 2019-06-11 09:22:45,060 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 93
   [druid] 2019-06-11 09:22:45,060 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[93] at socketTextStream at Transform.scala:18 of time 1560216165000 ms
   [druid] 2019-06-11 09:22:45,060 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216155000 ms
   [druid] 2019-06-11 09:22:45,060 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216155000 ms
   [druid] 2019-06-11 09:22:50,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216170000 ms
   [druid] 2019-06-11 09:22:50,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216170000 ms.0 from job set of time 1560216170000 ms
   [druid] 2019-06-11 09:22:50,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:50,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:22:50,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 108 (map at Transform.scala:20)
   [druid] 2019-06-11 09:22:50,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 31 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:50,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 93 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:50,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 91, ShuffleMapStage 92)
   [druid] 2019-06-11 09:22:50,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 91)
   [druid] 2019-06-11 09:22:50,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 91 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:22:50,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_46 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:50,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_46_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:22:50,015 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_46_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:22:50,016 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 46 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:50,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 91 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:22:50,016 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 91.0 with 2 tasks
   [druid] 2019-06-11 09:22:50,016 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 91.0 (TID 61, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:50,017 [ker for task 61] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 91.0 (TID 61)
   [druid] 2019-06-11 09:22:50,029 [ker for task 61] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 91.0 (TID 61). 810 bytes result sent to driver
   [druid] 2019-06-11 09:22:50,029 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 91.0 (TID 62, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:50,030 [ker for task 62] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 91.0 (TID 62)
   [druid] 2019-06-11 09:22:50,030 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 91.0 (TID 61) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:22:50,048 [ker for task 62] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 91.0 (TID 62). 896 bytes result sent to driver
   [druid] 2019-06-11 09:22:50,049 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 91.0 (TID 62) in 20 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:22:50,049 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 91.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 91 (parallelize at Transform.scala:16) finished in 0.033 s
   [druid] 2019-06-11 09:22:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:22:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:22:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 93)
   [druid] 2019-06-11 09:22:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:22:50,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 93 (MapPartitionsRDD[113] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:50,051 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_47 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:50,053 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_47_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:50,054 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_47_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:22:50,054 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 47 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:50,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 93 (MapPartitionsRDD[113] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:22:50,055 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 93.0 with 1 tasks
   [druid] 2019-06-11 09:22:50,055 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 93.0 (TID 63, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:50,055 [ker for task 63] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 93.0 (TID 63)
   [druid] 2019-06-11 09:22:50,057 [ker for task 63] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:50,057 [ker for task 63] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:50,057 [ker for task 63] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:50,057 [ker for task 63] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:50,059 [ker for task 63] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 93.0 (TID 63). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:22:50,059 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 93.0 (TID 63) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:50,059 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 93.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:50,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 93 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:22:50,060 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 31 finished: print at Transform.scala:36, took 0.049516 s
   [druid] 2019-06-11 09:22:50,063 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:50,063 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 31 is 160 bytes
   [druid] 2019-06-11 09:22:50,064 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 30 is 83 bytes
   [druid] 2019-06-11 09:22:50,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 32 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:50,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 96 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:50,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 94, ShuffleMapStage 95)
   [druid] 2019-06-11 09:22:50,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:22:50,065 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 96 (MapPartitionsRDD[113] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:50,066 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_48 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:50,068 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_48_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:50,069 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_48_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:22:50,069 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 48 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:50,069 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 96 (MapPartitionsRDD[113] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:22:50,069 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 96.0 with 1 tasks
   [druid] 2019-06-11 09:22:50,070 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 96.0 (TID 64, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:50,070 [ker for task 64] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 96.0 (TID 64)
   [druid] 2019-06-11 09:22:50,072 [ker for task 64] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:50,072 [ker for task 64] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:50,072 [ker for task 64] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:50,072 [ker for task 64] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:50,073 [ker for task 64] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 96.0 (TID 64). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:22:50,074 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 96.0 (TID 64) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:50,074 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 96.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:50,074 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 96 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:22:50,075 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 32 finished: print at Transform.scala:36, took 0.011669 s
   [druid] 2019-06-11 09:22:50,075 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216170000 ms.0 from job set of time 1560216170000 ms
   [druid] 2019-06-11 09:22:50,075 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.075 s for time 1560216170000 ms (execution: 0.067 s)
   [druid] 2019-06-11 09:22:50,075 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 106 from persistence list
   [druid] 2019-06-11 09:22:50,076 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 106
   [druid] 2019-06-11 09:22:50,076 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 101 from persistence list
   [druid] 2019-06-11 09:22:50,076 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 101
   [druid] 2019-06-11 09:22:50,076 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 100 from persistence list
   [druid] 2019-06-11 09:22:50,076 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 100
   [druid] 2019-06-11 09:22:50,076 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[100] at socketTextStream at Transform.scala:18 of time 1560216170000 ms
   [druid] 2019-06-11 09:22:50,076 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216160000 ms
   [druid] 2019-06-11 09:22:50,076 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216160000 ms
   [druid] 2019-06-11 09:22:55,011 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216175000 ms
   [druid] 2019-06-11 09:22:55,012 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216175000 ms.0 from job set of time 1560216175000 ms
   [druid] 2019-06-11 09:22:55,016 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:55,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 115 (map at Transform.scala:20)
   [druid] 2019-06-11 09:22:55,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:22:55,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 33 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:55,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 99 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:55,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 97, ShuffleMapStage 98)
   [druid] 2019-06-11 09:22:55,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 98)
   [druid] 2019-06-11 09:22:55,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 98 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:22:55,018 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_49 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:55,022 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_49_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:22:55,023 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_49_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:22:55,023 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 49 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:55,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 98 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:22:55,023 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 98.0 with 2 tasks
   [druid] 2019-06-11 09:22:55,024 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 98.0 (TID 65, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:55,024 [ker for task 65] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 98.0 (TID 65)
   [druid] 2019-06-11 09:22:55,040 [ker for task 65] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 98.0 (TID 65). 810 bytes result sent to driver
   [druid] 2019-06-11 09:22:55,040 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 98.0 (TID 66, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:22:55,040 [ker for task 66] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 98.0 (TID 66)
   [druid] 2019-06-11 09:22:55,040 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 98.0 (TID 65) in 16 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:22:55,055 [ker for task 66] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 98.0 (TID 66). 853 bytes result sent to driver
   [druid] 2019-06-11 09:22:55,055 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 98.0 (TID 66) in 15 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:22:55,055 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 98.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:55,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 98 (parallelize at Transform.scala:16) finished in 0.031 s
   [druid] 2019-06-11 09:22:55,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:22:55,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:22:55,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 99)
   [druid] 2019-06-11 09:22:55,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:22:55,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 99 (MapPartitionsRDD[120] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:55,057 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_50 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:55,071 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_50_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:22:55,071 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_42_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:22:55,073 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 21
   [druid] 2019-06-11 09:22:55,073 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_50_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:22:55,073 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 50 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:55,074 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 99 (MapPartitionsRDD[120] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:22:55,074 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 99.0 with 1 tasks
   [druid] 2019-06-11 09:22:55,074 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_44_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:22:55,075 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 99.0 (TID 67, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:55,075 [ker for task 67] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 99.0 (TID 67)
   [druid] 2019-06-11 09:22:55,075 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 28
   [druid] 2019-06-11 09:22:55,076 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_32_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:22:55,077 [ker for task 67] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:55,077 [ker for task 67] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:55,078 [ker for task 67] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:55,078 [ker for task 67] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:55,079 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_40_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:22:55,080 [ker for task 67] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 99.0 (TID 67). 966 bytes result sent to driver
   [druid] 2019-06-11 09:22:55,080 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 19
   [druid] 2019-06-11 09:22:55,082 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 99.0 (TID 67) in 7 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:55,082 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 99.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:55,082 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 99 (print at Transform.scala:36) finished in 0.008 s
   [druid] 2019-06-11 09:22:55,083 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_39_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,083 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 33 finished: print at Transform.scala:36, took 0.066445 s
   [druid] 2019-06-11 09:22:55,085 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 17
   [druid] 2019-06-11 09:22:55,089 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_27_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,092 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_43_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,093 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:22:55,094 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 32 is 83 bytes
   [druid] 2019-06-11 09:22:55,094 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_41_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,094 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 33 is 160 bytes
   [druid] 2019-06-11 09:22:55,094 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 26
   [druid] 2019-06-11 09:22:55,095 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 18
   [druid] 2019-06-11 09:22:55,095 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 20
   [druid] 2019-06-11 09:22:55,095 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 34 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:22:55,095 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 102 (print at Transform.scala:36)
   [druid] 2019-06-11 09:22:55,095 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 100, ShuffleMapStage 101)
   [druid] 2019-06-11 09:22:55,095 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 15
   [druid] 2019-06-11 09:22:55,096 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:22:55,096 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 102 (MapPartitionsRDD[120] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:22:55,097 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_48_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,098 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_51 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:55,099 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_34_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,099 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 24
   [druid] 2019-06-11 09:22:55,099 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_51_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:22:55,100 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_51_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,100 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 51 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:22:55,100 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_30_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,101 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 102 (MapPartitionsRDD[120] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:22:55,101 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 102.0 with 1 tasks
   [druid] 2019-06-11 09:22:55,101 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 22
   [druid] 2019-06-11 09:22:55,101 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 14
   [druid] 2019-06-11 09:22:55,101 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 102.0 (TID 68, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:22:55,102 [ker for task 68] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 102.0 (TID 68)
   [druid] 2019-06-11 09:22:55,102 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_38_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,105 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_28_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,105 [ker for task 68] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:22:55,105 [ker for task 68] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:55,106 [ker for task 68] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:22:55,106 [ker for task 68] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:22:55,106 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_33_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,107 [ker for task 68] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 102.0 (TID 68). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:22:55,107 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_45_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,107 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 102.0 (TID 68) in 6 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:22:55,108 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 102.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:22:55,108 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 27
   [druid] 2019-06-11 09:22:55,108 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 102 (print at Transform.scala:36) finished in 0.007 s
   [druid] 2019-06-11 09:22:55,108 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 25
   [druid] 2019-06-11 09:22:55,108 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 16
   [druid] 2019-06-11 09:22:55,108 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 34 finished: print at Transform.scala:36, took 0.015019 s
   [druid] 2019-06-11 09:22:55,109 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216175000 ms.0 from job set of time 1560216175000 ms
   [druid] 2019-06-11 09:22:55,109 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.109 s for time 1560216175000 ms (execution: 0.097 s)
   [druid] 2019-06-11 09:22:55,110 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 113 from persistence list
   [druid] 2019-06-11 09:22:55,110 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_29_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,110 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 113
   [druid] 2019-06-11 09:22:55,110 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 108 from persistence list
   [druid] 2019-06-11 09:22:55,111 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 108
   [druid] 2019-06-11 09:22:55,111 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 107 from persistence list
   [druid] 2019-06-11 09:22:55,111 [c-thread-pool-8] INFO  che.spark.storage.BlockManager {1} - Removing RDD 107
   [druid] 2019-06-11 09:22:55,111 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[107] at socketTextStream at Transform.scala:18 of time 1560216175000 ms
   [druid] 2019-06-11 09:22:55,111 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216165000 ms
   [druid] 2019-06-11 09:22:55,111 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216165000 ms
   [druid] 2019-06-11 09:22:55,113 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_35_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,114 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_36_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,115 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 23
   [druid] 2019-06-11 09:22:55,116 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_31_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,117 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 29
   [druid] 2019-06-11 09:22:55,119 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_37_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,121 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_47_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:22:55,122 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_46_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:00,015 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216180000 ms
   [druid] 2019-06-11 09:23:00,016 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216180000 ms.0 from job set of time 1560216180000 ms
   [druid] 2019-06-11 09:23:00,022 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:00,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:23:00,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 122 (map at Transform.scala:20)
   [druid] 2019-06-11 09:23:00,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 35 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:00,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 105 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:00,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 103, ShuffleMapStage 104)
   [druid] 2019-06-11 09:23:00,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 103)
   [druid] 2019-06-11 09:23:00,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 103 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:23:00,024 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_52 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:00,025 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_52_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:23:00,025 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_52_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:00,026 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 52 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:00,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 103 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:23:00,026 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 103.0 with 2 tasks
   [druid] 2019-06-11 09:23:00,026 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 103.0 (TID 69, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:00,027 [ker for task 69] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 103.0 (TID 69)
   [druid] 2019-06-11 09:23:00,041 [ker for task 69] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 103.0 (TID 69). 853 bytes result sent to driver
   [druid] 2019-06-11 09:23:00,041 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 103.0 (TID 70, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:00,042 [ker for task 70] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 103.0 (TID 70)
   [druid] 2019-06-11 09:23:00,042 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 103.0 (TID 69) in 16 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:23:00,059 [ker for task 70] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 103.0 (TID 70). 853 bytes result sent to driver
   [druid] 2019-06-11 09:23:00,060 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 103.0 (TID 70) in 19 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:23:00,060 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 103.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:00,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 103 (parallelize at Transform.scala:16) finished in 0.034 s
   [druid] 2019-06-11 09:23:00,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:23:00,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:23:00,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 105)
   [druid] 2019-06-11 09:23:00,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:23:00,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 105 (MapPartitionsRDD[127] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:00,061 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_53 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:00,063 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_53_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:00,063 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_53_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:00,064 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 53 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:00,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 105 (MapPartitionsRDD[127] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:23:00,064 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 105.0 with 1 tasks
   [druid] 2019-06-11 09:23:00,064 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 105.0 (TID 71, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:00,065 [ker for task 71] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 105.0 (TID 71)
   [druid] 2019-06-11 09:23:00,068 [ker for task 71] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:00,068 [ker for task 71] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:00,069 [ker for task 71] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:00,070 [ker for task 71] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:23:00,071 [ker for task 71] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 105.0 (TID 71). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:00,072 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 105.0 (TID 71) in 8 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:00,072 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 105.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:00,072 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 105 (print at Transform.scala:36) finished in 0.008 s
   [druid] 2019-06-11 09:23:00,072 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 35 finished: print at Transform.scala:36, took 0.050082 s
   [druid] 2019-06-11 09:23:00,075 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:00,076 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 35 is 160 bytes
   [druid] 2019-06-11 09:23:00,076 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 34 is 83 bytes
   [druid] 2019-06-11 09:23:00,076 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 36 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:00,076 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 108 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:00,076 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 107, ShuffleMapStage 106)
   [druid] 2019-06-11 09:23:00,076 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:23:00,077 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 108 (MapPartitionsRDD[127] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:00,078 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_54 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:00,079 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_54_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:00,080 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_54_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:00,080 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 54 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:00,081 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 108 (MapPartitionsRDD[127] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:23:00,081 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 108.0 with 1 tasks
   [druid] 2019-06-11 09:23:00,081 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 108.0 (TID 72, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:00,082 [ker for task 72] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 108.0 (TID 72)
   [druid] 2019-06-11 09:23:00,085 [ker for task 72] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:00,085 [ker for task 72] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:00,086 [ker for task 72] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:00,086 [ker for task 72] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:00,098 [ker for task 72] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 108.0 (TID 72). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:23:00,099 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 108.0 (TID 72) in 18 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:00,099 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 108.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:00,100 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 108 (print at Transform.scala:36) finished in 0.018 s
   [druid] 2019-06-11 09:23:00,101 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 36 finished: print at Transform.scala:36, took 0.025221 s
   [druid] 2019-06-11 09:23:00,102 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216180000 ms.0 from job set of time 1560216180000 ms
   [druid] 2019-06-11 09:23:00,103 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 120 from persistence list
   [druid] 2019-06-11 09:23:00,103 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.102 s for time 1560216180000 ms (execution: 0.086 s)
   [druid] 2019-06-11 09:23:00,103 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 120
   [druid] 2019-06-11 09:23:00,103 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 115 from persistence list
   [druid] 2019-06-11 09:23:00,104 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 115
   [druid] 2019-06-11 09:23:00,104 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 114 from persistence list
   [druid] 2019-06-11 09:23:00,104 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 114
   [druid] 2019-06-11 09:23:00,104 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[114] at socketTextStream at Transform.scala:18 of time 1560216180000 ms
   [druid] 2019-06-11 09:23:00,104 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216170000 ms
   [druid] 2019-06-11 09:23:00,104 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216170000 ms
   [druid] 2019-06-11 09:23:05,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216185000 ms
   [druid] 2019-06-11 09:23:05,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216185000 ms.0 from job set of time 1560216185000 ms
   [druid] 2019-06-11 09:23:05,012 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:05,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:23:05,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 129 (map at Transform.scala:20)
   [druid] 2019-06-11 09:23:05,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 37 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:05,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 111 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:05,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 109, ShuffleMapStage 110)
   [druid] 2019-06-11 09:23:05,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 109)
   [druid] 2019-06-11 09:23:05,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 109 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:23:05,017 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_55 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:05,019 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_55_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:23:05,019 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_55_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:05,020 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 55 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:05,020 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 109 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:23:05,020 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 109.0 with 2 tasks
   [druid] 2019-06-11 09:23:05,021 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 109.0 (TID 73, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:05,021 [ker for task 73] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 109.0 (TID 73)
   [druid] 2019-06-11 09:23:05,032 [ker for task 73] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 109.0 (TID 73). 853 bytes result sent to driver
   [druid] 2019-06-11 09:23:05,034 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 109.0 (TID 74, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:05,034 [ker for task 74] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 109.0 (TID 74)
   [druid] 2019-06-11 09:23:05,034 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 109.0 (TID 73) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:23:05,044 [ker for task 74] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 109.0 (TID 74). 810 bytes result sent to driver
   [druid] 2019-06-11 09:23:05,045 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 109.0 (TID 74) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:23:05,045 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 109.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:05,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 109 (parallelize at Transform.scala:16) finished in 0.025 s
   [druid] 2019-06-11 09:23:05,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:23:05,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:23:05,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 111)
   [druid] 2019-06-11 09:23:05,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:23:05,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 111 (MapPartitionsRDD[134] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:05,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_56 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:05,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_56_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:05,050 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_56_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:05,050 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 56 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:05,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 111 (MapPartitionsRDD[134] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:23:05,051 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 111.0 with 1 tasks
   [druid] 2019-06-11 09:23:05,052 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 111.0 (TID 75, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:05,052 [ker for task 75] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 111.0 (TID 75)
   [druid] 2019-06-11 09:23:05,053 [ker for task 75] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:05,053 [ker for task 75] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:05,053 [ker for task 75] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:05,053 [ker for task 75] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:05,054 [ker for task 75] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 111.0 (TID 75). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:05,054 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 111.0 (TID 75) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:05,054 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 111.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:05,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 111 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:23:05,055 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 37 finished: print at Transform.scala:36, took 0.042498 s
   [druid] 2019-06-11 09:23:05,057 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:05,058 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 37 is 160 bytes
   [druid] 2019-06-11 09:23:05,058 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 36 is 83 bytes
   [druid] 2019-06-11 09:23:05,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 38 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:05,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 114 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:05,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 112, ShuffleMapStage 113)
   [druid] 2019-06-11 09:23:05,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:23:05,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 114 (MapPartitionsRDD[134] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:05,059 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_57 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:05,060 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_57_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:05,061 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_57_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:05,061 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 57 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:05,061 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 114 (MapPartitionsRDD[134] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:23:05,062 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 114.0 with 1 tasks
   [druid] 2019-06-11 09:23:05,062 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 114.0 (TID 76, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:05,062 [ker for task 76] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 114.0 (TID 76)
   [druid] 2019-06-11 09:23:05,064 [ker for task 76] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:05,064 [ker for task 76] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:23:05,064 [ker for task 76] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:05,064 [ker for task 76] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:05,070 [ker for task 76] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 114.0 (TID 76). 966 bytes result sent to driver
   [druid] 2019-06-11 09:23:05,071 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 114.0 (TID 76) in 9 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:05,071 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 114.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:05,071 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 114 (print at Transform.scala:36) finished in 0.009 s
   [druid] 2019-06-11 09:23:05,071 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 38 finished: print at Transform.scala:36, took 0.013541 s
   [druid] 2019-06-11 09:23:05,071 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216185000 ms.0 from job set of time 1560216185000 ms
   [druid] 2019-06-11 09:23:05,072 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.071 s for time 1560216185000 ms (execution: 0.062 s)
   [druid] 2019-06-11 09:23:05,072 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 127 from persistence list
   [druid] 2019-06-11 09:23:05,072 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 127
   [druid] 2019-06-11 09:23:05,072 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 122 from persistence list
   [druid] 2019-06-11 09:23:05,072 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 122
   [druid] 2019-06-11 09:23:05,073 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 121 from persistence list
   [druid] 2019-06-11 09:23:05,073 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 121
   [druid] 2019-06-11 09:23:05,073 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[121] at socketTextStream at Transform.scala:18 of time 1560216185000 ms
   [druid] 2019-06-11 09:23:05,073 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216175000 ms
   [druid] 2019-06-11 09:23:05,073 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216175000 ms
   [druid] 2019-06-11 09:23:10,012 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216190000 ms
   [druid] 2019-06-11 09:23:10,013 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216190000 ms.0 from job set of time 1560216190000 ms
   [druid] 2019-06-11 09:23:10,014 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:10,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:23:10,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 136 (map at Transform.scala:20)
   [druid] 2019-06-11 09:23:10,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 39 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:10,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 117 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:10,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 115, ShuffleMapStage 116)
   [druid] 2019-06-11 09:23:10,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 115)
   [druid] 2019-06-11 09:23:10,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 115 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:23:10,016 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_58 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:10,017 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_58_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:23:10,018 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_58_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:10,018 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 58 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:10,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 115 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:23:10,018 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 115.0 with 2 tasks
   [druid] 2019-06-11 09:23:10,019 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 115.0 (TID 77, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:10,019 [ker for task 77] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 115.0 (TID 77)
   [druid] 2019-06-11 09:23:10,028 [ker for task 77] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 115.0 (TID 77). 810 bytes result sent to driver
   [druid] 2019-06-11 09:23:10,028 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 115.0 (TID 78, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:10,028 [ker for task 78] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 115.0 (TID 78)
   [druid] 2019-06-11 09:23:10,028 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 115.0 (TID 77) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:23:10,037 [ker for task 78] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 115.0 (TID 78). 810 bytes result sent to driver
   [druid] 2019-06-11 09:23:10,037 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 115.0 (TID 78) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:23:10,037 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 115.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:10,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 115 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:23:10,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:23:10,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:23:10,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 117)
   [druid] 2019-06-11 09:23:10,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:23:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 117 (MapPartitionsRDD[141] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:10,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_59 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:10,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_59_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:10,040 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_59_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:10,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 59 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 117 (MapPartitionsRDD[141] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:23:10,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 117.0 with 1 tasks
   [druid] 2019-06-11 09:23:10,041 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 117.0 (TID 79, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:10,041 [ker for task 79] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 117.0 (TID 79)
   [druid] 2019-06-11 09:23:10,042 [ker for task 79] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:10,042 [ker for task 79] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:10,042 [ker for task 79] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:10,042 [ker for task 79] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:10,054 [ker for task 79] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 117.0 (TID 79). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:10,054 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 117.0 (TID 79) in 13 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:10,054 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 117.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:10,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 117 (print at Transform.scala:36) finished in 0.013 s
   [druid] 2019-06-11 09:23:10,055 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 39 finished: print at Transform.scala:36, took 0.040452 s
   [druid] 2019-06-11 09:23:10,058 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:10,058 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 39 is 160 bytes
   [druid] 2019-06-11 09:23:10,058 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 38 is 83 bytes
   [druid] 2019-06-11 09:23:10,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 40 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:10,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 120 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:10,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 118, ShuffleMapStage 119)
   [druid] 2019-06-11 09:23:10,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:23:10,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 120 (MapPartitionsRDD[141] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:10,060 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_60 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:10,061 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_60_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:10,061 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_60_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:10,062 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 60 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:10,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 120 (MapPartitionsRDD[141] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:23:10,062 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 120.0 with 1 tasks
   [druid] 2019-06-11 09:23:10,063 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 120.0 (TID 80, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:10,063 [ker for task 80] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 120.0 (TID 80)
   [druid] 2019-06-11 09:23:10,064 [ker for task 80] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:10,064 [ker for task 80] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:10,065 [ker for task 80] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:10,065 [ker for task 80] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:10,066 [ker for task 80] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 120.0 (TID 80). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:10,066 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 120.0 (TID 80) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:10,066 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 120.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:10,066 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 120 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:23:10,066 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 40 finished: print at Transform.scala:36, took 0.008568 s
   [druid] 2019-06-11 09:23:10,067 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216190000 ms.0 from job set of time 1560216190000 ms
   [druid] 2019-06-11 09:23:10,067 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.067 s for time 1560216190000 ms (execution: 0.055 s)
   [druid] 2019-06-11 09:23:10,067 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 134 from persistence list
   [druid] 2019-06-11 09:23:10,067 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 134
   [druid] 2019-06-11 09:23:10,067 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 129 from persistence list
   [druid] 2019-06-11 09:23:10,067 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 129
   [druid] 2019-06-11 09:23:10,068 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 128 from persistence list
   [druid] 2019-06-11 09:23:10,068 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 128
   [druid] 2019-06-11 09:23:10,068 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[128] at socketTextStream at Transform.scala:18 of time 1560216190000 ms
   [druid] 2019-06-11 09:23:10,068 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216180000 ms
   [druid] 2019-06-11 09:23:10,068 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216180000 ms
   [druid] 2019-06-11 09:23:15,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216195000 ms
   [druid] 2019-06-11 09:23:15,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216195000 ms.0 from job set of time 1560216195000 ms
   [druid] 2019-06-11 09:23:15,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:15,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 143 (map at Transform.scala:20)
   [druid] 2019-06-11 09:23:15,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:23:15,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 41 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:15,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 123 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:15,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 121, ShuffleMapStage 122)
   [druid] 2019-06-11 09:23:15,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 122)
   [druid] 2019-06-11 09:23:15,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 122 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:23:15,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_61 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:15,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_61_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:23:15,014 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_61_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:15,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 61 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:15,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 122 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:23:15,014 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 122.0 with 2 tasks
   [druid] 2019-06-11 09:23:15,015 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 122.0 (TID 81, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:15,015 [ker for task 81] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 122.0 (TID 81)
   [druid] 2019-06-11 09:23:15,024 [ker for task 81] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 122.0 (TID 81). 767 bytes result sent to driver
   [druid] 2019-06-11 09:23:15,025 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 122.0 (TID 82, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:15,025 [ker for task 82] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 122.0 (TID 82)
   [druid] 2019-06-11 09:23:15,025 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 122.0 (TID 81) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:23:15,041 [ker for task 82] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 122.0 (TID 82). 853 bytes result sent to driver
   [druid] 2019-06-11 09:23:15,042 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 122.0 (TID 82) in 17 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:23:15,042 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 122.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 122 (parallelize at Transform.scala:16) finished in 0.027 s
   [druid] 2019-06-11 09:23:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:23:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:23:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 123)
   [druid] 2019-06-11 09:23:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:23:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 123 (MapPartitionsRDD[148] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:15,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_62 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:15,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_62_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:15,046 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_62_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:15,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 62 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:15,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 123 (MapPartitionsRDD[148] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:23:15,047 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 123.0 with 1 tasks
   [druid] 2019-06-11 09:23:15,047 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 123.0 (TID 83, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:15,047 [ker for task 83] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 123.0 (TID 83)
   [druid] 2019-06-11 09:23:15,049 [ker for task 83] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:15,049 [ker for task 83] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:15,049 [ker for task 83] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:15,049 [ker for task 83] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:15,050 [ker for task 83] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 123.0 (TID 83). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:15,051 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 123.0 (TID 83) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:15,051 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 123.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:15,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 123 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:23:15,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 41 finished: print at Transform.scala:36, took 0.040242 s
   [druid] 2019-06-11 09:23:15,054 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:15,055 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 40 is 83 bytes
   [druid] 2019-06-11 09:23:15,055 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 41 is 160 bytes
   [druid] 2019-06-11 09:23:15,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 42 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:15,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 126 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:15,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 125, ShuffleMapStage 124)
   [druid] 2019-06-11 09:23:15,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:23:15,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 126 (MapPartitionsRDD[148] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:15,056 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_63 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:15,058 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_63_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:15,058 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_63_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:15,058 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 63 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:15,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 126 (MapPartitionsRDD[148] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:23:15,059 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 126.0 with 1 tasks
   [druid] 2019-06-11 09:23:15,059 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 126.0 (TID 84, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:15,059 [ker for task 84] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 126.0 (TID 84)
   [druid] 2019-06-11 09:23:15,060 [ker for task 84] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:15,060 [ker for task 84] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:15,060 [ker for task 84] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:15,060 [ker for task 84] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:15,062 [ker for task 84] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 126.0 (TID 84). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:15,062 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 126.0 (TID 84) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:15,062 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 126.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:15,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 126 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:23:15,063 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 42 finished: print at Transform.scala:36, took 0.008396 s
   [druid] 2019-06-11 09:23:15,063 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216195000 ms.0 from job set of time 1560216195000 ms
   [druid] 2019-06-11 09:23:15,063 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.063 s for time 1560216195000 ms (execution: 0.055 s)
   [druid] 2019-06-11 09:23:15,063 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 141 from persistence list
   [druid] 2019-06-11 09:23:15,063 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 141
   [druid] 2019-06-11 09:23:15,063 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 136 from persistence list
   [druid] 2019-06-11 09:23:15,064 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 136
   [druid] 2019-06-11 09:23:15,064 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 135 from persistence list
   [druid] 2019-06-11 09:23:15,064 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 135
   [druid] 2019-06-11 09:23:15,064 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[135] at socketTextStream at Transform.scala:18 of time 1560216195000 ms
   [druid] 2019-06-11 09:23:15,064 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216185000 ms
   [druid] 2019-06-11 09:23:15,064 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216185000 ms
   [druid] 2019-06-11 09:23:20,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216200000 ms
   [druid] 2019-06-11 09:23:20,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216200000 ms.0 from job set of time 1560216200000 ms
   [druid] 2019-06-11 09:23:20,011 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:20,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:23:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 150 (map at Transform.scala:20)
   [druid] 2019-06-11 09:23:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 43 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 129 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 127, ShuffleMapStage 128)
   [druid] 2019-06-11 09:23:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 127)
   [druid] 2019-06-11 09:23:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 127 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:23:20,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_64 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:20,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_64_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:23:20,015 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_64_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:20,015 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 64 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:20,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 127 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:23:20,015 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 127.0 with 2 tasks
   [druid] 2019-06-11 09:23:20,015 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 127.0 (TID 85, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:20,016 [ker for task 85] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 127.0 (TID 85)
   [druid] 2019-06-11 09:23:20,025 [ker for task 85] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 127.0 (TID 85). 767 bytes result sent to driver
   [druid] 2019-06-11 09:23:20,025 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 127.0 (TID 86, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:20,025 [ker for task 86] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 127.0 (TID 86)
   [druid] 2019-06-11 09:23:20,026 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 127.0 (TID 85) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:23:20,034 [ker for task 86] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 127.0 (TID 86). 853 bytes result sent to driver
   [druid] 2019-06-11 09:23:20,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 127.0 (TID 86) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:23:20,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 127.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 127 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:23:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:23:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:23:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 129)
   [druid] 2019-06-11 09:23:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:23:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 129 (MapPartitionsRDD[155] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:20,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_65 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:20,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_65_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:20,037 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_65_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:20,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 65 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:20,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 129 (MapPartitionsRDD[155] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:23:20,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 129.0 with 1 tasks
   [druid] 2019-06-11 09:23:20,038 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 129.0 (TID 87, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:20,039 [ker for task 87] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 129.0 (TID 87)
   [druid] 2019-06-11 09:23:20,040 [ker for task 87] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:20,040 [ker for task 87] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:20,040 [ker for task 87] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:20,040 [ker for task 87] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:20,042 [ker for task 87] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 129.0 (TID 87). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:20,042 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 129.0 (TID 87) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:20,042 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 129.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:20,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 129 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:23:20,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 43 finished: print at Transform.scala:36, took 0.031671 s
   [druid] 2019-06-11 09:23:20,047 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:20,047 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 43 is 160 bytes
   [druid] 2019-06-11 09:23:20,048 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 42 is 83 bytes
   [druid] 2019-06-11 09:23:20,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 44 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:20,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 132 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:20,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 130, ShuffleMapStage 131)
   [druid] 2019-06-11 09:23:20,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:23:20,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 132 (MapPartitionsRDD[155] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:20,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_66 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:20,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_66_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:23:20,051 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_66_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:20,051 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 66 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:20,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 132 (MapPartitionsRDD[155] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:23:20,051 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 132.0 with 1 tasks
   [druid] 2019-06-11 09:23:20,052 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 132.0 (TID 88, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:20,052 [ker for task 88] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 132.0 (TID 88)
   [druid] 2019-06-11 09:23:20,053 [ker for task 88] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:20,053 [ker for task 88] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:20,053 [ker for task 88] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:20,053 [ker for task 88] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:20,053 [ker for task 88] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 132.0 (TID 88). 966 bytes result sent to driver
   [druid] 2019-06-11 09:23:20,054 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 132.0 (TID 88) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:20,054 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 132.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:20,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 132 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:23:20,054 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 44 finished: print at Transform.scala:36, took 0.007081 s
   [druid] 2019-06-11 09:23:20,055 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216200000 ms.0 from job set of time 1560216200000 ms
   [druid] 2019-06-11 09:23:20,055 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.055 s for time 1560216200000 ms (execution: 0.046 s)
   [druid] 2019-06-11 09:23:20,055 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 148 from persistence list
   [druid] 2019-06-11 09:23:20,055 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 148
   [druid] 2019-06-11 09:23:20,055 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 143 from persistence list
   [druid] 2019-06-11 09:23:20,055 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 143
   [druid] 2019-06-11 09:23:20,056 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 142 from persistence list
   [druid] 2019-06-11 09:23:20,056 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 142
   [druid] 2019-06-11 09:23:20,056 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[142] at socketTextStream at Transform.scala:18 of time 1560216200000 ms
   [druid] 2019-06-11 09:23:20,056 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216190000 ms
   [druid] 2019-06-11 09:23:20,056 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216190000 ms
   [druid] 2019-06-11 09:23:25,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216205000 ms
   [druid] 2019-06-11 09:23:25,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216205000 ms.0 from job set of time 1560216205000 ms
   [druid] 2019-06-11 09:23:25,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:23:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 157 (map at Transform.scala:20)
   [druid] 2019-06-11 09:23:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 45 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 135 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 133, ShuffleMapStage 134)
   [druid] 2019-06-11 09:23:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 133)
   [druid] 2019-06-11 09:23:25,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 133 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:23:25,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_67 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:23:25,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_67_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:23:25,013 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_67_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:25,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 67 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:25,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 133 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:23:25,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 133.0 with 2 tasks
   [druid] 2019-06-11 09:23:25,014 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 133.0 (TID 89, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:25,014 [ker for task 89] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 133.0 (TID 89)
   [druid] 2019-06-11 09:23:25,024 [ker for task 89] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 133.0 (TID 89). 810 bytes result sent to driver
   [druid] 2019-06-11 09:23:25,024 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 133.0 (TID 90, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:25,025 [ker for task 90] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 133.0 (TID 90)
   [druid] 2019-06-11 09:23:25,025 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 133.0 (TID 89) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:23:25,039 [ker for task 90] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 133.0 (TID 90). 810 bytes result sent to driver
   [druid] 2019-06-11 09:23:25,040 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 133.0 (TID 90) in 16 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:23:25,040 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 133.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 133 (parallelize at Transform.scala:16) finished in 0.026 s
   [druid] 2019-06-11 09:23:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:23:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:23:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 135)
   [druid] 2019-06-11 09:23:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:23:25,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 135 (MapPartitionsRDD[162] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:25,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_68 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:23:25,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_68_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:23:25,042 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_68_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:23:25,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 68 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:25,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 135 (MapPartitionsRDD[162] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:23:25,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 135.0 with 1 tasks
   [druid] 2019-06-11 09:23:25,043 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 135.0 (TID 91, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:25,043 [ker for task 91] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 135.0 (TID 91)
   [druid] 2019-06-11 09:23:25,044 [ker for task 91] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:25,044 [ker for task 91] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:25,045 [ker for task 91] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:25,045 [ker for task 91] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:25,045 [ker for task 91] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 135.0 (TID 91). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:25,046 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 135.0 (TID 91) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:25,046 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 135.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:25,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 135 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:23:25,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 45 finished: print at Transform.scala:36, took 0.036049 s
   [druid] 2019-06-11 09:23:25,050 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:25,051 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 45 is 160 bytes
   [druid] 2019-06-11 09:23:25,051 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 44 is 83 bytes
   [druid] 2019-06-11 09:23:25,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 46 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:25,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 138 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:25,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 136, ShuffleMapStage 137)
   [druid] 2019-06-11 09:23:25,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:23:25,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 138 (MapPartitionsRDD[162] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:25,052 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_69 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:23:25,053 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_69_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:23:25,053 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_69_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:23:25,054 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 69 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:25,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 138 (MapPartitionsRDD[162] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:23:25,054 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 138.0 with 1 tasks
   [druid] 2019-06-11 09:23:25,054 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 138.0 (TID 92, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:25,055 [ker for task 92] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 138.0 (TID 92)
   [druid] 2019-06-11 09:23:25,056 [ker for task 92] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:25,056 [ker for task 92] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:25,056 [ker for task 92] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:25,056 [ker for task 92] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:25,057 [ker for task 92] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 138.0 (TID 92). 966 bytes result sent to driver
   [druid] 2019-06-11 09:23:25,057 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 138.0 (TID 92) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:25,057 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 138.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:25,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 138 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:23:25,058 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 46 finished: print at Transform.scala:36, took 0.007015 s
   [druid] 2019-06-11 09:23:25,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216205000 ms.0 from job set of time 1560216205000 ms
   [druid] 2019-06-11 09:23:25,058 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 155 from persistence list
   [druid] 2019-06-11 09:23:25,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.058 s for time 1560216205000 ms (execution: 0.051 s)
   [druid] 2019-06-11 09:23:25,058 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 155
   [druid] 2019-06-11 09:23:25,058 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 150 from persistence list
   [druid] 2019-06-11 09:23:25,058 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 150
   [druid] 2019-06-11 09:23:25,059 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 149 from persistence list
   [druid] 2019-06-11 09:23:25,059 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 149
   [druid] 2019-06-11 09:23:25,059 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[149] at socketTextStream at Transform.scala:18 of time 1560216205000 ms
   [druid] 2019-06-11 09:23:25,059 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216195000 ms
   [druid] 2019-06-11 09:23:25,059 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216195000 ms
   [druid] 2019-06-11 09:23:30,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216210000 ms
   [druid] 2019-06-11 09:23:30,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216210000 ms.0 from job set of time 1560216210000 ms
   [druid] 2019-06-11 09:23:30,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:23:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 164 (map at Transform.scala:20)
   [druid] 2019-06-11 09:23:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 47 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 141 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 139, ShuffleMapStage 140)
   [druid] 2019-06-11 09:23:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 139)
   [druid] 2019-06-11 09:23:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 139 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:23:30,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_70 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:23:30,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_70_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:23:30,013 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_70_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:23:30,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 70 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:30,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 139 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:23:30,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 139.0 with 2 tasks
   [druid] 2019-06-11 09:23:30,014 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 139.0 (TID 93, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:30,014 [ker for task 93] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 139.0 (TID 93)
   [druid] 2019-06-11 09:23:30,024 [ker for task 93] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 139.0 (TID 93). 810 bytes result sent to driver
   [druid] 2019-06-11 09:23:30,024 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 139.0 (TID 94, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:30,024 [ker for task 94] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 139.0 (TID 94)
   [druid] 2019-06-11 09:23:30,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 139.0 (TID 93) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:23:30,033 [ker for task 94] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 139.0 (TID 94). 810 bytes result sent to driver
   [druid] 2019-06-11 09:23:30,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 139.0 (TID 94) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:23:30,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 139.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 139 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:23:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:23:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:23:30,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 141)
   [druid] 2019-06-11 09:23:30,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:23:30,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 141 (MapPartitionsRDD[169] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:30,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_71 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:23:30,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_71_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:23:30,035 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_71_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:23:30,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 71 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 141 (MapPartitionsRDD[169] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:23:30,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 141.0 with 1 tasks
   [druid] 2019-06-11 09:23:30,036 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 141.0 (TID 95, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:30,037 [ker for task 95] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 141.0 (TID 95)
   [druid] 2019-06-11 09:23:30,038 [ker for task 95] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:30,038 [ker for task 95] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:30,038 [ker for task 95] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:30,038 [ker for task 95] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:30,040 [ker for task 95] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 141.0 (TID 95). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:30,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 141.0 (TID 95) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:30,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 141.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:30,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 141 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:23:30,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 47 finished: print at Transform.scala:36, took 0.030654 s
   [druid] 2019-06-11 09:23:30,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:30,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 47 is 160 bytes
   [druid] 2019-06-11 09:23:30,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 46 is 83 bytes
   [druid] 2019-06-11 09:23:30,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 48 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:30,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 144 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:30,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 143, ShuffleMapStage 142)
   [druid] 2019-06-11 09:23:30,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:23:30,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 144 (MapPartitionsRDD[169] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:30,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_72 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:23:30,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_72_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:23:30,047 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_72_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:23:30,047 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 72 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 144 (MapPartitionsRDD[169] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:23:30,048 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 144.0 with 1 tasks
   [druid] 2019-06-11 09:23:30,048 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 144.0 (TID 96, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:30,049 [ker for task 96] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 144.0 (TID 96)
   [druid] 2019-06-11 09:23:30,050 [ker for task 96] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:30,050 [ker for task 96] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:30,050 [ker for task 96] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:30,050 [ker for task 96] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:30,051 [ker for task 96] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 144.0 (TID 96). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:30,051 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 144.0 (TID 96) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:30,051 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 144.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:30,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 144 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:23:30,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 48 finished: print at Transform.scala:36, took 0.009085 s
   [druid] 2019-06-11 09:23:30,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216210000 ms.0 from job set of time 1560216210000 ms
   [druid] 2019-06-11 09:23:30,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.052 s for time 1560216210000 ms (execution: 0.045 s)
   [druid] 2019-06-11 09:23:30,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 162 from persistence list
   [druid] 2019-06-11 09:23:30,053 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 162
   [druid] 2019-06-11 09:23:30,053 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 157 from persistence list
   [druid] 2019-06-11 09:23:30,053 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 157
   [druid] 2019-06-11 09:23:30,053 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 156 from persistence list
   [druid] 2019-06-11 09:23:30,053 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 156
   [druid] 2019-06-11 09:23:30,053 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[156] at socketTextStream at Transform.scala:18 of time 1560216210000 ms
   [druid] 2019-06-11 09:23:30,053 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216200000 ms
   [druid] 2019-06-11 09:23:30,053 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216200000 ms
   [druid] 2019-06-11 09:23:35,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216215000 ms
   [druid] 2019-06-11 09:23:35,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216215000 ms.0 from job set of time 1560216215000 ms
   [druid] 2019-06-11 09:23:35,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:35,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:23:35,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 171 (map at Transform.scala:20)
   [druid] 2019-06-11 09:23:35,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 49 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:35,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 147 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:35,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 145, ShuffleMapStage 146)
   [druid] 2019-06-11 09:23:35,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 145)
   [druid] 2019-06-11 09:23:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 145 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:23:35,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_73 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:23:35,019 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 34
   [druid] 2019-06-11 09:23:35,020 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_73_piece0 stored as bytes in memory (estimated size 1308.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:23:35,023 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_73_piece0 in memory on 192.168.91.1:63518 (size: 1308.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:23:35,024 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_60_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:23:35,024 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 73 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:35,025 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 145 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:23:35,025 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 145.0 with 2 tasks
   [druid] 2019-06-11 09:23:35,025 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_69_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:23:35,025 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 145.0 (TID 97, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:35,025 [ker for task 97] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 145.0 (TID 97)
   [druid] 2019-06-11 09:23:35,026 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_54_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:23:35,027 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_71_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:23:35,027 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 39
   [druid] 2019-06-11 09:23:35,028 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_53_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,029 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 35
   [druid] 2019-06-11 09:23:35,029 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 38
   [druid] 2019-06-11 09:23:35,030 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_62_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,032 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_58_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,034 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_65_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,035 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 40
   [druid] 2019-06-11 09:23:35,035 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 41
   [druid] 2019-06-11 09:23:35,036 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_66_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,037 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 45
   [druid] 2019-06-11 09:23:35,038 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_67_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,039 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_70_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,039 [ker for task 97] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 145.0 (TID 97). 810 bytes result sent to driver
   [druid] 2019-06-11 09:23:35,039 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 145.0 (TID 98, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:35,039 [ker for task 98] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 145.0 (TID 98)
   [druid] 2019-06-11 09:23:35,040 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 145.0 (TID 97) in 15 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:23:35,040 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_51_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,041 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_52_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,042 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_56_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,043 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 37
   [druid] 2019-06-11 09:23:35,044 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_57_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,046 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_50_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 36
   [druid] 2019-06-11 09:23:35,048 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_55_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,048 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 43
   [druid] 2019-06-11 09:23:35,049 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 30
   [druid] 2019-06-11 09:23:35,050 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_61_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,050 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_63_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 44
   [druid] 2019-06-11 09:23:35,051 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_49_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 31
   [druid] 2019-06-11 09:23:35,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 32
   [druid] 2019-06-11 09:23:35,053 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_72_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,054 [ker for task 98] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 145.0 (TID 98). 853 bytes result sent to driver
   [druid] 2019-06-11 09:23:35,054 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 145.0 (TID 98) in 15 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:23:35,054 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 145.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:35,054 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_59_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 145 (parallelize at Transform.scala:16) finished in 0.029 s
   [druid] 2019-06-11 09:23:35,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:23:35,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:23:35,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 147)
   [druid] 2019-06-11 09:23:35,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:23:35,054 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 42
   [druid] 2019-06-11 09:23:35,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 147 (MapPartitionsRDD[176] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:35,055 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_68_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,056 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 33
   [druid] 2019-06-11 09:23:35,056 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_74 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:35,056 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_64_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,057 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_74_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:35,057 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_74_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,058 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 74 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:35,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 147 (MapPartitionsRDD[176] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:23:35,058 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 147.0 with 1 tasks
   [druid] 2019-06-11 09:23:35,059 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 147.0 (TID 99, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:35,059 [ker for task 99] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 147.0 (TID 99)
   [druid] 2019-06-11 09:23:35,060 [ker for task 99] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:35,060 [ker for task 99] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:35,060 [ker for task 99] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:35,060 [ker for task 99] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:35,062 [ker for task 99] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 147.0 (TID 99). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:35,062 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 147.0 (TID 99) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:35,062 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 147.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:35,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 147 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:23:35,063 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 49 finished: print at Transform.scala:36, took 0.052808 s
   [druid] 2019-06-11 09:23:35,067 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:35,068 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 49 is 160 bytes
   [druid] 2019-06-11 09:23:35,068 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 48 is 83 bytes
   [druid] 2019-06-11 09:23:35,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 50 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:35,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 150 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:35,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 148, ShuffleMapStage 149)
   [druid] 2019-06-11 09:23:35,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:23:35,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 150 (MapPartitionsRDD[176] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:35,069 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_75 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:35,070 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_75_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:35,070 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_75_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:35,071 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 75 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:35,071 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 150 (MapPartitionsRDD[176] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:23:35,071 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 150.0 with 1 tasks
   [druid] 2019-06-11 09:23:35,072 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 150.0 (TID 100, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:35,072 [er for task 100] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 150.0 (TID 100)
   [druid] 2019-06-11 09:23:35,073 [er for task 100] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:35,073 [er for task 100] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:35,073 [er for task 100] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:35,073 [er for task 100] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:35,074 [er for task 100] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 150.0 (TID 100). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:35,075 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 150.0 (TID 100) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:35,075 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 150.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:35,075 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 150 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:23:35,075 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 50 finished: print at Transform.scala:36, took 0.007766 s
   [druid] 2019-06-11 09:23:35,075 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216215000 ms.0 from job set of time 1560216215000 ms
   [druid] 2019-06-11 09:23:35,076 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.075 s for time 1560216215000 ms (execution: 0.067 s)
   [druid] 2019-06-11 09:23:35,076 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 169 from persistence list
   [druid] 2019-06-11 09:23:35,076 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 169
   [druid] 2019-06-11 09:23:35,076 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 164 from persistence list
   [druid] 2019-06-11 09:23:35,076 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 164
   [druid] 2019-06-11 09:23:35,076 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 163 from persistence list
   [druid] 2019-06-11 09:23:35,076 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 163
   [druid] 2019-06-11 09:23:35,077 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[163] at socketTextStream at Transform.scala:18 of time 1560216215000 ms
   [druid] 2019-06-11 09:23:35,077 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216205000 ms
   [druid] 2019-06-11 09:23:35,077 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216205000 ms
   [druid] 2019-06-11 09:23:40,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216220000 ms
   [druid] 2019-06-11 09:23:40,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216220000 ms.0 from job set of time 1560216220000 ms
   [druid] 2019-06-11 09:23:40,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:23:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 178 (map at Transform.scala:20)
   [druid] 2019-06-11 09:23:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 51 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 153 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 151, ShuffleMapStage 152)
   [druid] 2019-06-11 09:23:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 151)
   [druid] 2019-06-11 09:23:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 151 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:23:40,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_76 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:40,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_76_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:23:40,012 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_76_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:40,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 76 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:40,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 151 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:23:40,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 151.0 with 2 tasks
   [druid] 2019-06-11 09:23:40,013 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 151.0 (TID 101, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:40,014 [er for task 101] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 151.0 (TID 101)
   [druid] 2019-06-11 09:23:40,024 [er for task 101] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 151.0 (TID 101). 853 bytes result sent to driver
   [druid] 2019-06-11 09:23:40,024 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 151.0 (TID 102, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:40,024 [er for task 102] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 151.0 (TID 102)
   [druid] 2019-06-11 09:23:40,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 151.0 (TID 101) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:23:40,033 [er for task 102] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 151.0 (TID 102). 810 bytes result sent to driver
   [druid] 2019-06-11 09:23:40,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 151.0 (TID 102) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:23:40,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 151.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 151 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:23:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:23:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:23:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 153)
   [druid] 2019-06-11 09:23:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:23:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 153 (MapPartitionsRDD[183] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:40,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_77 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:40,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_77_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:40,037 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_77_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:40,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 77 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:40,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 153 (MapPartitionsRDD[183] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:23:40,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 153.0 with 1 tasks
   [druid] 2019-06-11 09:23:40,038 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 153.0 (TID 103, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:40,038 [er for task 103] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 153.0 (TID 103)
   [druid] 2019-06-11 09:23:40,039 [er for task 103] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:40,039 [er for task 103] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:40,039 [er for task 103] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:40,039 [er for task 103] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:40,040 [er for task 103] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 153.0 (TID 103). 966 bytes result sent to driver
   [druid] 2019-06-11 09:23:40,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 153.0 (TID 103) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:40,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 153.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:40,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 153 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:23:40,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 51 finished: print at Transform.scala:36, took 0.031061 s
   [druid] 2019-06-11 09:23:40,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:40,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 51 is 160 bytes
   [druid] 2019-06-11 09:23:40,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 50 is 83 bytes
   [druid] 2019-06-11 09:23:40,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 52 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:40,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 156 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:40,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 154, ShuffleMapStage 155)
   [druid] 2019-06-11 09:23:40,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:23:40,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 156 (MapPartitionsRDD[183] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:40,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_78 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:40,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_78_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:40,046 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_78_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:40,047 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 78 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:40,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 156 (MapPartitionsRDD[183] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:23:40,047 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 156.0 with 1 tasks
   [druid] 2019-06-11 09:23:40,048 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 156.0 (TID 104, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:40,048 [er for task 104] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 156.0 (TID 104)
   [druid] 2019-06-11 09:23:40,049 [er for task 104] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:40,049 [er for task 104] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:40,049 [er for task 104] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:40,049 [er for task 104] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:40,050 [er for task 104] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 156.0 (TID 104). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:40,051 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 156.0 (TID 104) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:40,051 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 156.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:40,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 156 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:23:40,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 52 finished: print at Transform.scala:36, took 0.007802 s
   [druid] 2019-06-11 09:23:40,051 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216220000 ms.0 from job set of time 1560216220000 ms
   [druid] 2019-06-11 09:23:40,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.051 s for time 1560216220000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:23:40,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 176 from persistence list
   [druid] 2019-06-11 09:23:40,052 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 176
   [druid] 2019-06-11 09:23:40,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 171 from persistence list
   [druid] 2019-06-11 09:23:40,052 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 171
   [druid] 2019-06-11 09:23:40,052 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 170 from persistence list
   [druid] 2019-06-11 09:23:40,052 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 170
   [druid] 2019-06-11 09:23:40,052 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[170] at socketTextStream at Transform.scala:18 of time 1560216220000 ms
   [druid] 2019-06-11 09:23:40,052 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216210000 ms
   [druid] 2019-06-11 09:23:40,052 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216210000 ms
   [druid] 2019-06-11 09:23:45,009 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216225000 ms
   [druid] 2019-06-11 09:23:45,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216225000 ms.0 from job set of time 1560216225000 ms
   [druid] 2019-06-11 09:23:45,011 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:45,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:23:45,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 185 (map at Transform.scala:20)
   [druid] 2019-06-11 09:23:45,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 53 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:45,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 159 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:45,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 157, ShuffleMapStage 158)
   [druid] 2019-06-11 09:23:45,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 157)
   [druid] 2019-06-11 09:23:45,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 157 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:23:45,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_79 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:45,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_79_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:23:45,014 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_79_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:45,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 79 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:45,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 157 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:23:45,014 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 157.0 with 2 tasks
   [druid] 2019-06-11 09:23:45,014 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 157.0 (TID 105, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:45,015 [er for task 105] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 157.0 (TID 105)
   [druid] 2019-06-11 09:23:45,029 [er for task 105] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 157.0 (TID 105). 767 bytes result sent to driver
   [druid] 2019-06-11 09:23:45,029 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 157.0 (TID 106, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:45,029 [er for task 106] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 157.0 (TID 106)
   [druid] 2019-06-11 09:23:45,029 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 157.0 (TID 105) in 15 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:23:45,039 [er for task 106] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 157.0 (TID 106). 810 bytes result sent to driver
   [druid] 2019-06-11 09:23:45,039 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 157.0 (TID 106) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:23:45,039 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 157.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 157 (parallelize at Transform.scala:16) finished in 0.026 s
   [druid] 2019-06-11 09:23:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:23:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:23:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 159)
   [druid] 2019-06-11 09:23:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:23:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 159 (MapPartitionsRDD[190] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:45,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_80 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:45,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_80_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:45,042 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_80_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:45,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 80 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:45,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 159 (MapPartitionsRDD[190] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:23:45,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 159.0 with 1 tasks
   [druid] 2019-06-11 09:23:45,043 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 159.0 (TID 107, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:45,043 [er for task 107] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 159.0 (TID 107)
   [druid] 2019-06-11 09:23:45,044 [er for task 107] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:45,044 [er for task 107] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:45,044 [er for task 107] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:45,044 [er for task 107] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:45,045 [er for task 107] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 159.0 (TID 107). 966 bytes result sent to driver
   [druid] 2019-06-11 09:23:45,045 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 159.0 (TID 107) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:45,045 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 159.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:45,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 159 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:23:45,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 53 finished: print at Transform.scala:36, took 0.034187 s
   [druid] 2019-06-11 09:23:45,049 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:45,050 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 53 is 160 bytes
   [druid] 2019-06-11 09:23:45,050 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 52 is 83 bytes
   [druid] 2019-06-11 09:23:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 54 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 162 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 161, ShuffleMapStage 160)
   [druid] 2019-06-11 09:23:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:23:45,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 162 (MapPartitionsRDD[190] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:45,051 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_81 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:45,052 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_81_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:45,052 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_81_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:45,053 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 81 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:45,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 162 (MapPartitionsRDD[190] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:23:45,053 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 162.0 with 1 tasks
   [druid] 2019-06-11 09:23:45,053 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 162.0 (TID 108, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:45,053 [er for task 108] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 162.0 (TID 108)
   [druid] 2019-06-11 09:23:45,054 [er for task 108] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:45,054 [er for task 108] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:45,054 [er for task 108] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:45,054 [er for task 108] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:45,055 [er for task 108] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 162.0 (TID 108). 966 bytes result sent to driver
   [druid] 2019-06-11 09:23:45,055 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 162.0 (TID 108) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:45,055 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 162.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:45,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 162 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:23:45,056 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 54 finished: print at Transform.scala:36, took 0.006649 s
   [druid] 2019-06-11 09:23:45,056 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216225000 ms.0 from job set of time 1560216225000 ms
   [druid] 2019-06-11 09:23:45,056 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.056 s for time 1560216225000 ms (execution: 0.047 s)
   [druid] 2019-06-11 09:23:45,056 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 183 from persistence list
   [druid] 2019-06-11 09:23:45,057 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 183
   [druid] 2019-06-11 09:23:45,057 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 178 from persistence list
   [druid] 2019-06-11 09:23:45,057 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 178
   [druid] 2019-06-11 09:23:45,057 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 177 from persistence list
   [druid] 2019-06-11 09:23:45,057 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 177
   [druid] 2019-06-11 09:23:45,057 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[177] at socketTextStream at Transform.scala:18 of time 1560216225000 ms
   [druid] 2019-06-11 09:23:45,057 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216215000 ms
   [druid] 2019-06-11 09:23:45,057 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216215000 ms
   [druid] 2019-06-11 09:23:50,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216230000 ms
   [druid] 2019-06-11 09:23:50,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216230000 ms.0 from job set of time 1560216230000 ms
   [druid] 2019-06-11 09:23:50,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 192 (map at Transform.scala:20)
   [druid] 2019-06-11 09:23:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:23:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 55 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 165 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 163, ShuffleMapStage 164)
   [druid] 2019-06-11 09:23:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 164)
   [druid] 2019-06-11 09:23:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 164 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:23:50,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_82 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:50,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_82_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:23:50,011 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_82_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:50,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 82 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:50,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 164 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:23:50,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 164.0 with 2 tasks
   [druid] 2019-06-11 09:23:50,012 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 164.0 (TID 109, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:50,012 [er for task 109] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 164.0 (TID 109)
   [druid] 2019-06-11 09:23:50,027 [er for task 109] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 164.0 (TID 109). 853 bytes result sent to driver
   [druid] 2019-06-11 09:23:50,028 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 164.0 (TID 110, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:50,028 [er for task 110] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 164.0 (TID 110)
   [druid] 2019-06-11 09:23:50,028 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 164.0 (TID 109) in 16 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:23:50,036 [er for task 110] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 164.0 (TID 110). 810 bytes result sent to driver
   [druid] 2019-06-11 09:23:50,037 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 164.0 (TID 110) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:23:50,037 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 164.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 164 (parallelize at Transform.scala:16) finished in 0.025 s
   [druid] 2019-06-11 09:23:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:23:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:23:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 165)
   [druid] 2019-06-11 09:23:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:23:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 165 (MapPartitionsRDD[197] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:50,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_83 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:50,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_83_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:50,039 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_83_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:50,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 83 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 165 (MapPartitionsRDD[197] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:23:50,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 165.0 with 1 tasks
   [druid] 2019-06-11 09:23:50,040 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 165.0 (TID 111, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:50,040 [er for task 111] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 165.0 (TID 111)
   [druid] 2019-06-11 09:23:50,041 [er for task 111] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:50,041 [er for task 111] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:50,041 [er for task 111] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:50,041 [er for task 111] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:50,042 [er for task 111] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 165.0 (TID 111). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:50,042 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 165.0 (TID 111) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:50,042 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 165.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:50,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 165 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:23:50,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 55 finished: print at Transform.scala:36, took 0.033726 s
   [druid] 2019-06-11 09:23:50,045 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:50,045 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 54 is 83 bytes
   [druid] 2019-06-11 09:23:50,046 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 55 is 160 bytes
   [druid] 2019-06-11 09:23:50,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 56 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:50,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 168 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:50,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 166, ShuffleMapStage 167)
   [druid] 2019-06-11 09:23:50,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:23:50,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 168 (MapPartitionsRDD[197] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:50,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_84 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:50,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_84_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:50,048 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_84_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:50,048 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 84 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 168 (MapPartitionsRDD[197] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:23:50,049 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 168.0 with 1 tasks
   [druid] 2019-06-11 09:23:50,049 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 168.0 (TID 112, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:50,049 [er for task 112] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 168.0 (TID 112)
   [druid] 2019-06-11 09:23:50,050 [er for task 112] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:50,050 [er for task 112] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:50,050 [er for task 112] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:50,050 [er for task 112] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:50,051 [er for task 112] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 168.0 (TID 112). 966 bytes result sent to driver
   [druid] 2019-06-11 09:23:50,051 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 168.0 (TID 112) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:50,051 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 168.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:50,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 168 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:23:50,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 56 finished: print at Transform.scala:36, took 0.006742 s
   [druid] 2019-06-11 09:23:50,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216230000 ms.0 from job set of time 1560216230000 ms
   [druid] 2019-06-11 09:23:50,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.052 s for time 1560216230000 ms (execution: 0.045 s)
   [druid] 2019-06-11 09:23:50,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 190 from persistence list
   [druid] 2019-06-11 09:23:50,052 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 190
   [druid] 2019-06-11 09:23:50,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 185 from persistence list
   [druid] 2019-06-11 09:23:50,052 [c-thread-pool-9] INFO  che.spark.storage.BlockManager {1} - Removing RDD 185
   [druid] 2019-06-11 09:23:50,053 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 184 from persistence list
   [druid] 2019-06-11 09:23:50,053 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 184
   [druid] 2019-06-11 09:23:50,053 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[184] at socketTextStream at Transform.scala:18 of time 1560216230000 ms
   [druid] 2019-06-11 09:23:50,053 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216220000 ms
   [druid] 2019-06-11 09:23:50,053 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216220000 ms
   [druid] 2019-06-11 09:23:55,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216235000 ms
   [druid] 2019-06-11 09:23:55,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216235000 ms.0 from job set of time 1560216235000 ms
   [druid] 2019-06-11 09:23:55,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 199 (map at Transform.scala:20)
   [druid] 2019-06-11 09:23:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:23:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 57 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 171 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 169, ShuffleMapStage 170)
   [druid] 2019-06-11 09:23:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 170)
   [druid] 2019-06-11 09:23:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 170 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:23:55,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_85 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:55,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_85_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:23:55,013 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_85_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:55,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 85 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 170 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:23:55,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 170.0 with 2 tasks
   [druid] 2019-06-11 09:23:55,014 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 170.0 (TID 113, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:55,014 [er for task 113] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 170.0 (TID 113)
   [druid] 2019-06-11 09:23:55,024 [er for task 113] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 170.0 (TID 113). 853 bytes result sent to driver
   [druid] 2019-06-11 09:23:55,024 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 170.0 (TID 114, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:23:55,024 [er for task 114] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 170.0 (TID 114)
   [druid] 2019-06-11 09:23:55,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 170.0 (TID 113) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:23:55,033 [er for task 114] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 170.0 (TID 114). 853 bytes result sent to driver
   [druid] 2019-06-11 09:23:55,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 170.0 (TID 114) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:23:55,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 170.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 170 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:23:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:23:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:23:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 171)
   [druid] 2019-06-11 09:23:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:23:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 171 (MapPartitionsRDD[204] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:55,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_86 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:55,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_86_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:55,037 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_86_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:55,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 86 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 171 (MapPartitionsRDD[204] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:23:55,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 171.0 with 1 tasks
   [druid] 2019-06-11 09:23:55,038 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 171.0 (TID 115, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:55,038 [er for task 115] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 171.0 (TID 115)
   [druid] 2019-06-11 09:23:55,039 [er for task 115] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:55,039 [er for task 115] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:55,039 [er for task 115] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:55,039 [er for task 115] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:55,040 [er for task 115] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 171.0 (TID 115). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:23:55,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 171.0 (TID 115) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:55,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 171.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:55,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 171 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:23:55,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 57 finished: print at Transform.scala:36, took 0.030771 s
   [druid] 2019-06-11 09:23:55,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:23:55,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 56 is 83 bytes
   [druid] 2019-06-11 09:23:55,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 57 is 160 bytes
   [druid] 2019-06-11 09:23:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 58 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:23:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 174 (print at Transform.scala:36)
   [druid] 2019-06-11 09:23:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 172, ShuffleMapStage 173)
   [druid] 2019-06-11 09:23:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:23:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 174 (MapPartitionsRDD[204] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:23:55,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_87 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:55,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_87_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:23:55,046 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_87_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:23:55,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 87 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:23:55,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 174 (MapPartitionsRDD[204] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:23:55,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 174.0 with 1 tasks
   [druid] 2019-06-11 09:23:55,047 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 174.0 (TID 116, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:23:55,047 [er for task 116] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 174.0 (TID 116)
   [druid] 2019-06-11 09:23:55,048 [er for task 116] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:23:55,048 [er for task 116] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:55,049 [er for task 116] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:23:55,049 [er for task 116] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:23:55,050 [er for task 116] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 174.0 (TID 116). 966 bytes result sent to driver
   [druid] 2019-06-11 09:23:55,050 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 174.0 (TID 116) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:23:55,050 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 174.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:23:55,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 174 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:23:55,050 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 58 finished: print at Transform.scala:36, took 0.007024 s
   [druid] 2019-06-11 09:23:55,051 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216235000 ms.0 from job set of time 1560216235000 ms
   [druid] 2019-06-11 09:23:55,051 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.051 s for time 1560216235000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:23:55,051 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 197 from persistence list
   [druid] 2019-06-11 09:23:55,051 [c-thread-pool-9] INFO  che.spark.storage.BlockManager {1} - Removing RDD 197
   [druid] 2019-06-11 09:23:55,051 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 192 from persistence list
   [druid] 2019-06-11 09:23:55,051 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 192
   [druid] 2019-06-11 09:23:55,051 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 191 from persistence list
   [druid] 2019-06-11 09:23:55,052 [c-thread-pool-9] INFO  che.spark.storage.BlockManager {1} - Removing RDD 191
   [druid] 2019-06-11 09:23:55,052 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[191] at socketTextStream at Transform.scala:18 of time 1560216235000 ms
   [druid] 2019-06-11 09:23:55,052 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216225000 ms
   [druid] 2019-06-11 09:23:55,052 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216225000 ms
   [druid] 2019-06-11 09:24:00,010 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216240000 ms
   [druid] 2019-06-11 09:24:00,010 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216240000 ms.0 from job set of time 1560216240000 ms
   [druid] 2019-06-11 09:24:00,012 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:24:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 206 (map at Transform.scala:20)
   [druid] 2019-06-11 09:24:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 59 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 177 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 176, ShuffleMapStage 175)
   [druid] 2019-06-11 09:24:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 175)
   [druid] 2019-06-11 09:24:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 175 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:24:00,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_88 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:00,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_88_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:24:00,015 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_88_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:00,016 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 88 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:00,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 175 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:24:00,016 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 175.0 with 2 tasks
   [druid] 2019-06-11 09:24:00,016 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 175.0 (TID 117, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:00,017 [er for task 117] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 175.0 (TID 117)
   [druid] 2019-06-11 09:24:00,025 [er for task 117] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 175.0 (TID 117). 810 bytes result sent to driver
   [druid] 2019-06-11 09:24:00,026 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 175.0 (TID 118, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:00,026 [er for task 118] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 175.0 (TID 118)
   [druid] 2019-06-11 09:24:00,026 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 175.0 (TID 117) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:24:00,033 [er for task 118] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 175.0 (TID 118). 810 bytes result sent to driver
   [druid] 2019-06-11 09:24:00,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 175.0 (TID 118) in 7 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:24:00,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 175.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:00,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 175 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:24:00,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:24:00,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:24:00,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 177)
   [druid] 2019-06-11 09:24:00,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:24:00,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 177 (MapPartitionsRDD[211] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:00,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_89 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:00,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_89_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:00,036 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_89_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:00,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 89 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:00,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 177 (MapPartitionsRDD[211] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:24:00,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 177.0 with 1 tasks
   [druid] 2019-06-11 09:24:00,037 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 177.0 (TID 119, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:00,037 [er for task 119] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 177.0 (TID 119)
   [druid] 2019-06-11 09:24:00,038 [er for task 119] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:00,038 [er for task 119] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:00,039 [er for task 119] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:00,039 [er for task 119] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:00,040 [er for task 119] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 177.0 (TID 119). 966 bytes result sent to driver
   [druid] 2019-06-11 09:24:00,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 177.0 (TID 119) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:00,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 177.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 177 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:24:00,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 59 finished: print at Transform.scala:36, took 0.028352 s
   [druid] 2019-06-11 09:24:00,044 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:00,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 59 is 160 bytes
   [druid] 2019-06-11 09:24:00,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 58 is 83 bytes
   [druid] 2019-06-11 09:24:00,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 60 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:00,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 180 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:00,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 179, ShuffleMapStage 178)
   [druid] 2019-06-11 09:24:00,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:24:00,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 180 (MapPartitionsRDD[211] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:00,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_90 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:00,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_90_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:00,046 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_90_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:00,047 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 90 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:00,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 180 (MapPartitionsRDD[211] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:24:00,047 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 180.0 with 1 tasks
   [druid] 2019-06-11 09:24:00,047 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 180.0 (TID 120, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:00,047 [er for task 120] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 180.0 (TID 120)
   [druid] 2019-06-11 09:24:00,049 [er for task 120] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:00,049 [er for task 120] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:00,049 [er for task 120] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:00,049 [er for task 120] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:00,050 [er for task 120] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 180.0 (TID 120). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:24:00,051 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 180.0 (TID 120) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:00,051 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 180.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:00,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 180 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:24:00,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 60 finished: print at Transform.scala:36, took 0.007428 s
   [druid] 2019-06-11 09:24:00,051 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216240000 ms.0 from job set of time 1560216240000 ms
   [druid] 2019-06-11 09:24:00,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.051 s for time 1560216240000 ms (execution: 0.041 s)
   [druid] 2019-06-11 09:24:00,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 204 from persistence list
   [druid] 2019-06-11 09:24:00,052 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 204
   [druid] 2019-06-11 09:24:00,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 199 from persistence list
   [druid] 2019-06-11 09:24:00,052 [c-thread-pool-9] INFO  che.spark.storage.BlockManager {1} - Removing RDD 199
   [druid] 2019-06-11 09:24:00,052 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 198 from persistence list
   [druid] 2019-06-11 09:24:00,052 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 198
   [druid] 2019-06-11 09:24:00,052 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[198] at socketTextStream at Transform.scala:18 of time 1560216240000 ms
   [druid] 2019-06-11 09:24:00,052 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216230000 ms
   [druid] 2019-06-11 09:24:00,052 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216230000 ms
   [druid] 2019-06-11 09:24:05,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216245000 ms
   [druid] 2019-06-11 09:24:05,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216245000 ms.0 from job set of time 1560216245000 ms
   [druid] 2019-06-11 09:24:05,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 213 (map at Transform.scala:20)
   [druid] 2019-06-11 09:24:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:24:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 61 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 183 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 181, ShuffleMapStage 182)
   [druid] 2019-06-11 09:24:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 182)
   [druid] 2019-06-11 09:24:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 182 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:24:05,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_91 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:05,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_91_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:24:05,010 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_91_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:05,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 91 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 182 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:24:05,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 182.0 with 2 tasks
   [druid] 2019-06-11 09:24:05,011 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 182.0 (TID 121, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:05,011 [er for task 121] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 182.0 (TID 121)
   [druid] 2019-06-11 09:24:05,025 [er for task 121] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 182.0 (TID 121). 810 bytes result sent to driver
   [druid] 2019-06-11 09:24:05,026 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 182.0 (TID 122, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:05,026 [er for task 122] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 182.0 (TID 122)
   [druid] 2019-06-11 09:24:05,026 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 182.0 (TID 121) in 15 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:24:05,034 [er for task 122] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 182.0 (TID 122). 810 bytes result sent to driver
   [druid] 2019-06-11 09:24:05,035 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 182.0 (TID 122) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:24:05,035 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 182.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 182 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:24:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:24:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:24:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 183)
   [druid] 2019-06-11 09:24:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:24:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 183 (MapPartitionsRDD[218] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:05,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_92 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:05,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_92_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:05,037 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_92_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:05,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 92 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 183 (MapPartitionsRDD[218] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:24:05,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 183.0 with 1 tasks
   [druid] 2019-06-11 09:24:05,039 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 183.0 (TID 123, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:05,040 [er for task 123] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 183.0 (TID 123)
   [druid] 2019-06-11 09:24:05,041 [er for task 123] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:05,041 [er for task 123] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:05,041 [er for task 123] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:05,041 [er for task 123] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:05,042 [er for task 123] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 183.0 (TID 123). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:24:05,043 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 183.0 (TID 123) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:05,043 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 183.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:05,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 183 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:24:05,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 61 finished: print at Transform.scala:36, took 0.035251 s
   [druid] 2019-06-11 09:24:05,047 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:05,048 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 60 is 83 bytes
   [druid] 2019-06-11 09:24:05,048 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 61 is 160 bytes
   [druid] 2019-06-11 09:24:05,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 62 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:05,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 186 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:05,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 184, ShuffleMapStage 185)
   [druid] 2019-06-11 09:24:05,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:24:05,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 186 (MapPartitionsRDD[218] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:05,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_93 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:05,051 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_93_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:05,051 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_93_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:05,052 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 93 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:05,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 186 (MapPartitionsRDD[218] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:24:05,052 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 186.0 with 1 tasks
   [druid] 2019-06-11 09:24:05,052 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 186.0 (TID 124, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:05,052 [er for task 124] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 186.0 (TID 124)
   [druid] 2019-06-11 09:24:05,053 [er for task 124] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:05,053 [er for task 124] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:05,053 [er for task 124] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:05,053 [er for task 124] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:05,054 [er for task 124] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 186.0 (TID 124). 966 bytes result sent to driver
   [druid] 2019-06-11 09:24:05,054 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 186.0 (TID 124) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:05,055 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 186.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:05,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 186 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:24:05,055 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 62 finished: print at Transform.scala:36, took 0.007669 s
   [druid] 2019-06-11 09:24:05,055 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216245000 ms.0 from job set of time 1560216245000 ms
   [druid] 2019-06-11 09:24:05,055 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.055 s for time 1560216245000 ms (execution: 0.050 s)
   [druid] 2019-06-11 09:24:05,055 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 211 from persistence list
   [druid] 2019-06-11 09:24:05,056 [c-thread-pool-9] INFO  che.spark.storage.BlockManager {1} - Removing RDD 211
   [druid] 2019-06-11 09:24:05,056 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 206 from persistence list
   [druid] 2019-06-11 09:24:05,056 [c-thread-pool-6] INFO  che.spark.storage.BlockManager {1} - Removing RDD 206
   [druid] 2019-06-11 09:24:05,056 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 205 from persistence list
   [druid] 2019-06-11 09:24:05,056 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[205] at socketTextStream at Transform.scala:18 of time 1560216245000 ms
   [druid] 2019-06-11 09:24:05,056 [c-thread-pool-9] INFO  che.spark.storage.BlockManager {1} - Removing RDD 205
   [druid] 2019-06-11 09:24:05,056 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216235000 ms
   [druid] 2019-06-11 09:24:05,056 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216235000 ms
   [druid] 2019-06-11 09:24:10,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216250000 ms
   [druid] 2019-06-11 09:24:10,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216250000 ms.0 from job set of time 1560216250000 ms
   [druid] 2019-06-11 09:24:10,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:24:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 220 (map at Transform.scala:20)
   [druid] 2019-06-11 09:24:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 63 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 189 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 187, ShuffleMapStage 188)
   [druid] 2019-06-11 09:24:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 187)
   [druid] 2019-06-11 09:24:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 187 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:24:10,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_94 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:10,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_94_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:24:10,012 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_94_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:10,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 94 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:10,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 187 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:24:10,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 187.0 with 2 tasks
   [druid] 2019-06-11 09:24:10,013 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 187.0 (TID 125, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:10,013 [er for task 125] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 187.0 (TID 125)
   [druid] 2019-06-11 09:24:10,023 [er for task 125] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 187.0 (TID 125). 853 bytes result sent to driver
   [druid] 2019-06-11 09:24:10,023 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 187.0 (TID 126, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:10,023 [er for task 126] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 187.0 (TID 126)
   [druid] 2019-06-11 09:24:10,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 187.0 (TID 125) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:24:10,034 [er for task 126] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 187.0 (TID 126). 853 bytes result sent to driver
   [druid] 2019-06-11 09:24:10,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 187.0 (TID 126) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:24:10,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 187.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 187 (parallelize at Transform.scala:16) finished in 0.021 s
   [druid] 2019-06-11 09:24:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:24:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:24:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 189)
   [druid] 2019-06-11 09:24:10,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:24:10,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 189 (MapPartitionsRDD[225] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:10,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_95 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:10,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_95_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:10,037 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_95_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:10,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 95 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 189 (MapPartitionsRDD[225] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:24:10,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 189.0 with 1 tasks
   [druid] 2019-06-11 09:24:10,038 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 189.0 (TID 127, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:10,038 [er for task 127] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 189.0 (TID 127)
   [druid] 2019-06-11 09:24:10,040 [er for task 127] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:10,040 [er for task 127] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:10,040 [er for task 127] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:10,040 [er for task 127] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:10,041 [er for task 127] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 189.0 (TID 127). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:24:10,042 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 189.0 (TID 127) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:10,042 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 189.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 189 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:24:10,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 63 finished: print at Transform.scala:36, took 0.033137 s
   [druid] 2019-06-11 09:24:10,045 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:10,046 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 63 is 160 bytes
   [druid] 2019-06-11 09:24:10,046 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 62 is 83 bytes
   [druid] 2019-06-11 09:24:10,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 64 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:10,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 192 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:10,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 190, ShuffleMapStage 191)
   [druid] 2019-06-11 09:24:10,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:24:10,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 192 (MapPartitionsRDD[225] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:10,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_96 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:10,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_96_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:10,055 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_96_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:10,061 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 96 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:10,063 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_88_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:10,063 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 192 (MapPartitionsRDD[225] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:24:10,063 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 192.0 with 1 tasks
   [druid] 2019-06-11 09:24:10,064 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 192.0 (TID 128, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:10,064 [er for task 128] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 192.0 (TID 128)
   [druid] 2019-06-11 09:24:10,064 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_78_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:10,064 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 48
   [druid] 2019-06-11 09:24:10,065 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_89_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:10,066 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 54
   [druid] 2019-06-11 09:24:10,066 [er for task 128] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:10,066 [er for task 128] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:10,066 [er for task 128] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:10,066 [er for task 128] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:10,067 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_75_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:10,067 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 56
   [druid] 2019-06-11 09:24:10,068 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_86_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,068 [er for task 128] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 192.0 (TID 128). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:24:10,068 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 192.0 (TID 128) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:10,068 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 192.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:10,069 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_84_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,069 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 192 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:24:10,069 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 64 finished: print at Transform.scala:36, took 0.023085 s
   [druid] 2019-06-11 09:24:10,069 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216250000 ms.0 from job set of time 1560216250000 ms
   [druid] 2019-06-11 09:24:10,070 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 218 from persistence list
   [druid] 2019-06-11 09:24:10,070 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.069 s for time 1560216250000 ms (execution: 0.062 s)
   [druid] 2019-06-11 09:24:10,070 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 218
   [druid] 2019-06-11 09:24:10,070 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 213 from persistence list
   [druid] 2019-06-11 09:24:10,070 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_82_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,070 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 213
   [druid] 2019-06-11 09:24:10,070 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 212 from persistence list
   [druid] 2019-06-11 09:24:10,070 [c-thread-pool-9] INFO  che.spark.storage.BlockManager {1} - Removing RDD 212
   [druid] 2019-06-11 09:24:10,070 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[212] at socketTextStream at Transform.scala:18 of time 1560216250000 ms
   [druid] 2019-06-11 09:24:10,071 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216240000 ms
   [druid] 2019-06-11 09:24:10,071 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216240000 ms
   [druid] 2019-06-11 09:24:10,071 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_94_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,072 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 59
   [druid] 2019-06-11 09:24:10,072 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_85_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,073 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 51
   [druid] 2019-06-11 09:24:10,074 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 49
   [druid] 2019-06-11 09:24:10,074 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 50
   [druid] 2019-06-11 09:24:10,075 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_81_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,075 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 58
   [druid] 2019-06-11 09:24:10,076 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_73_piece0 on 192.168.91.1:63518 in memory (size: 1308.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,077 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_93_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,078 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 55
   [druid] 2019-06-11 09:24:10,079 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 53
   [druid] 2019-06-11 09:24:10,080 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_92_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,082 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_79_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,083 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_91_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,083 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 46
   [druid] 2019-06-11 09:24:10,085 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_83_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,086 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_90_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,087 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_80_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,088 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_74_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,088 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 57
   [druid] 2019-06-11 09:24:10,089 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_76_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,089 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 47
   [druid] 2019-06-11 09:24:10,090 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_77_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,091 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_87_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:10,091 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 52
   [druid] 2019-06-11 09:24:10,092 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_95_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:15,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216255000 ms
   [druid] 2019-06-11 09:24:15,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216255000 ms.0 from job set of time 1560216255000 ms
   [druid] 2019-06-11 09:24:15,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 227 (map at Transform.scala:20)
   [druid] 2019-06-11 09:24:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:24:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 65 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 195 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 194, ShuffleMapStage 193)
   [druid] 2019-06-11 09:24:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 194)
   [druid] 2019-06-11 09:24:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 194 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:24:15,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_97 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:15,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_97_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:24:15,011 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_97_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:15,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 97 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:15,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 194 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:24:15,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 194.0 with 2 tasks
   [druid] 2019-06-11 09:24:15,012 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 194.0 (TID 129, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:15,012 [er for task 129] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 194.0 (TID 129)
   [druid] 2019-06-11 09:24:15,022 [er for task 129] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 194.0 (TID 129). 853 bytes result sent to driver
   [druid] 2019-06-11 09:24:15,022 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 194.0 (TID 130, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:15,022 [er for task 130] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 194.0 (TID 130)
   [druid] 2019-06-11 09:24:15,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 194.0 (TID 129) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:24:15,030 [er for task 130] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 194.0 (TID 130). 810 bytes result sent to driver
   [druid] 2019-06-11 09:24:15,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 194.0 (TID 130) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:24:15,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 194.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 194 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:24:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:24:15,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:24:15,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 195)
   [druid] 2019-06-11 09:24:15,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:24:15,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 195 (MapPartitionsRDD[232] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:15,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_98 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:15,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_98_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:15,032 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_98_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:15,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 98 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 195 (MapPartitionsRDD[232] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:24:15,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 195.0 with 1 tasks
   [druid] 2019-06-11 09:24:15,033 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 195.0 (TID 131, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:15,034 [er for task 131] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 195.0 (TID 131)
   [druid] 2019-06-11 09:24:15,035 [er for task 131] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:15,035 [er for task 131] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:24:15,035 [er for task 131] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:15,035 [er for task 131] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:15,036 [er for task 131] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 195.0 (TID 131). 966 bytes result sent to driver
   [druid] 2019-06-11 09:24:15,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 195.0 (TID 131) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:15,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 195.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 195 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:24:15,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 65 finished: print at Transform.scala:36, took 0.027885 s
   [druid] 2019-06-11 09:24:15,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:15,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 64 is 83 bytes
   [druid] 2019-06-11 09:24:15,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 65 is 160 bytes
   [druid] 2019-06-11 09:24:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 66 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 198 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 197, ShuffleMapStage 196)
   [druid] 2019-06-11 09:24:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:24:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 198 (MapPartitionsRDD[232] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:15,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_99 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:15,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_99_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:15,042 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_99_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:15,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 99 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:15,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 198 (MapPartitionsRDD[232] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:24:15,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 198.0 with 1 tasks
   [druid] 2019-06-11 09:24:15,043 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 198.0 (TID 132, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:15,043 [er for task 132] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 198.0 (TID 132)
   [druid] 2019-06-11 09:24:15,044 [er for task 132] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:15,044 [er for task 132] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:15,044 [er for task 132] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:15,044 [er for task 132] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:15,045 [er for task 132] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 198.0 (TID 132). 966 bytes result sent to driver
   [druid] 2019-06-11 09:24:15,045 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 198.0 (TID 132) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:15,045 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 198.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:15,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 198 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:24:15,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 66 finished: print at Transform.scala:36, took 0.006234 s
   [druid] 2019-06-11 09:24:15,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216255000 ms.0 from job set of time 1560216255000 ms
   [druid] 2019-06-11 09:24:15,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.046 s for time 1560216255000 ms (execution: 0.039 s)
   [druid] 2019-06-11 09:24:15,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 225 from persistence list
   [druid] 2019-06-11 09:24:15,046 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 225
   [druid] 2019-06-11 09:24:15,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 220 from persistence list
   [druid] 2019-06-11 09:24:15,046 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 220
   [druid] 2019-06-11 09:24:15,047 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 219 from persistence list
   [druid] 2019-06-11 09:24:15,047 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 219
   [druid] 2019-06-11 09:24:15,047 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[219] at socketTextStream at Transform.scala:18 of time 1560216255000 ms
   [druid] 2019-06-11 09:24:15,047 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216245000 ms
   [druid] 2019-06-11 09:24:15,047 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216245000 ms
   [druid] 2019-06-11 09:24:20,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216260000 ms
   [druid] 2019-06-11 09:24:20,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216260000 ms.0 from job set of time 1560216260000 ms
   [druid] 2019-06-11 09:24:20,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 234 (map at Transform.scala:20)
   [druid] 2019-06-11 09:24:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:24:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 67 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 201 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 199, ShuffleMapStage 200)
   [druid] 2019-06-11 09:24:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 200)
   [druid] 2019-06-11 09:24:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 200 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:24:20,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_100 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:20,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_100_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:24:20,012 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_100_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:20,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 100 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 200 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:24:20,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 200.0 with 2 tasks
   [druid] 2019-06-11 09:24:20,013 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 200.0 (TID 133, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:20,013 [er for task 133] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 200.0 (TID 133)
   [druid] 2019-06-11 09:24:20,022 [er for task 133] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 200.0 (TID 133). 853 bytes result sent to driver
   [druid] 2019-06-11 09:24:20,023 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 200.0 (TID 134, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:20,023 [er for task 134] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 200.0 (TID 134)
   [druid] 2019-06-11 09:24:20,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 200.0 (TID 133) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:24:20,031 [er for task 134] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 200.0 (TID 134). 853 bytes result sent to driver
   [druid] 2019-06-11 09:24:20,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 200.0 (TID 134) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:24:20,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 200.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 200 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:24:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:24:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:24:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 201)
   [druid] 2019-06-11 09:24:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:24:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 201 (MapPartitionsRDD[239] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:20,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_101 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:20,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_101_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:20,034 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_101_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:20,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 101 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:20,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 201 (MapPartitionsRDD[239] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:24:20,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 201.0 with 1 tasks
   [druid] 2019-06-11 09:24:20,034 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 201.0 (TID 135, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:20,035 [er for task 135] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 201.0 (TID 135)
   [druid] 2019-06-11 09:24:20,035 [er for task 135] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:20,035 [er for task 135] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:20,035 [er for task 135] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:20,035 [er for task 135] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:20,036 [er for task 135] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 201.0 (TID 135). 966 bytes result sent to driver
   [druid] 2019-06-11 09:24:20,037 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 201.0 (TID 135) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:20,037 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 201.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 201 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:24:20,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 67 finished: print at Transform.scala:36, took 0.027660 s
   [druid] 2019-06-11 09:24:20,041 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:20,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 66 is 83 bytes
   [druid] 2019-06-11 09:24:20,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 67 is 160 bytes
   [druid] 2019-06-11 09:24:20,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 68 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:20,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 204 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:20,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 202, ShuffleMapStage 203)
   [druid] 2019-06-11 09:24:20,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:24:20,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 204 (MapPartitionsRDD[239] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:20,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_102 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:20,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_102_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:20,043 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_102_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:20,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 102 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:20,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 204 (MapPartitionsRDD[239] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:24:20,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 204.0 with 1 tasks
   [druid] 2019-06-11 09:24:20,044 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 204.0 (TID 136, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:20,045 [er for task 136] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 204.0 (TID 136)
   [druid] 2019-06-11 09:24:20,045 [er for task 136] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:20,045 [er for task 136] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:20,046 [er for task 136] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:20,046 [er for task 136] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:20,046 [er for task 136] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 204.0 (TID 136). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:24:20,047 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 204.0 (TID 136) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:20,047 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 204.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:20,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 204 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:24:20,047 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 68 finished: print at Transform.scala:36, took 0.006523 s
   [druid] 2019-06-11 09:24:20,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216260000 ms.0 from job set of time 1560216260000 ms
   [druid] 2019-06-11 09:24:20,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.048 s for time 1560216260000 ms (execution: 0.041 s)
   [druid] 2019-06-11 09:24:20,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 232 from persistence list
   [druid] 2019-06-11 09:24:20,048 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 232
   [druid] 2019-06-11 09:24:20,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 227 from persistence list
   [druid] 2019-06-11 09:24:20,049 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 227
   [druid] 2019-06-11 09:24:20,049 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 226 from persistence list
   [druid] 2019-06-11 09:24:20,049 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 226
   [druid] 2019-06-11 09:24:20,049 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[226] at socketTextStream at Transform.scala:18 of time 1560216260000 ms
   [druid] 2019-06-11 09:24:20,049 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216250000 ms
   [druid] 2019-06-11 09:24:20,049 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216250000 ms
   [druid] 2019-06-11 09:24:25,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216265000 ms
   [druid] 2019-06-11 09:24:25,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216265000 ms.0 from job set of time 1560216265000 ms
   [druid] 2019-06-11 09:24:25,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:25,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 241 (map at Transform.scala:20)
   [druid] 2019-06-11 09:24:25,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:24:25,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 69 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:25,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 207 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:25,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 205, ShuffleMapStage 206)
   [druid] 2019-06-11 09:24:25,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 206)
   [druid] 2019-06-11 09:24:25,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 206 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:24:25,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_103 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:25,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_103_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:24:25,013 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_103_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:25,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 103 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:25,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 206 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:24:25,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 206.0 with 2 tasks
   [druid] 2019-06-11 09:24:25,014 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 206.0 (TID 137, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:25,014 [er for task 137] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 206.0 (TID 137)
   [druid] 2019-06-11 09:24:25,023 [er for task 137] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 206.0 (TID 137). 853 bytes result sent to driver
   [druid] 2019-06-11 09:24:25,024 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 206.0 (TID 138, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:25,024 [er for task 138] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 206.0 (TID 138)
   [druid] 2019-06-11 09:24:25,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 206.0 (TID 137) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:24:25,032 [er for task 138] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 206.0 (TID 138). 810 bytes result sent to driver
   [druid] 2019-06-11 09:24:25,032 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 206.0 (TID 138) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:24:25,032 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 206.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 206 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:24:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:24:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:24:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 207)
   [druid] 2019-06-11 09:24:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:24:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 207 (MapPartitionsRDD[246] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:25,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_104 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:25,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_104_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:25,034 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_104_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:25,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 104 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 207 (MapPartitionsRDD[246] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:24:25,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 207.0 with 1 tasks
   [druid] 2019-06-11 09:24:25,035 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 207.0 (TID 139, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:25,035 [er for task 139] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 207.0 (TID 139)
   [druid] 2019-06-11 09:24:25,036 [er for task 139] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:25,036 [er for task 139] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:25,036 [er for task 139] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:25,036 [er for task 139] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:25,037 [er for task 139] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 207.0 (TID 139). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:24:25,037 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 207.0 (TID 139) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:25,037 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 207.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 207 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:24:25,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 69 finished: print at Transform.scala:36, took 0.028121 s
   [druid] 2019-06-11 09:24:25,041 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:25,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 68 is 83 bytes
   [druid] 2019-06-11 09:24:25,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 69 is 160 bytes
   [druid] 2019-06-11 09:24:25,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 70 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:25,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 210 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:25,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 208, ShuffleMapStage 209)
   [druid] 2019-06-11 09:24:25,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:24:25,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 210 (MapPartitionsRDD[246] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:25,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_105 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:25,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_105_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:25,044 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_105_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:25,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 105 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:25,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 210 (MapPartitionsRDD[246] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:24:25,045 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 210.0 with 1 tasks
   [druid] 2019-06-11 09:24:25,045 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 210.0 (TID 140, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:25,045 [er for task 140] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 210.0 (TID 140)
   [druid] 2019-06-11 09:24:25,046 [er for task 140] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:25,046 [er for task 140] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:25,046 [er for task 140] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:25,046 [er for task 140] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:25,047 [er for task 140] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 210.0 (TID 140). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:24:25,048 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 210.0 (TID 140) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:25,048 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 210.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:25,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 210 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:24:25,048 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 70 finished: print at Transform.scala:36, took 0.006843 s
   [druid] 2019-06-11 09:24:25,049 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216265000 ms.0 from job set of time 1560216265000 ms
   [druid] 2019-06-11 09:24:25,049 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.048 s for time 1560216265000 ms (execution: 0.041 s)
   [druid] 2019-06-11 09:24:25,049 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 239 from persistence list
   [druid] 2019-06-11 09:24:25,049 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 239
   [druid] 2019-06-11 09:24:25,049 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 234 from persistence list
   [druid] 2019-06-11 09:24:25,049 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 234
   [druid] 2019-06-11 09:24:25,049 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 233 from persistence list
   [druid] 2019-06-11 09:24:25,050 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 233
   [druid] 2019-06-11 09:24:25,050 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[233] at socketTextStream at Transform.scala:18 of time 1560216265000 ms
   [druid] 2019-06-11 09:24:25,050 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216255000 ms
   [druid] 2019-06-11 09:24:25,050 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216255000 ms
   [druid] 2019-06-11 09:24:30,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216270000 ms
   [druid] 2019-06-11 09:24:30,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216270000 ms.0 from job set of time 1560216270000 ms
   [druid] 2019-06-11 09:24:30,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:24:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 248 (map at Transform.scala:20)
   [druid] 2019-06-11 09:24:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 71 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 213 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 212, ShuffleMapStage 211)
   [druid] 2019-06-11 09:24:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 211)
   [druid] 2019-06-11 09:24:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 211 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:24:30,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_106 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:30,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_106_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:24:30,013 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_106_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:30,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 106 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:30,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 211 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:24:30,014 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 211.0 with 2 tasks
   [druid] 2019-06-11 09:24:30,014 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 211.0 (TID 141, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:30,015 [er for task 141] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 211.0 (TID 141)
   [druid] 2019-06-11 09:24:30,024 [er for task 141] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 211.0 (TID 141). 767 bytes result sent to driver
   [druid] 2019-06-11 09:24:30,024 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 211.0 (TID 142, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:30,024 [er for task 142] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 211.0 (TID 142)
   [druid] 2019-06-11 09:24:30,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 211.0 (TID 141) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:24:30,033 [er for task 142] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 211.0 (TID 142). 810 bytes result sent to driver
   [druid] 2019-06-11 09:24:30,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 211.0 (TID 142) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:24:30,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 211.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 211 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:24:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:24:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:24:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 213)
   [druid] 2019-06-11 09:24:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:24:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 213 (MapPartitionsRDD[253] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:30,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_107 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:30,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_107_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:30,035 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_107_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:30,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 107 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 213 (MapPartitionsRDD[253] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:24:30,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 213.0 with 1 tasks
   [druid] 2019-06-11 09:24:30,036 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 213.0 (TID 143, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:30,036 [er for task 143] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 213.0 (TID 143)
   [druid] 2019-06-11 09:24:30,038 [er for task 143] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:30,038 [er for task 143] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:30,038 [er for task 143] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:30,038 [er for task 143] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:30,039 [er for task 143] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 213.0 (TID 143). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:24:30,039 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 213.0 (TID 143) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:30,039 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 213.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:30,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 213 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:24:30,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 71 finished: print at Transform.scala:36, took 0.029775 s
   [druid] 2019-06-11 09:24:30,042 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:30,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 71 is 160 bytes
   [druid] 2019-06-11 09:24:30,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 70 is 83 bytes
   [druid] 2019-06-11 09:24:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 72 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 216 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 215, ShuffleMapStage 214)
   [druid] 2019-06-11 09:24:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:24:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 216 (MapPartitionsRDD[253] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:30,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_108 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:30,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_108_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:30,045 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_108_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:30,045 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 108 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:30,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 216 (MapPartitionsRDD[253] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:24:30,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 216.0 with 1 tasks
   [druid] 2019-06-11 09:24:30,046 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 216.0 (TID 144, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:30,046 [er for task 144] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 216.0 (TID 144)
   [druid] 2019-06-11 09:24:30,047 [er for task 144] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:30,047 [er for task 144] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:30,047 [er for task 144] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:30,047 [er for task 144] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:30,048 [er for task 144] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 216.0 (TID 144). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:24:30,049 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 216.0 (TID 144) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:30,049 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 216.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:30,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 216 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:24:30,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 72 finished: print at Transform.scala:36, took 0.006680 s
   [druid] 2019-06-11 09:24:30,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216270000 ms.0 from job set of time 1560216270000 ms
   [druid] 2019-06-11 09:24:30,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.050 s for time 1560216270000 ms (execution: 0.043 s)
   [druid] 2019-06-11 09:24:30,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 246 from persistence list
   [druid] 2019-06-11 09:24:30,050 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 246
   [druid] 2019-06-11 09:24:30,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 241 from persistence list
   [druid] 2019-06-11 09:24:30,050 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 241
   [druid] 2019-06-11 09:24:30,050 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 240 from persistence list
   [druid] 2019-06-11 09:24:30,051 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 240
   [druid] 2019-06-11 09:24:30,051 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[240] at socketTextStream at Transform.scala:18 of time 1560216270000 ms
   [druid] 2019-06-11 09:24:30,051 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216260000 ms
   [druid] 2019-06-11 09:24:30,051 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216260000 ms
   [druid] 2019-06-11 09:24:35,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216275000 ms
   [druid] 2019-06-11 09:24:35,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216275000 ms.0 from job set of time 1560216275000 ms
   [druid] 2019-06-11 09:24:35,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 255 (map at Transform.scala:20)
   [druid] 2019-06-11 09:24:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:24:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 73 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 219 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 217, ShuffleMapStage 218)
   [druid] 2019-06-11 09:24:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 218)
   [druid] 2019-06-11 09:24:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 218 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:24:35,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_109 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:35,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_109_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:24:35,014 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_109_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:35,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 109 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:35,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 218 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:24:35,015 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 218.0 with 2 tasks
   [druid] 2019-06-11 09:24:35,015 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 218.0 (TID 145, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:35,015 [er for task 145] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 218.0 (TID 145)
   [druid] 2019-06-11 09:24:35,025 [er for task 145] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 218.0 (TID 145). 810 bytes result sent to driver
   [druid] 2019-06-11 09:24:35,025 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 218.0 (TID 146, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:35,025 [er for task 146] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 218.0 (TID 146)
   [druid] 2019-06-11 09:24:35,025 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 218.0 (TID 145) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:24:35,034 [er for task 146] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 218.0 (TID 146). 853 bytes result sent to driver
   [druid] 2019-06-11 09:24:35,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 218.0 (TID 146) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:24:35,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 218.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 218 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:24:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:24:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:24:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 219)
   [druid] 2019-06-11 09:24:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:24:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 219 (MapPartitionsRDD[260] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:35,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_110 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:35,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_110_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:35,037 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_110_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:35,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 110 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 219 (MapPartitionsRDD[260] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:24:35,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 219.0 with 1 tasks
   [druid] 2019-06-11 09:24:35,039 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 219.0 (TID 147, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:35,039 [er for task 147] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 219.0 (TID 147)
   [druid] 2019-06-11 09:24:35,040 [er for task 147] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:35,040 [er for task 147] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:35,040 [er for task 147] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:35,040 [er for task 147] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:35,041 [er for task 147] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 219.0 (TID 147). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:24:35,041 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 219.0 (TID 147) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:35,041 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 219.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:35,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 219 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:24:35,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 73 finished: print at Transform.scala:36, took 0.031365 s
   [druid] 2019-06-11 09:24:35,044 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:35,045 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 72 is 83 bytes
   [druid] 2019-06-11 09:24:35,045 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 73 is 160 bytes
   [druid] 2019-06-11 09:24:35,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 74 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:35,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 222 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:35,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 220, ShuffleMapStage 221)
   [druid] 2019-06-11 09:24:35,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:24:35,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 222 (MapPartitionsRDD[260] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:35,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_111 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:35,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_111_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:35,049 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_111_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:35,049 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 111 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:35,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 222 (MapPartitionsRDD[260] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:24:35,049 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 222.0 with 1 tasks
   [druid] 2019-06-11 09:24:35,050 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 222.0 (TID 148, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:35,050 [er for task 148] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 222.0 (TID 148)
   [druid] 2019-06-11 09:24:35,050 [er for task 148] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:35,051 [er for task 148] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:24:35,051 [er for task 148] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:35,051 [er for task 148] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:35,051 [er for task 148] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 222.0 (TID 148). 966 bytes result sent to driver
   [druid] 2019-06-11 09:24:35,052 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 222.0 (TID 148) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:35,052 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 222.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:35,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 222 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:24:35,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 74 finished: print at Transform.scala:36, took 0.007942 s
   [druid] 2019-06-11 09:24:35,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216275000 ms.0 from job set of time 1560216275000 ms
   [druid] 2019-06-11 09:24:35,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.052 s for time 1560216275000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:24:35,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 253 from persistence list
   [druid] 2019-06-11 09:24:35,053 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 253
   [druid] 2019-06-11 09:24:35,053 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 248 from persistence list
   [druid] 2019-06-11 09:24:35,053 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 248
   [druid] 2019-06-11 09:24:35,053 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 247 from persistence list
   [druid] 2019-06-11 09:24:35,053 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 247
   [druid] 2019-06-11 09:24:35,053 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[247] at socketTextStream at Transform.scala:18 of time 1560216275000 ms
   [druid] 2019-06-11 09:24:35,053 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216265000 ms
   [druid] 2019-06-11 09:24:35,053 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216265000 ms
   [druid] 2019-06-11 09:24:40,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216280000 ms
   [druid] 2019-06-11 09:24:40,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216280000 ms.0 from job set of time 1560216280000 ms
   [druid] 2019-06-11 09:24:40,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 262 (map at Transform.scala:20)
   [druid] 2019-06-11 09:24:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:24:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 75 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 225 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 223, ShuffleMapStage 224)
   [druid] 2019-06-11 09:24:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 224)
   [druid] 2019-06-11 09:24:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 224 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:24:40,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_112 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:40,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_112_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:24:40,012 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_112_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:40,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 112 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:40,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 224 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:24:40,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 224.0 with 2 tasks
   [druid] 2019-06-11 09:24:40,013 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 224.0 (TID 149, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:40,013 [er for task 149] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 224.0 (TID 149)
   [druid] 2019-06-11 09:24:40,022 [er for task 149] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 224.0 (TID 149). 853 bytes result sent to driver
   [druid] 2019-06-11 09:24:40,023 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 224.0 (TID 150, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:40,023 [er for task 150] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 224.0 (TID 150)
   [druid] 2019-06-11 09:24:40,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 224.0 (TID 149) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:24:40,031 [er for task 150] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 224.0 (TID 150). 896 bytes result sent to driver
   [druid] 2019-06-11 09:24:40,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 224.0 (TID 150) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:24:40,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 224.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:40,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 224 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:24:40,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:24:40,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:24:40,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 225)
   [druid] 2019-06-11 09:24:40,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:24:40,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 225 (MapPartitionsRDD[267] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:40,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_113 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:40,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_113_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:40,033 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_113_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:40,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 113 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 225 (MapPartitionsRDD[267] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:24:40,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 225.0 with 1 tasks
   [druid] 2019-06-11 09:24:40,034 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 225.0 (TID 151, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:40,034 [er for task 151] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 225.0 (TID 151)
   [druid] 2019-06-11 09:24:40,035 [er for task 151] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:40,036 [er for task 151] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:24:40,036 [er for task 151] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:40,036 [er for task 151] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:40,037 [er for task 151] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 225.0 (TID 151). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:24:40,037 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 225.0 (TID 151) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:40,037 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 225.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:40,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 225 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:24:40,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 75 finished: print at Transform.scala:36, took 0.028025 s
   [druid] 2019-06-11 09:24:40,040 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:40,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 74 is 83 bytes
   [druid] 2019-06-11 09:24:40,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 75 is 160 bytes
   [druid] 2019-06-11 09:24:40,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 76 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:40,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 228 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:40,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 227, ShuffleMapStage 226)
   [druid] 2019-06-11 09:24:40,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:24:40,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 228 (MapPartitionsRDD[267] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:40,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_114 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:40,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_114_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:40,043 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_114_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:40,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 114 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:40,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 228 (MapPartitionsRDD[267] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:24:40,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 228.0 with 1 tasks
   [druid] 2019-06-11 09:24:40,043 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 228.0 (TID 152, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:40,044 [er for task 152] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 228.0 (TID 152)
   [druid] 2019-06-11 09:24:40,044 [er for task 152] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:40,044 [er for task 152] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:40,044 [er for task 152] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:40,044 [er for task 152] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:40,045 [er for task 152] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 228.0 (TID 152). 966 bytes result sent to driver
   [druid] 2019-06-11 09:24:40,045 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 228.0 (TID 152) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:40,045 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 228.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:40,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 228 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:24:40,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 76 finished: print at Transform.scala:36, took 0.005738 s
   [druid] 2019-06-11 09:24:40,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216280000 ms.0 from job set of time 1560216280000 ms
   [druid] 2019-06-11 09:24:40,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.046 s for time 1560216280000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:24:40,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 260 from persistence list
   [druid] 2019-06-11 09:24:40,046 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 260
   [druid] 2019-06-11 09:24:40,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 255 from persistence list
   [druid] 2019-06-11 09:24:40,047 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 255
   [druid] 2019-06-11 09:24:40,047 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 254 from persistence list
   [druid] 2019-06-11 09:24:40,047 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 254
   [druid] 2019-06-11 09:24:40,047 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[254] at socketTextStream at Transform.scala:18 of time 1560216280000 ms
   [druid] 2019-06-11 09:24:40,047 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216270000 ms
   [druid] 2019-06-11 09:24:40,047 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216270000 ms
   [druid] 2019-06-11 09:24:45,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216285000 ms
   [druid] 2019-06-11 09:24:45,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216285000 ms.0 from job set of time 1560216285000 ms
   [druid] 2019-06-11 09:24:45,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:45,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 269 (map at Transform.scala:20)
   [druid] 2019-06-11 09:24:45,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:24:45,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 77 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:45,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 231 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:45,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 230, ShuffleMapStage 229)
   [druid] 2019-06-11 09:24:45,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 230)
   [druid] 2019-06-11 09:24:45,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 230 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:24:45,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_115 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:45,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_115_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:24:45,012 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_115_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:45,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 115 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:45,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 230 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:24:45,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 230.0 with 2 tasks
   [druid] 2019-06-11 09:24:45,013 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 230.0 (TID 153, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:45,013 [er for task 153] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 230.0 (TID 153)
   [druid] 2019-06-11 09:24:45,022 [er for task 153] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 230.0 (TID 153). 853 bytes result sent to driver
   [druid] 2019-06-11 09:24:45,022 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 230.0 (TID 154, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:45,022 [er for task 154] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 230.0 (TID 154)
   [druid] 2019-06-11 09:24:45,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 230.0 (TID 153) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:24:45,039 [er for task 154] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 230.0 (TID 154). 896 bytes result sent to driver
   [druid] 2019-06-11 09:24:45,039 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 230.0 (TID 154) in 17 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:24:45,039 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 230.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:45,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 230 (parallelize at Transform.scala:16) finished in 0.026 s
   [druid] 2019-06-11 09:24:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:24:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:24:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 231)
   [druid] 2019-06-11 09:24:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:24:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 231 (MapPartitionsRDD[274] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:45,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_116 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:45,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_116_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:45,042 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_116_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:45,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 116 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 231 (MapPartitionsRDD[274] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:24:45,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 231.0 with 1 tasks
   [druid] 2019-06-11 09:24:45,043 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 231.0 (TID 155, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:45,043 [er for task 155] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 231.0 (TID 155)
   [druid] 2019-06-11 09:24:45,044 [er for task 155] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:45,044 [er for task 155] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:45,044 [er for task 155] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:45,044 [er for task 155] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:45,045 [er for task 155] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 231.0 (TID 155). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:24:45,045 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 231.0 (TID 155) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:45,045 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 231.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:45,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 231 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:24:45,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 77 finished: print at Transform.scala:36, took 0.035413 s
   [druid] 2019-06-11 09:24:45,049 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:45,049 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 76 is 83 bytes
   [druid] 2019-06-11 09:24:45,050 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 77 is 160 bytes
   [druid] 2019-06-11 09:24:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 78 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 234 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 233, ShuffleMapStage 232)
   [druid] 2019-06-11 09:24:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:24:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 234 (MapPartitionsRDD[274] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:45,051 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_117 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:45,052 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_117_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:45,052 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_117_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:45,052 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 117 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 234 (MapPartitionsRDD[274] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:24:45,052 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 234.0 with 1 tasks
   [druid] 2019-06-11 09:24:45,053 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 234.0 (TID 156, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:45,053 [er for task 156] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 234.0 (TID 156)
   [druid] 2019-06-11 09:24:45,054 [er for task 156] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:45,054 [er for task 156] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:45,054 [er for task 156] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:45,054 [er for task 156] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:45,054 [er for task 156] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 234.0 (TID 156). 966 bytes result sent to driver
   [druid] 2019-06-11 09:24:45,055 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 234.0 (TID 156) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:45,055 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 234.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:45,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 234 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:24:45,055 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 78 finished: print at Transform.scala:36, took 0.006080 s
   [druid] 2019-06-11 09:24:45,056 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216285000 ms.0 from job set of time 1560216285000 ms
   [druid] 2019-06-11 09:24:45,056 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.056 s for time 1560216285000 ms (execution: 0.049 s)
   [druid] 2019-06-11 09:24:45,056 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 267 from persistence list
   [druid] 2019-06-11 09:24:45,056 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 267
   [druid] 2019-06-11 09:24:45,056 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 262 from persistence list
   [druid] 2019-06-11 09:24:45,056 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 262
   [druid] 2019-06-11 09:24:45,056 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 261 from persistence list
   [druid] 2019-06-11 09:24:45,057 [c-thread-pool-9] INFO  che.spark.storage.BlockManager {1} - Removing RDD 261
   [druid] 2019-06-11 09:24:45,057 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[261] at socketTextStream at Transform.scala:18 of time 1560216285000 ms
   [druid] 2019-06-11 09:24:45,057 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216275000 ms
   [druid] 2019-06-11 09:24:45,057 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216275000 ms
   [druid] 2019-06-11 09:24:50,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216290000 ms
   [druid] 2019-06-11 09:24:50,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216290000 ms.0 from job set of time 1560216290000 ms
   [druid] 2019-06-11 09:24:50,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:50,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 276 (map at Transform.scala:20)
   [druid] 2019-06-11 09:24:50,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:24:50,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 79 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:50,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 237 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:50,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 235, ShuffleMapStage 236)
   [druid] 2019-06-11 09:24:50,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 236)
   [druid] 2019-06-11 09:24:50,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 236 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:24:50,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_118 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:50,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_118_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:24:50,012 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_118_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:50,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 118 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:50,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 236 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:24:50,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 236.0 with 2 tasks
   [druid] 2019-06-11 09:24:50,013 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 236.0 (TID 157, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:50,013 [er for task 157] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 236.0 (TID 157)
   [druid] 2019-06-11 09:24:50,022 [er for task 157] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 236.0 (TID 157). 810 bytes result sent to driver
   [druid] 2019-06-11 09:24:50,022 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 236.0 (TID 158, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:50,022 [er for task 158] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 236.0 (TID 158)
   [druid] 2019-06-11 09:24:50,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 236.0 (TID 157) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:24:50,030 [er for task 158] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 236.0 (TID 158). 810 bytes result sent to driver
   [druid] 2019-06-11 09:24:50,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 236.0 (TID 158) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:24:50,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 236.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 236 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:24:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:24:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:24:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 237)
   [druid] 2019-06-11 09:24:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:24:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 237 (MapPartitionsRDD[281] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:50,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_119 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:50,040 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 67
   [druid] 2019-06-11 09:24:50,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_119_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:24:50,041 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 62
   [druid] 2019-06-11 09:24:50,042 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_119_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:50,043 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 68
   [druid] 2019-06-11 09:24:50,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 119 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 237 (MapPartitionsRDD[281] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:24:50,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 237.0 with 1 tasks
   [druid] 2019-06-11 09:24:50,044 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_115_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:50,045 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 237.0 (TID 159, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:50,045 [er for task 159] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 237.0 (TID 159)
   [druid] 2019-06-11 09:24:50,045 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_97_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:50,046 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 70
   [druid] 2019-06-11 09:24:50,046 [er for task 159] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:50,046 [er for task 159] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:50,047 [er for task 159] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:50,047 [er for task 159] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:50,047 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_108_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:50,048 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 73
   [druid] 2019-06-11 09:24:50,048 [er for task 159] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 237.0 (TID 159). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:24:50,048 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 237.0 (TID 159) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:50,048 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_96_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:24:50,048 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 237.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:50,049 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 72
   [druid] 2019-06-11 09:24:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 237 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:24:50,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 79 finished: print at Transform.scala:36, took 0.039637 s
   [druid] 2019-06-11 09:24:50,050 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_102_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,050 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_101_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,051 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_103_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,052 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_109_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 75
   [druid] 2019-06-11 09:24:50,054 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_114_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 69
   [druid] 2019-06-11 09:24:50,056 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:50,056 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 78 is 83 bytes
   [druid] 2019-06-11 09:24:50,056 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_113_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,057 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 79 is 160 bytes
   [druid] 2019-06-11 09:24:50,058 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 65
   [druid] 2019-06-11 09:24:50,058 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 61
   [druid] 2019-06-11 09:24:50,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 80 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:50,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 240 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:50,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 238, ShuffleMapStage 239)
   [druid] 2019-06-11 09:24:50,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:24:50,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 240 (MapPartitionsRDD[281] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:50,058 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_99_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,059 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_120 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:50,060 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_105_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,060 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_120_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:50,061 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_120_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,061 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_110_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,061 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 120 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:50,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 240 (MapPartitionsRDD[281] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:24:50,062 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 240.0 with 1 tasks
   [druid] 2019-06-11 09:24:50,062 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 74
   [druid] 2019-06-11 09:24:50,062 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 240.0 (TID 160, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:50,062 [er for task 160] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 240.0 (TID 160)
   [druid] 2019-06-11 09:24:50,063 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_98_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,063 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 66
   [druid] 2019-06-11 09:24:50,063 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 71
   [druid] 2019-06-11 09:24:50,063 [er for task 160] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:50,063 [er for task 160] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:50,064 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_100_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,064 [er for task 160] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:50,064 [er for task 160] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:50,066 [er for task 160] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 240.0 (TID 160). 966 bytes result sent to driver
   [druid] 2019-06-11 09:24:50,066 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 240.0 (TID 160) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:50,066 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 240.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:50,066 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_117_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,066 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 240 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:24:50,067 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 80 finished: print at Transform.scala:36, took 0.010800 s
   [druid] 2019-06-11 09:24:50,067 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 60
   [druid] 2019-06-11 09:24:50,067 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216290000 ms.0 from job set of time 1560216290000 ms
   [druid] 2019-06-11 09:24:50,067 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.067 s for time 1560216290000 ms (execution: 0.060 s)
   [druid] 2019-06-11 09:24:50,067 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 274 from persistence list
   [druid] 2019-06-11 09:24:50,067 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 274
   [druid] 2019-06-11 09:24:50,067 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 269 from persistence list
   [druid] 2019-06-11 09:24:50,067 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_104_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,068 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 269
   [druid] 2019-06-11 09:24:50,068 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 268 from persistence list
   [druid] 2019-06-11 09:24:50,068 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 63
   [druid] 2019-06-11 09:24:50,068 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 268
   [druid] 2019-06-11 09:24:50,068 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[268] at socketTextStream at Transform.scala:18 of time 1560216290000 ms
   [druid] 2019-06-11 09:24:50,068 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216280000 ms
   [druid] 2019-06-11 09:24:50,069 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216280000 ms
   [druid] 2019-06-11 09:24:50,069 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_106_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,070 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_112_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,071 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_107_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,072 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_116_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:50,072 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 64
   [druid] 2019-06-11 09:24:50,073 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_111_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:55,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216295000 ms
   [druid] 2019-06-11 09:24:55,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216295000 ms.0 from job set of time 1560216295000 ms
   [druid] 2019-06-11 09:24:55,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:24:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 283 (map at Transform.scala:20)
   [druid] 2019-06-11 09:24:55,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 81 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:55,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 243 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:55,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 241, ShuffleMapStage 242)
   [druid] 2019-06-11 09:24:55,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 241)
   [druid] 2019-06-11 09:24:55,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 241 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:24:55,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_121 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:55,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_121_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:24:55,013 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_121_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:55,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 121 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 241 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:24:55,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 241.0 with 2 tasks
   [druid] 2019-06-11 09:24:55,013 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 241.0 (TID 161, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:55,013 [er for task 161] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 241.0 (TID 161)
   [druid] 2019-06-11 09:24:55,023 [er for task 161] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 241.0 (TID 161). 853 bytes result sent to driver
   [druid] 2019-06-11 09:24:55,023 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 241.0 (TID 162, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:24:55,024 [er for task 162] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 241.0 (TID 162)
   [druid] 2019-06-11 09:24:55,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 241.0 (TID 161) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:24:55,031 [er for task 162] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 241.0 (TID 162). 810 bytes result sent to driver
   [druid] 2019-06-11 09:24:55,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 241.0 (TID 162) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:24:55,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 241.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:55,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 241 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:24:55,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:24:55,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:24:55,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 243)
   [druid] 2019-06-11 09:24:55,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:24:55,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 243 (MapPartitionsRDD[288] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:55,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_122 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:55,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_122_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:55,033 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_122_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:55,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 122 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 243 (MapPartitionsRDD[288] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:24:55,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 243.0 with 1 tasks
   [druid] 2019-06-11 09:24:55,034 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 243.0 (TID 163, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:55,034 [er for task 163] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 243.0 (TID 163)
   [druid] 2019-06-11 09:24:55,035 [er for task 163] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:55,035 [er for task 163] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:55,035 [er for task 163] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:55,035 [er for task 163] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:55,036 [er for task 163] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 243.0 (TID 163). 966 bytes result sent to driver
   [druid] 2019-06-11 09:24:55,037 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 243.0 (TID 163) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:55,037 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 243.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 243 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:24:55,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 81 finished: print at Transform.scala:36, took 0.027634 s
   [druid] 2019-06-11 09:24:55,041 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:24:55,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 81 is 160 bytes
   [druid] 2019-06-11 09:24:55,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 80 is 83 bytes
   [druid] 2019-06-11 09:24:55,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 82 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:24:55,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 246 (print at Transform.scala:36)
   [druid] 2019-06-11 09:24:55,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 245, ShuffleMapStage 244)
   [druid] 2019-06-11 09:24:55,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:24:55,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 246 (MapPartitionsRDD[288] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:24:55,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_123 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:55,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_123_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:24:55,044 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_123_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:24:55,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 123 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:24:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 246 (MapPartitionsRDD[288] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:24:55,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 246.0 with 1 tasks
   [druid] 2019-06-11 09:24:55,044 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 246.0 (TID 164, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:24:55,045 [er for task 164] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 246.0 (TID 164)
   [druid] 2019-06-11 09:24:55,046 [er for task 164] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:24:55,046 [er for task 164] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:55,046 [er for task 164] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:24:55,046 [er for task 164] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:24:55,047 [er for task 164] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 246.0 (TID 164). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:24:55,047 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 246.0 (TID 164) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:24:55,047 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 246.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:24:55,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 246 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:24:55,048 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 82 finished: print at Transform.scala:36, took 0.006650 s
   [druid] 2019-06-11 09:24:55,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216295000 ms.0 from job set of time 1560216295000 ms
   [druid] 2019-06-11 09:24:55,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.048 s for time 1560216295000 ms (execution: 0.040 s)
   [druid] 2019-06-11 09:24:55,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 281 from persistence list
   [druid] 2019-06-11 09:24:55,048 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 281
   [druid] 2019-06-11 09:24:55,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 276 from persistence list
   [druid] 2019-06-11 09:24:55,048 [-thread-pool-14] INFO  che.spark.storage.BlockManager {1} - Removing RDD 276
   [druid] 2019-06-11 09:24:55,048 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 275 from persistence list
   [druid] 2019-06-11 09:24:55,049 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 275
   [druid] 2019-06-11 09:24:55,049 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[275] at socketTextStream at Transform.scala:18 of time 1560216295000 ms
   [druid] 2019-06-11 09:24:55,049 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216285000 ms
   [druid] 2019-06-11 09:24:55,049 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216285000 ms
   [druid] 2019-06-11 09:25:00,009 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216300000 ms
   [druid] 2019-06-11 09:25:00,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216300000 ms.0 from job set of time 1560216300000 ms
   [druid] 2019-06-11 09:25:00,011 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:25:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 290 (map at Transform.scala:20)
   [druid] 2019-06-11 09:25:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 83 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 249 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 248, ShuffleMapStage 247)
   [druid] 2019-06-11 09:25:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 247)
   [druid] 2019-06-11 09:25:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 247 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:25:00,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_124 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:00,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_124_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:25:00,014 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_124_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:00,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 124 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:00,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 247 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:25:00,014 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 247.0 with 2 tasks
   [druid] 2019-06-11 09:25:00,014 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 247.0 (TID 165, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:00,014 [er for task 165] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 247.0 (TID 165)
   [druid] 2019-06-11 09:25:00,024 [er for task 165] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 247.0 (TID 165). 853 bytes result sent to driver
   [druid] 2019-06-11 09:25:00,024 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 247.0 (TID 166, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:00,024 [er for task 166] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 247.0 (TID 166)
   [druid] 2019-06-11 09:25:00,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 247.0 (TID 165) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:25:00,033 [er for task 166] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 247.0 (TID 166). 810 bytes result sent to driver
   [druid] 2019-06-11 09:25:00,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 247.0 (TID 166) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:25:00,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 247.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 247 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:25:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:25:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:25:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 249)
   [druid] 2019-06-11 09:25:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:25:00,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 249 (MapPartitionsRDD[295] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:00,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_125 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:00,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_125_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:00,035 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_125_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:00,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 125 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:00,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 249 (MapPartitionsRDD[295] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:25:00,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 249.0 with 1 tasks
   [druid] 2019-06-11 09:25:00,036 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 249.0 (TID 167, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:00,036 [er for task 167] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 249.0 (TID 167)
   [druid] 2019-06-11 09:25:00,037 [er for task 167] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:00,037 [er for task 167] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:00,037 [er for task 167] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:00,037 [er for task 167] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:00,038 [er for task 167] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 249.0 (TID 167). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:00,038 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 249.0 (TID 167) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:00,038 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 249.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 249 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:25:00,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 83 finished: print at Transform.scala:36, took 0.027488 s
   [druid] 2019-06-11 09:25:00,042 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:00,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 83 is 160 bytes
   [druid] 2019-06-11 09:25:00,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 82 is 83 bytes
   [druid] 2019-06-11 09:25:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 84 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 252 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 251, ShuffleMapStage 250)
   [druid] 2019-06-11 09:25:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:25:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 252 (MapPartitionsRDD[295] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:00,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_126 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:00,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_126_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:00,045 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_126_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:00,045 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 126 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:00,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 252 (MapPartitionsRDD[295] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:25:00,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 252.0 with 1 tasks
   [druid] 2019-06-11 09:25:00,046 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 252.0 (TID 168, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:00,046 [er for task 168] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 252.0 (TID 168)
   [druid] 2019-06-11 09:25:00,047 [er for task 168] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:00,047 [er for task 168] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:00,047 [er for task 168] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:00,047 [er for task 168] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:00,048 [er for task 168] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 252.0 (TID 168). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:00,049 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 252.0 (TID 168) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:00,049 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 252.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:00,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 252 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:00,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 84 finished: print at Transform.scala:36, took 0.007282 s
   [druid] 2019-06-11 09:25:00,049 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216300000 ms.0 from job set of time 1560216300000 ms
   [druid] 2019-06-11 09:25:00,049 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.049 s for time 1560216300000 ms (execution: 0.040 s)
   [druid] 2019-06-11 09:25:00,049 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 288 from persistence list
   [druid] 2019-06-11 09:25:00,050 [-thread-pool-14] INFO  che.spark.storage.BlockManager {1} - Removing RDD 288
   [druid] 2019-06-11 09:25:00,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 283 from persistence list
   [druid] 2019-06-11 09:25:00,050 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 283
   [druid] 2019-06-11 09:25:00,050 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 282 from persistence list
   [druid] 2019-06-11 09:25:00,050 [-thread-pool-14] INFO  che.spark.storage.BlockManager {1} - Removing RDD 282
   [druid] 2019-06-11 09:25:00,050 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[282] at socketTextStream at Transform.scala:18 of time 1560216300000 ms
   [druid] 2019-06-11 09:25:00,050 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216290000 ms
   [druid] 2019-06-11 09:25:00,050 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216290000 ms
   [druid] 2019-06-11 09:25:05,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216305000 ms
   [druid] 2019-06-11 09:25:05,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216305000 ms.0 from job set of time 1560216305000 ms
   [druid] 2019-06-11 09:25:05,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:25:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 297 (map at Transform.scala:20)
   [druid] 2019-06-11 09:25:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 85 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 255 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 253, ShuffleMapStage 254)
   [druid] 2019-06-11 09:25:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 253)
   [druid] 2019-06-11 09:25:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 253 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:25:05,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_127 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:05,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_127_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:25:05,015 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_127_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:05,015 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 127 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:05,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 253 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:25:05,016 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 253.0 with 2 tasks
   [druid] 2019-06-11 09:25:05,017 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 253.0 (TID 169, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:05,017 [er for task 169] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 253.0 (TID 169)
   [druid] 2019-06-11 09:25:05,026 [er for task 169] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 253.0 (TID 169). 767 bytes result sent to driver
   [druid] 2019-06-11 09:25:05,026 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 253.0 (TID 170, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:05,026 [er for task 170] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 253.0 (TID 170)
   [druid] 2019-06-11 09:25:05,026 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 253.0 (TID 169) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:25:05,035 [er for task 170] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 253.0 (TID 170). 853 bytes result sent to driver
   [druid] 2019-06-11 09:25:05,036 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 253.0 (TID 170) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:25:05,036 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 253.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 253 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:25:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:25:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:25:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 255)
   [druid] 2019-06-11 09:25:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:25:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 255 (MapPartitionsRDD[302] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:05,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_128 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:05,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_128_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:05,038 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_128_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:05,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 128 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 255 (MapPartitionsRDD[302] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:25:05,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 255.0 with 1 tasks
   [druid] 2019-06-11 09:25:05,039 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 255.0 (TID 171, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:05,040 [er for task 171] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 255.0 (TID 171)
   [druid] 2019-06-11 09:25:05,040 [er for task 171] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:05,040 [er for task 171] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:05,041 [er for task 171] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:05,041 [er for task 171] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:05,042 [er for task 171] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 255.0 (TID 171). 966 bytes result sent to driver
   [druid] 2019-06-11 09:25:05,042 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 255.0 (TID 171) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:05,042 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 255.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:05,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 255 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:05,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 85 finished: print at Transform.scala:36, took 0.033033 s
   [druid] 2019-06-11 09:25:05,046 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:05,046 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 85 is 160 bytes
   [druid] 2019-06-11 09:25:05,047 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 84 is 83 bytes
   [druid] 2019-06-11 09:25:05,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 86 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:05,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 258 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:05,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 256, ShuffleMapStage 257)
   [druid] 2019-06-11 09:25:05,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:25:05,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 258 (MapPartitionsRDD[302] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:05,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_129 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:05,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_129_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:05,050 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_129_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:05,050 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 129 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:05,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 258 (MapPartitionsRDD[302] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:25:05,050 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 258.0 with 1 tasks
   [druid] 2019-06-11 09:25:05,051 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 258.0 (TID 172, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:05,051 [er for task 172] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 258.0 (TID 172)
   [druid] 2019-06-11 09:25:05,052 [er for task 172] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:05,052 [er for task 172] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:05,052 [er for task 172] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:05,052 [er for task 172] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:05,053 [er for task 172] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 258.0 (TID 172). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:05,053 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 258.0 (TID 172) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:05,053 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 258.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:05,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 258 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:05,053 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 86 finished: print at Transform.scala:36, took 0.007566 s
   [druid] 2019-06-11 09:25:05,054 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216305000 ms.0 from job set of time 1560216305000 ms
   [druid] 2019-06-11 09:25:05,054 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.054 s for time 1560216305000 ms (execution: 0.047 s)
   [druid] 2019-06-11 09:25:05,054 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 295 from persistence list
   [druid] 2019-06-11 09:25:05,054 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 295
   [druid] 2019-06-11 09:25:05,054 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 290 from persistence list
   [druid] 2019-06-11 09:25:05,054 [-thread-pool-14] INFO  che.spark.storage.BlockManager {1} - Removing RDD 290
   [druid] 2019-06-11 09:25:05,054 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 289 from persistence list
   [druid] 2019-06-11 09:25:05,054 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 289
   [druid] 2019-06-11 09:25:05,054 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[289] at socketTextStream at Transform.scala:18 of time 1560216305000 ms
   [druid] 2019-06-11 09:25:05,055 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216295000 ms
   [druid] 2019-06-11 09:25:05,055 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216295000 ms
   [druid] 2019-06-11 09:25:10,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216310000 ms
   [druid] 2019-06-11 09:25:10,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216310000 ms.0 from job set of time 1560216310000 ms
   [druid] 2019-06-11 09:25:10,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 304 (map at Transform.scala:20)
   [druid] 2019-06-11 09:25:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:25:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 87 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 261 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 259, ShuffleMapStage 260)
   [druid] 2019-06-11 09:25:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 260)
   [druid] 2019-06-11 09:25:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 260 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:25:10,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_130 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:10,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_130_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:25:10,011 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_130_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:10,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 130 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 260 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:25:10,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 260.0 with 2 tasks
   [druid] 2019-06-11 09:25:10,012 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 260.0 (TID 173, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:10,012 [er for task 173] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 260.0 (TID 173)
   [druid] 2019-06-11 09:25:10,021 [er for task 173] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 260.0 (TID 173). 853 bytes result sent to driver
   [druid] 2019-06-11 09:25:10,022 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 260.0 (TID 174, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:10,022 [er for task 174] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 260.0 (TID 174)
   [druid] 2019-06-11 09:25:10,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 260.0 (TID 173) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:25:10,030 [er for task 174] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 260.0 (TID 174). 767 bytes result sent to driver
   [druid] 2019-06-11 09:25:10,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 260.0 (TID 174) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:25:10,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 260.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 260 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:25:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:25:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:25:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 261)
   [druid] 2019-06-11 09:25:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:25:10,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 261 (MapPartitionsRDD[309] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:10,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_131 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:10,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_131_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:10,032 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_131_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:10,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 131 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 261 (MapPartitionsRDD[309] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:25:10,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 261.0 with 1 tasks
   [druid] 2019-06-11 09:25:10,033 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 261.0 (TID 175, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:10,033 [er for task 175] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 261.0 (TID 175)
   [druid] 2019-06-11 09:25:10,034 [er for task 175] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:10,034 [er for task 175] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:10,034 [er for task 175] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:10,034 [er for task 175] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:10,035 [er for task 175] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 261.0 (TID 175). 966 bytes result sent to driver
   [druid] 2019-06-11 09:25:10,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 261.0 (TID 175) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:10,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 261.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:10,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 261 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:10,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 87 finished: print at Transform.scala:36, took 0.027000 s
   [druid] 2019-06-11 09:25:10,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:10,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 86 is 83 bytes
   [druid] 2019-06-11 09:25:10,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 87 is 160 bytes
   [druid] 2019-06-11 09:25:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 88 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 264 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 263, ShuffleMapStage 262)
   [druid] 2019-06-11 09:25:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:25:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 264 (MapPartitionsRDD[309] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:10,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_132 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:10,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_132_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:10,042 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_132_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:10,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 132 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 264 (MapPartitionsRDD[309] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:25:10,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 264.0 with 1 tasks
   [druid] 2019-06-11 09:25:10,043 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 264.0 (TID 176, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:10,043 [er for task 176] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 264.0 (TID 176)
   [druid] 2019-06-11 09:25:10,044 [er for task 176] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:10,044 [er for task 176] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:10,044 [er for task 176] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:10,044 [er for task 176] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:10,044 [er for task 176] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 264.0 (TID 176). 966 bytes result sent to driver
   [druid] 2019-06-11 09:25:10,045 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 264.0 (TID 176) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:10,045 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 264.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:10,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 264 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:25:10,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 88 finished: print at Transform.scala:36, took 0.005737 s
   [druid] 2019-06-11 09:25:10,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216310000 ms.0 from job set of time 1560216310000 ms
   [druid] 2019-06-11 09:25:10,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560216310000 ms (execution: 0.039 s)
   [druid] 2019-06-11 09:25:10,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 302 from persistence list
   [druid] 2019-06-11 09:25:10,046 [-thread-pool-14] INFO  che.spark.storage.BlockManager {1} - Removing RDD 302
   [druid] 2019-06-11 09:25:10,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 297 from persistence list
   [druid] 2019-06-11 09:25:10,046 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 297
   [druid] 2019-06-11 09:25:10,046 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 296 from persistence list
   [druid] 2019-06-11 09:25:10,046 [-thread-pool-14] INFO  che.spark.storage.BlockManager {1} - Removing RDD 296
   [druid] 2019-06-11 09:25:10,046 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[296] at socketTextStream at Transform.scala:18 of time 1560216310000 ms
   [druid] 2019-06-11 09:25:10,046 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216300000 ms
   [druid] 2019-06-11 09:25:10,046 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216300000 ms
   [druid] 2019-06-11 09:25:15,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216315000 ms
   [druid] 2019-06-11 09:25:15,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216315000 ms.0 from job set of time 1560216315000 ms
   [druid] 2019-06-11 09:25:15,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:25:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 311 (map at Transform.scala:20)
   [druid] 2019-06-11 09:25:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 89 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 267 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 266, ShuffleMapStage 265)
   [druid] 2019-06-11 09:25:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 265)
   [druid] 2019-06-11 09:25:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 265 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:25:15,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_133 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:15,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_133_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:25:15,011 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_133_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:15,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 133 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:15,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 265 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:25:15,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 265.0 with 2 tasks
   [druid] 2019-06-11 09:25:15,011 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 265.0 (TID 177, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:15,011 [er for task 177] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 265.0 (TID 177)
   [druid] 2019-06-11 09:25:15,020 [er for task 177] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 265.0 (TID 177). 853 bytes result sent to driver
   [druid] 2019-06-11 09:25:15,021 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 265.0 (TID 178, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:15,021 [er for task 178] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 265.0 (TID 178)
   [druid] 2019-06-11 09:25:15,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 265.0 (TID 177) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:25:15,037 [er for task 178] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 265.0 (TID 178). 810 bytes result sent to driver
   [druid] 2019-06-11 09:25:15,037 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 265.0 (TID 178) in 16 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:25:15,037 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 265.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 265 (parallelize at Transform.scala:16) finished in 0.027 s
   [druid] 2019-06-11 09:25:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:25:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:25:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 267)
   [druid] 2019-06-11 09:25:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:25:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 267 (MapPartitionsRDD[316] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:15,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_134 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:15,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_134_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:15,039 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_134_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:15,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 134 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 267 (MapPartitionsRDD[316] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:25:15,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 267.0 with 1 tasks
   [druid] 2019-06-11 09:25:15,040 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 267.0 (TID 179, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:15,040 [er for task 179] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 267.0 (TID 179)
   [druid] 2019-06-11 09:25:15,041 [er for task 179] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:15,041 [er for task 179] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:15,041 [er for task 179] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:15,041 [er for task 179] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:15,042 [er for task 179] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 267.0 (TID 179). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:15,042 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 267.0 (TID 179) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:15,043 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 267.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:15,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 267 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:15,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 89 finished: print at Transform.scala:36, took 0.034528 s
   [druid] 2019-06-11 09:25:15,045 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:15,046 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 89 is 160 bytes
   [druid] 2019-06-11 09:25:15,046 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 88 is 83 bytes
   [druid] 2019-06-11 09:25:15,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 90 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:15,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 270 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:15,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 269, ShuffleMapStage 268)
   [druid] 2019-06-11 09:25:15,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:25:15,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 270 (MapPartitionsRDD[316] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:15,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_135 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:15,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_135_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:15,048 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_135_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:15,048 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 135 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:15,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 270 (MapPartitionsRDD[316] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:25:15,049 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 270.0 with 1 tasks
   [druid] 2019-06-11 09:25:15,049 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 270.0 (TID 180, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:15,049 [er for task 180] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 270.0 (TID 180)
   [druid] 2019-06-11 09:25:15,050 [er for task 180] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:15,050 [er for task 180] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:15,050 [er for task 180] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:15,050 [er for task 180] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:15,051 [er for task 180] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 270.0 (TID 180). 966 bytes result sent to driver
   [druid] 2019-06-11 09:25:15,051 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 270.0 (TID 180) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:15,051 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 270.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:15,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 270 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:15,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 90 finished: print at Transform.scala:36, took 0.006975 s
   [druid] 2019-06-11 09:25:15,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216315000 ms.0 from job set of time 1560216315000 ms
   [druid] 2019-06-11 09:25:15,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.052 s for time 1560216315000 ms (execution: 0.046 s)
   [druid] 2019-06-11 09:25:15,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 309 from persistence list
   [druid] 2019-06-11 09:25:15,052 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 309
   [druid] 2019-06-11 09:25:15,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 304 from persistence list
   [druid] 2019-06-11 09:25:15,053 [-thread-pool-14] INFO  che.spark.storage.BlockManager {1} - Removing RDD 304
   [druid] 2019-06-11 09:25:15,053 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 303 from persistence list
   [druid] 2019-06-11 09:25:15,053 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 303
   [druid] 2019-06-11 09:25:15,053 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[303] at socketTextStream at Transform.scala:18 of time 1560216315000 ms
   [druid] 2019-06-11 09:25:15,053 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216305000 ms
   [druid] 2019-06-11 09:25:15,053 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216305000 ms
   [druid] 2019-06-11 09:25:20,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216320000 ms
   [druid] 2019-06-11 09:25:20,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216320000 ms.0 from job set of time 1560216320000 ms
   [druid] 2019-06-11 09:25:20,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 318 (map at Transform.scala:20)
   [druid] 2019-06-11 09:25:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:25:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 91 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 273 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 271, ShuffleMapStage 272)
   [druid] 2019-06-11 09:25:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 272)
   [druid] 2019-06-11 09:25:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 272 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:25:20,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_136 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:20,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_136_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:25:20,011 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_136_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:20,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 136 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 272 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:25:20,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 272.0 with 2 tasks
   [druid] 2019-06-11 09:25:20,012 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 272.0 (TID 181, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:20,012 [er for task 181] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 272.0 (TID 181)
   [druid] 2019-06-11 09:25:20,021 [er for task 181] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 272.0 (TID 181). 853 bytes result sent to driver
   [druid] 2019-06-11 09:25:20,021 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 272.0 (TID 182, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:20,022 [er for task 182] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 272.0 (TID 182)
   [druid] 2019-06-11 09:25:20,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 272.0 (TID 181) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:25:20,029 [er for task 182] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 272.0 (TID 182). 767 bytes result sent to driver
   [druid] 2019-06-11 09:25:20,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 272.0 (TID 182) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:25:20,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 272.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 272 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:25:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:25:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:25:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 273)
   [druid] 2019-06-11 09:25:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:25:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 273 (MapPartitionsRDD[323] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:20,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_137 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:20,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_137_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:20,032 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_137_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:20,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 137 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 273 (MapPartitionsRDD[323] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:25:20,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 273.0 with 1 tasks
   [druid] 2019-06-11 09:25:20,033 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 273.0 (TID 183, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:20,033 [er for task 183] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 273.0 (TID 183)
   [druid] 2019-06-11 09:25:20,034 [er for task 183] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:20,034 [er for task 183] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:20,034 [er for task 183] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:20,034 [er for task 183] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:20,035 [er for task 183] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 273.0 (TID 183). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:20,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 273.0 (TID 183) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:20,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 273.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 273 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:20,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 91 finished: print at Transform.scala:36, took 0.026314 s
   [druid] 2019-06-11 09:25:20,038 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:20,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 90 is 83 bytes
   [druid] 2019-06-11 09:25:20,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 91 is 160 bytes
   [druid] 2019-06-11 09:25:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 92 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 276 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 274, ShuffleMapStage 275)
   [druid] 2019-06-11 09:25:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:25:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 276 (MapPartitionsRDD[323] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:20,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_138 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:20,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_138_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:20,041 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_138_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:20,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 138 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:20,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 276 (MapPartitionsRDD[323] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:25:20,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 276.0 with 1 tasks
   [druid] 2019-06-11 09:25:20,042 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 276.0 (TID 184, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:20,042 [er for task 184] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 276.0 (TID 184)
   [druid] 2019-06-11 09:25:20,043 [er for task 184] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:20,043 [er for task 184] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:20,043 [er for task 184] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:20,043 [er for task 184] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:20,044 [er for task 184] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 276.0 (TID 184). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:20,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 276.0 (TID 184) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:20,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 276.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:20,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 276 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:25:20,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 92 finished: print at Transform.scala:36, took 0.006565 s
   [druid] 2019-06-11 09:25:20,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216320000 ms.0 from job set of time 1560216320000 ms
   [druid] 2019-06-11 09:25:20,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560216320000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:25:20,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 316 from persistence list
   [druid] 2019-06-11 09:25:20,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 311 from persistence list
   [druid] 2019-06-11 09:25:20,045 [-thread-pool-14] INFO  che.spark.storage.BlockManager {1} - Removing RDD 316
   [druid] 2019-06-11 09:25:20,045 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 311
   [druid] 2019-06-11 09:25:20,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 310 from persistence list
   [druid] 2019-06-11 09:25:20,046 [-thread-pool-14] INFO  che.spark.storage.BlockManager {1} - Removing RDD 310
   [druid] 2019-06-11 09:25:20,046 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[310] at socketTextStream at Transform.scala:18 of time 1560216320000 ms
   [druid] 2019-06-11 09:25:20,046 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216310000 ms
   [druid] 2019-06-11 09:25:20,046 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216310000 ms
   [druid] 2019-06-11 09:25:25,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216325000 ms
   [druid] 2019-06-11 09:25:25,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216325000 ms.0 from job set of time 1560216325000 ms
   [druid] 2019-06-11 09:25:25,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:25,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 325 (map at Transform.scala:20)
   [druid] 2019-06-11 09:25:25,116 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:25:25,116 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 93 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:25,116 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 279 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:25,116 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 277, ShuffleMapStage 278)
   [druid] 2019-06-11 09:25:25,116 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 278)
   [druid] 2019-06-11 09:25:25,116 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 278 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:25:25,117 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_139 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:25,117 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_139_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:25:25,118 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_139_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:25,118 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 139 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:25,118 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 278 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:25:25,118 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 278.0 with 2 tasks
   [druid] 2019-06-11 09:25:25,119 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 278.0 (TID 185, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:25,119 [er for task 185] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 278.0 (TID 185)
   [druid] 2019-06-11 09:25:25,127 [er for task 185] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 278.0 (TID 185). 810 bytes result sent to driver
   [druid] 2019-06-11 09:25:25,127 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 278.0 (TID 186, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:25,127 [er for task 186] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 278.0 (TID 186)
   [druid] 2019-06-11 09:25:25,127 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 278.0 (TID 185) in 8 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:25:25,136 [er for task 186] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 278.0 (TID 186). 810 bytes result sent to driver
   [druid] 2019-06-11 09:25:25,137 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 278.0 (TID 186) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:25:25,137 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 278.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:25,137 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 278 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:25:25,137 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:25:25,137 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:25:25,137 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 279)
   [druid] 2019-06-11 09:25:25,137 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:25:25,137 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 279 (MapPartitionsRDD[330] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:25,138 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_140 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:25,139 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_140_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:25,139 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_140_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:25,139 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 140 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:25,139 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 279 (MapPartitionsRDD[330] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:25:25,139 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 279.0 with 1 tasks
   [druid] 2019-06-11 09:25:25,140 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 279.0 (TID 187, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:25,140 [er for task 187] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 279.0 (TID 187)
   [druid] 2019-06-11 09:25:25,141 [er for task 187] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:25,141 [er for task 187] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:25,141 [er for task 187] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:25,141 [er for task 187] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:25,142 [er for task 187] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 279.0 (TID 187). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:25,142 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 279.0 (TID 187) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:25,142 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 279.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:25,142 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 279 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:25,143 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 93 finished: print at Transform.scala:36, took 0.135357 s
   [druid] 2019-06-11 09:25:25,145 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:25,145 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 92 is 83 bytes
   [druid] 2019-06-11 09:25:25,147 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 93 is 160 bytes
   [druid] 2019-06-11 09:25:25,147 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 94 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:25,147 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 282 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:25,147 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 281, ShuffleMapStage 280)
   [druid] 2019-06-11 09:25:25,147 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:25:25,147 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 282 (MapPartitionsRDD[330] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:25,148 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_141 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:25,155 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_141_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:25,156 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_141_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:25,158 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_122_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:25,159 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 141 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:25,160 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 282 (MapPartitionsRDD[330] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:25:25,160 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 282.0 with 1 tasks
   [druid] 2019-06-11 09:25:25,161 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_119_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:25,161 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 282.0 (TID 188, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:25,161 [er for task 188] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 282.0 (TID 188)
   [druid] 2019-06-11 09:25:25,161 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_124_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:25,162 [er for task 188] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:25,162 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 81
   [druid] 2019-06-11 09:25:25,162 [er for task 188] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:25,162 [er for task 188] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:25,162 [er for task 188] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:25,163 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_126_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:25,163 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 89
   [druid] 2019-06-11 09:25:25,164 [er for task 188] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 282.0 (TID 188). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:25,164 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 282.0 (TID 188) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:25,164 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 282.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:25,164 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_121_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:25,165 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 282 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:25:25,165 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 94 finished: print at Transform.scala:36, took 0.019901 s
   [druid] 2019-06-11 09:25:25,165 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216325000 ms.0 from job set of time 1560216325000 ms
   [druid] 2019-06-11 09:25:25,165 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.165 s for time 1560216325000 ms (execution: 0.160 s)
   [druid] 2019-06-11 09:25:25,165 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 323 from persistence list
   [druid] 2019-06-11 09:25:25,165 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_134_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:25,166 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 323
   [druid] 2019-06-11 09:25:25,166 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 318 from persistence list
   [druid] 2019-06-11 09:25:25,166 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 318
   [druid] 2019-06-11 09:25:25,166 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 317 from persistence list
   [druid] 2019-06-11 09:25:25,166 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 317
   [druid] 2019-06-11 09:25:25,166 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[317] at socketTextStream at Transform.scala:18 of time 1560216325000 ms
   [druid] 2019-06-11 09:25:25,166 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_133_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:25,166 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216315000 ms
   [druid] 2019-06-11 09:25:25,166 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216315000 ms
   [druid] 2019-06-11 09:25:25,168 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_123_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:25,168 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 77
   [druid] 2019-06-11 09:25:25,168 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 76
   [druid] 2019-06-11 09:25:25,169 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_128_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:25,170 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 82
   [druid] 2019-06-11 09:25:25,170 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 88
   [druid] 2019-06-11 09:25:25,171 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_130_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:25,171 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_120_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:25,172 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_135_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:25,173 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_118_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:30,014 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216330000 ms
   [druid] 2019-06-11 09:25:30,014 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216330000 ms.0 from job set of time 1560216330000 ms
   [druid] 2019-06-11 09:25:30,017 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:30,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:25:30,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 332 (map at Transform.scala:20)
   [druid] 2019-06-11 09:25:30,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 95 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:30,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 285 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:30,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 284, ShuffleMapStage 283)
   [druid] 2019-06-11 09:25:30,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 283)
   [druid] 2019-06-11 09:25:30,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 283 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:25:30,018 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_142 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:30,019 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_142_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:25:30,019 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_142_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:30,019 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 142 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:30,020 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 283 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:25:30,020 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 283.0 with 2 tasks
   [druid] 2019-06-11 09:25:30,020 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 283.0 (TID 189, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:30,020 [er for task 189] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 283.0 (TID 189)
   [druid] 2019-06-11 09:25:30,028 [er for task 189] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 283.0 (TID 189). 810 bytes result sent to driver
   [druid] 2019-06-11 09:25:30,028 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 283.0 (TID 190, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:30,028 [er for task 190] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 283.0 (TID 190)
   [druid] 2019-06-11 09:25:30,028 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 283.0 (TID 189) in 8 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:25:30,037 [er for task 190] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 283.0 (TID 190). 853 bytes result sent to driver
   [druid] 2019-06-11 09:25:30,037 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 283.0 (TID 190) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:25:30,037 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 283.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 283 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:25:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:25:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:25:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 285)
   [druid] 2019-06-11 09:25:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:25:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 285 (MapPartitionsRDD[337] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:30,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_143 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:30,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_143_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:30,040 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_143_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:30,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 143 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:30,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 285 (MapPartitionsRDD[337] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:25:30,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 285.0 with 1 tasks
   [druid] 2019-06-11 09:25:30,041 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 285.0 (TID 191, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:30,041 [er for task 191] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 285.0 (TID 191)
   [druid] 2019-06-11 09:25:30,042 [er for task 191] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:30,042 [er for task 191] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:30,042 [er for task 191] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:30,042 [er for task 191] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:30,043 [er for task 191] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 285.0 (TID 191). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:30,044 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 285.0 (TID 191) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:30,044 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 285.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:30,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 285 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:30,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 95 finished: print at Transform.scala:36, took 0.027364 s
   [druid] 2019-06-11 09:25:30,047 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:30,048 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 95 is 160 bytes
   [druid] 2019-06-11 09:25:30,048 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 94 is 83 bytes
   [druid] 2019-06-11 09:25:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 96 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 288 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 287, ShuffleMapStage 286)
   [druid] 2019-06-11 09:25:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:25:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 288 (MapPartitionsRDD[337] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:30,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_144 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:30,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_144_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:30,050 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_144_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:30,050 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 144 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:30,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 288 (MapPartitionsRDD[337] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:25:30,051 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 288.0 with 1 tasks
   [druid] 2019-06-11 09:25:30,051 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 288.0 (TID 192, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:30,051 [er for task 192] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 288.0 (TID 192)
   [druid] 2019-06-11 09:25:30,052 [er for task 192] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:30,052 [er for task 192] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:30,052 [er for task 192] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:30,052 [er for task 192] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:30,054 [er for task 192] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 288.0 (TID 192). 966 bytes result sent to driver
   [druid] 2019-06-11 09:25:30,054 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 288.0 (TID 192) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:30,054 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 288.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:30,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 288 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:30,055 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 96 finished: print at Transform.scala:36, took 0.007208 s
   [druid] 2019-06-11 09:25:30,055 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216330000 ms.0 from job set of time 1560216330000 ms
   [druid] 2019-06-11 09:25:30,055 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.055 s for time 1560216330000 ms (execution: 0.041 s)
   [druid] 2019-06-11 09:25:30,055 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 330 from persistence list
   [druid] 2019-06-11 09:25:30,055 [-thread-pool-13] INFO  che.spark.storage.BlockManager {1} - Removing RDD 330
   [druid] 2019-06-11 09:25:30,055 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 325 from persistence list
   [druid] 2019-06-11 09:25:30,055 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 325
   [druid] 2019-06-11 09:25:30,055 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 324 from persistence list
   [druid] 2019-06-11 09:25:30,056 [-thread-pool-13] INFO  che.spark.storage.BlockManager {1} - Removing RDD 324
   [druid] 2019-06-11 09:25:30,056 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[324] at socketTextStream at Transform.scala:18 of time 1560216330000 ms
   [druid] 2019-06-11 09:25:30,056 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216320000 ms
   [druid] 2019-06-11 09:25:30,056 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216320000 ms
   [druid] 2019-06-11 09:25:35,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216335000 ms
   [druid] 2019-06-11 09:25:35,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216335000 ms.0 from job set of time 1560216335000 ms
   [druid] 2019-06-11 09:25:35,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 339 (map at Transform.scala:20)
   [druid] 2019-06-11 09:25:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:25:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 97 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 291 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 289, ShuffleMapStage 290)
   [druid] 2019-06-11 09:25:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 290)
   [druid] 2019-06-11 09:25:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 290 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:25:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_145 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:35,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_145_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:25:35,011 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_145_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:35,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 145 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 290 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:25:35,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 290.0 with 2 tasks
   [druid] 2019-06-11 09:25:35,011 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 290.0 (TID 193, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:35,012 [er for task 193] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 290.0 (TID 193)
   [druid] 2019-06-11 09:25:35,023 [er for task 193] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 290.0 (TID 193). 810 bytes result sent to driver
   [druid] 2019-06-11 09:25:35,023 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 290.0 (TID 194, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:35,024 [er for task 194] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 290.0 (TID 194)
   [druid] 2019-06-11 09:25:35,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 290.0 (TID 193) in 13 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:25:35,032 [er for task 194] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 290.0 (TID 194). 810 bytes result sent to driver
   [druid] 2019-06-11 09:25:35,032 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 290.0 (TID 194) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:25:35,032 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 290.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:35,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 290 (parallelize at Transform.scala:16) finished in 0.022 s
   [druid] 2019-06-11 09:25:35,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:25:35,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:25:35,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 291)
   [druid] 2019-06-11 09:25:35,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:25:35,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 291 (MapPartitionsRDD[344] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:35,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_146 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:35,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_146_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:35,034 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_146_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:35,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 146 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 291 (MapPartitionsRDD[344] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:25:35,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 291.0 with 1 tasks
   [druid] 2019-06-11 09:25:35,035 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 291.0 (TID 195, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:35,035 [er for task 195] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 291.0 (TID 195)
   [druid] 2019-06-11 09:25:35,036 [er for task 195] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:35,036 [er for task 195] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:35,036 [er for task 195] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:35,036 [er for task 195] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:35,037 [er for task 195] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 291.0 (TID 195). 966 bytes result sent to driver
   [druid] 2019-06-11 09:25:35,037 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 291.0 (TID 195) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:35,037 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 291.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 291 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:25:35,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 97 finished: print at Transform.scala:36, took 0.029155 s
   [druid] 2019-06-11 09:25:35,040 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:35,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 96 is 83 bytes
   [druid] 2019-06-11 09:25:35,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 97 is 160 bytes
   [druid] 2019-06-11 09:25:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 98 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:35,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 294 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:35,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 292, ShuffleMapStage 293)
   [druid] 2019-06-11 09:25:35,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:25:35,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 294 (MapPartitionsRDD[344] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:35,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_147 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:35,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_147_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:25:35,042 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_147_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:35,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 147 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:35,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 294 (MapPartitionsRDD[344] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:25:35,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 294.0 with 1 tasks
   [druid] 2019-06-11 09:25:35,042 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 294.0 (TID 196, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:35,043 [er for task 196] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 294.0 (TID 196)
   [druid] 2019-06-11 09:25:35,043 [er for task 196] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:35,043 [er for task 196] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:35,044 [er for task 196] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:35,044 [er for task 196] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:35,044 [er for task 196] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 294.0 (TID 196). 966 bytes result sent to driver
   [druid] 2019-06-11 09:25:35,045 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 294.0 (TID 196) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:35,045 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 294.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:35,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 294 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:35,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 98 finished: print at Transform.scala:36, took 0.005629 s
   [druid] 2019-06-11 09:25:35,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216335000 ms.0 from job set of time 1560216335000 ms
   [druid] 2019-06-11 09:25:35,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560216335000 ms (execution: 0.039 s)
   [druid] 2019-06-11 09:25:35,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 337 from persistence list
   [druid] 2019-06-11 09:25:35,046 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 337
   [druid] 2019-06-11 09:25:35,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 332 from persistence list
   [druid] 2019-06-11 09:25:35,046 [-thread-pool-13] INFO  che.spark.storage.BlockManager {1} - Removing RDD 332
   [druid] 2019-06-11 09:25:35,046 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 331 from persistence list
   [druid] 2019-06-11 09:25:35,046 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 331
   [druid] 2019-06-11 09:25:35,046 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[331] at socketTextStream at Transform.scala:18 of time 1560216335000 ms
   [druid] 2019-06-11 09:25:35,046 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216325000 ms
   [druid] 2019-06-11 09:25:35,046 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216325000 ms
   [druid] 2019-06-11 09:25:40,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216340000 ms
   [druid] 2019-06-11 09:25:40,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216340000 ms.0 from job set of time 1560216340000 ms
   [druid] 2019-06-11 09:25:40,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 346 (map at Transform.scala:20)
   [druid] 2019-06-11 09:25:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:25:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 99 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 297 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 296, ShuffleMapStage 295)
   [druid] 2019-06-11 09:25:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 296)
   [druid] 2019-06-11 09:25:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 296 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:25:40,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_148 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:40,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_148_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:25:40,011 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_148_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:40,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 148 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:40,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 296 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:25:40,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 296.0 with 2 tasks
   [druid] 2019-06-11 09:25:40,012 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 296.0 (TID 197, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:40,012 [er for task 197] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 296.0 (TID 197)
   [druid] 2019-06-11 09:25:40,022 [er for task 197] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 296.0 (TID 197). 853 bytes result sent to driver
   [druid] 2019-06-11 09:25:40,022 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 296.0 (TID 198, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:40,022 [er for task 198] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 296.0 (TID 198)
   [druid] 2019-06-11 09:25:40,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 296.0 (TID 197) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:25:40,030 [er for task 198] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 296.0 (TID 198). 810 bytes result sent to driver
   [druid] 2019-06-11 09:25:40,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 296.0 (TID 198) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:25:40,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 296.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 296 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:25:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:25:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:25:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 297)
   [druid] 2019-06-11 09:25:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:25:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 297 (MapPartitionsRDD[351] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:40,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_149 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:40,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_149_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:40,032 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_149_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:25:40,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 149 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 297 (MapPartitionsRDD[351] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:25:40,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 297.0 with 1 tasks
   [druid] 2019-06-11 09:25:40,033 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 297.0 (TID 199, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:40,033 [er for task 199] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 297.0 (TID 199)
   [druid] 2019-06-11 09:25:40,034 [er for task 199] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:40,034 [er for task 199] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:40,034 [er for task 199] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:40,034 [er for task 199] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:40,036 [er for task 199] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 297.0 (TID 199). 966 bytes result sent to driver
   [druid] 2019-06-11 09:25:40,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 297.0 (TID 199) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:40,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 297.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 297 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:25:40,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 99 finished: print at Transform.scala:36, took 0.027534 s
   [druid] 2019-06-11 09:25:40,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:40,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 98 is 83 bytes
   [druid] 2019-06-11 09:25:40,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 99 is 160 bytes
   [druid] 2019-06-11 09:25:40,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 100 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:40,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 300 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:40,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 299, ShuffleMapStage 298)
   [druid] 2019-06-11 09:25:40,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:25:40,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 300 (MapPartitionsRDD[351] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:40,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_150 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:40,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_150_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:40,042 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_150_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:40,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 150 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:40,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 300 (MapPartitionsRDD[351] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:25:40,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 300.0 with 1 tasks
   [druid] 2019-06-11 09:25:40,043 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 300.0 (TID 200, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:40,043 [er for task 200] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 300.0 (TID 200)
   [druid] 2019-06-11 09:25:40,044 [er for task 200] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:40,044 [er for task 200] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:40,044 [er for task 200] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:40,044 [er for task 200] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:40,045 [er for task 200] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 300.0 (TID 200). 966 bytes result sent to driver
   [druid] 2019-06-11 09:25:40,046 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 300.0 (TID 200) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:40,046 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 300.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:40,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 300 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:40,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 100 finished: print at Transform.scala:36, took 0.006800 s
   [druid] 2019-06-11 09:25:40,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216340000 ms.0 from job set of time 1560216340000 ms
   [druid] 2019-06-11 09:25:40,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.046 s for time 1560216340000 ms (execution: 0.039 s)
   [druid] 2019-06-11 09:25:40,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 344 from persistence list
   [druid] 2019-06-11 09:25:40,047 [-thread-pool-13] INFO  che.spark.storage.BlockManager {1} - Removing RDD 344
   [druid] 2019-06-11 09:25:40,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 339 from persistence list
   [druid] 2019-06-11 09:25:40,047 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 339
   [druid] 2019-06-11 09:25:40,047 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 338 from persistence list
   [druid] 2019-06-11 09:25:40,047 [-thread-pool-13] INFO  che.spark.storage.BlockManager {1} - Removing RDD 338
   [druid] 2019-06-11 09:25:40,047 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[338] at socketTextStream at Transform.scala:18 of time 1560216340000 ms
   [druid] 2019-06-11 09:25:40,047 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216330000 ms
   [druid] 2019-06-11 09:25:40,047 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216330000 ms
   [druid] 2019-06-11 09:25:45,011 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216345000 ms
   [druid] 2019-06-11 09:25:45,011 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216345000 ms.0 from job set of time 1560216345000 ms
   [druid] 2019-06-11 09:25:45,013 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:45,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 353 (map at Transform.scala:20)
   [druid] 2019-06-11 09:25:45,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:25:45,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 101 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:45,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 303 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:45,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 302, ShuffleMapStage 301)
   [druid] 2019-06-11 09:25:45,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 302)
   [druid] 2019-06-11 09:25:45,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 302 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:25:45,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_151 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:45,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_151_piece0 stored as bytes in memory (estimated size 1308.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:25:45,016 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_151_piece0 in memory on 192.168.91.1:63518 (size: 1308.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:45,016 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 151 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:45,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 302 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:25:45,016 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 302.0 with 2 tasks
   [druid] 2019-06-11 09:25:45,016 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 302.0 (TID 201, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:45,017 [er for task 201] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 302.0 (TID 201)
   [druid] 2019-06-11 09:25:45,025 [er for task 201] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 302.0 (TID 201). 810 bytes result sent to driver
   [druid] 2019-06-11 09:25:45,026 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 302.0 (TID 202, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:45,026 [er for task 202] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 302.0 (TID 202)
   [druid] 2019-06-11 09:25:45,026 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 302.0 (TID 201) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:25:45,034 [er for task 202] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 302.0 (TID 202). 853 bytes result sent to driver
   [druid] 2019-06-11 09:25:45,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 302.0 (TID 202) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:25:45,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 302.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 302 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:25:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:25:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:25:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 303)
   [druid] 2019-06-11 09:25:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:25:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 303 (MapPartitionsRDD[358] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:45,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_152 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:45,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_152_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:45,036 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_152_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:45,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 152 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:45,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 303 (MapPartitionsRDD[358] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:25:45,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 303.0 with 1 tasks
   [druid] 2019-06-11 09:25:45,037 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 303.0 (TID 203, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:45,038 [er for task 203] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 303.0 (TID 203)
   [druid] 2019-06-11 09:25:45,039 [er for task 203] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:45,039 [er for task 203] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:45,039 [er for task 203] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:45,039 [er for task 203] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:45,040 [er for task 203] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 303.0 (TID 203). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:45,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 303.0 (TID 203) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:45,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 303.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 303 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:45,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 101 finished: print at Transform.scala:36, took 0.027309 s
   [druid] 2019-06-11 09:25:45,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:45,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 100 is 83 bytes
   [druid] 2019-06-11 09:25:45,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 101 is 160 bytes
   [druid] 2019-06-11 09:25:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 102 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 306 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 305, ShuffleMapStage 304)
   [druid] 2019-06-11 09:25:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:25:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 306 (MapPartitionsRDD[358] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:45,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_153 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:45,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_153_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:45,045 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_153_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:45,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 153 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:45,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 306 (MapPartitionsRDD[358] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:25:45,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 306.0 with 1 tasks
   [druid] 2019-06-11 09:25:45,046 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 306.0 (TID 204, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:45,046 [er for task 204] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 306.0 (TID 204)
   [druid] 2019-06-11 09:25:45,047 [er for task 204] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:45,047 [er for task 204] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:45,047 [er for task 204] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:45,047 [er for task 204] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:45,048 [er for task 204] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 306.0 (TID 204). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:45,049 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 306.0 (TID 204) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:45,049 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 306.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:45,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 306 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:45,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 102 finished: print at Transform.scala:36, took 0.006279 s
   [druid] 2019-06-11 09:25:45,049 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216345000 ms.0 from job set of time 1560216345000 ms
   [druid] 2019-06-11 09:25:45,049 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.049 s for time 1560216345000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:25:45,049 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 351 from persistence list
   [druid] 2019-06-11 09:25:45,050 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 351
   [druid] 2019-06-11 09:25:45,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 346 from persistence list
   [druid] 2019-06-11 09:25:45,050 [-thread-pool-13] INFO  che.spark.storage.BlockManager {1} - Removing RDD 346
   [druid] 2019-06-11 09:25:45,050 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 345 from persistence list
   [druid] 2019-06-11 09:25:45,050 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 345
   [druid] 2019-06-11 09:25:45,050 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[345] at socketTextStream at Transform.scala:18 of time 1560216345000 ms
   [druid] 2019-06-11 09:25:45,050 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216335000 ms
   [druid] 2019-06-11 09:25:45,050 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216335000 ms
   [druid] 2019-06-11 09:25:50,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216350000 ms
   [druid] 2019-06-11 09:25:50,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216350000 ms.0 from job set of time 1560216350000 ms
   [druid] 2019-06-11 09:25:50,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:25:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 360 (map at Transform.scala:20)
   [druid] 2019-06-11 09:25:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 103 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 309 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 307, ShuffleMapStage 308)
   [druid] 2019-06-11 09:25:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 307)
   [druid] 2019-06-11 09:25:50,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 307 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:25:50,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_154 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:50,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_154_piece0 stored as bytes in memory (estimated size 1308.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:25:50,011 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_154_piece0 in memory on 192.168.91.1:63518 (size: 1308.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:50,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 154 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:50,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 307 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:25:50,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 307.0 with 2 tasks
   [druid] 2019-06-11 09:25:50,012 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 307.0 (TID 205, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:50,012 [er for task 205] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 307.0 (TID 205)
   [druid] 2019-06-11 09:25:50,022 [er for task 205] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 307.0 (TID 205). 853 bytes result sent to driver
   [druid] 2019-06-11 09:25:50,022 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 307.0 (TID 206, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:50,022 [er for task 206] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 307.0 (TID 206)
   [druid] 2019-06-11 09:25:50,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 307.0 (TID 205) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:25:50,030 [er for task 206] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 307.0 (TID 206). 767 bytes result sent to driver
   [druid] 2019-06-11 09:25:50,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 307.0 (TID 206) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:25:50,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 307.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 307 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:25:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:25:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:25:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 309)
   [druid] 2019-06-11 09:25:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:25:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 309 (MapPartitionsRDD[365] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:50,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_155 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:50,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_155_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:50,032 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_155_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:50,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 155 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:50,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 309 (MapPartitionsRDD[365] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:25:50,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 309.0 with 1 tasks
   [druid] 2019-06-11 09:25:50,033 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 309.0 (TID 207, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:50,033 [er for task 207] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 309.0 (TID 207)
   [druid] 2019-06-11 09:25:50,034 [er for task 207] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:50,034 [er for task 207] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:50,034 [er for task 207] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:50,034 [er for task 207] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:50,035 [er for task 207] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 309.0 (TID 207). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:50,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 309.0 (TID 207) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:50,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 309.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 309 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:50,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 103 finished: print at Transform.scala:36, took 0.027167 s
   [druid] 2019-06-11 09:25:50,038 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:50,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 103 is 160 bytes
   [druid] 2019-06-11 09:25:50,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 102 is 83 bytes
   [druid] 2019-06-11 09:25:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 104 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 312 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 310, ShuffleMapStage 311)
   [druid] 2019-06-11 09:25:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:25:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 312 (MapPartitionsRDD[365] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:50,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_156 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:50,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_156_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:50,041 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_156_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:50,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 156 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:50,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 312 (MapPartitionsRDD[365] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:25:50,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 312.0 with 1 tasks
   [druid] 2019-06-11 09:25:50,042 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 312.0 (TID 208, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:50,042 [er for task 208] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 312.0 (TID 208)
   [druid] 2019-06-11 09:25:50,043 [er for task 208] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:50,043 [er for task 208] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:50,043 [er for task 208] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:50,043 [er for task 208] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:50,044 [er for task 208] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 312.0 (TID 208). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:50,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 312.0 (TID 208) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:50,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 312.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 312 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:50,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 104 finished: print at Transform.scala:36, took 0.006240 s
   [druid] 2019-06-11 09:25:50,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216350000 ms.0 from job set of time 1560216350000 ms
   [druid] 2019-06-11 09:25:50,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560216350000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:25:50,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 358 from persistence list
   [druid] 2019-06-11 09:25:50,045 [-thread-pool-13] INFO  che.spark.storage.BlockManager {1} - Removing RDD 358
   [druid] 2019-06-11 09:25:50,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 353 from persistence list
   [druid] 2019-06-11 09:25:50,046 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 353
   [druid] 2019-06-11 09:25:50,046 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 352 from persistence list
   [druid] 2019-06-11 09:25:50,046 [-thread-pool-13] INFO  che.spark.storage.BlockManager {1} - Removing RDD 352
   [druid] 2019-06-11 09:25:50,046 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[352] at socketTextStream at Transform.scala:18 of time 1560216350000 ms
   [druid] 2019-06-11 09:25:50,046 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216340000 ms
   [druid] 2019-06-11 09:25:50,046 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216340000 ms
   [druid] 2019-06-11 09:25:55,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216355000 ms
   [druid] 2019-06-11 09:25:55,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216355000 ms.0 from job set of time 1560216355000 ms
   [druid] 2019-06-11 09:25:55,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:25:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 367 (map at Transform.scala:20)
   [druid] 2019-06-11 09:25:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 105 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 315 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 314, ShuffleMapStage 313)
   [druid] 2019-06-11 09:25:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 313)
   [druid] 2019-06-11 09:25:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 313 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:25:55,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_157 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:55,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_157_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:25:55,011 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_157_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:55,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 157 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:55,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 313 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:25:55,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 313.0 with 2 tasks
   [druid] 2019-06-11 09:25:55,012 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 313.0 (TID 209, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:55,012 [er for task 209] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 313.0 (TID 209)
   [druid] 2019-06-11 09:25:55,022 [er for task 209] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 313.0 (TID 209). 853 bytes result sent to driver
   [druid] 2019-06-11 09:25:55,022 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 313.0 (TID 210, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:25:55,022 [er for task 210] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 313.0 (TID 210)
   [druid] 2019-06-11 09:25:55,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 313.0 (TID 209) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:25:55,036 [er for task 210] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 313.0 (TID 210). 853 bytes result sent to driver
   [druid] 2019-06-11 09:25:55,037 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 313.0 (TID 210) in 15 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:25:55,037 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 313.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 313 (parallelize at Transform.scala:16) finished in 0.025 s
   [druid] 2019-06-11 09:25:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:25:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:25:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 315)
   [druid] 2019-06-11 09:25:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:25:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 315 (MapPartitionsRDD[372] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:55,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_158 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:55,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_158_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:55,039 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_158_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:55,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 158 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:55,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 315 (MapPartitionsRDD[372] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:25:55,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 315.0 with 1 tasks
   [druid] 2019-06-11 09:25:55,040 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 315.0 (TID 211, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:55,040 [er for task 211] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 315.0 (TID 211)
   [druid] 2019-06-11 09:25:55,041 [er for task 211] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:55,041 [er for task 211] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:55,041 [er for task 211] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:55,042 [er for task 211] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:25:55,042 [er for task 211] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 315.0 (TID 211). 966 bytes result sent to driver
   [druid] 2019-06-11 09:25:55,043 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 315.0 (TID 211) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:55,043 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 315.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:55,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 315 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:55,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 105 finished: print at Transform.scala:36, took 0.034282 s
   [druid] 2019-06-11 09:25:55,045 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:25:55,046 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 105 is 160 bytes
   [druid] 2019-06-11 09:25:55,046 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 104 is 83 bytes
   [druid] 2019-06-11 09:25:55,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 106 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:25:55,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 318 (print at Transform.scala:36)
   [druid] 2019-06-11 09:25:55,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 317, ShuffleMapStage 316)
   [druid] 2019-06-11 09:25:55,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:25:55,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 318 (MapPartitionsRDD[372] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:25:55,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_159 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:55,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_159_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:25:55,048 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_159_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:25:55,049 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 159 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:25:55,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 318 (MapPartitionsRDD[372] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:25:55,049 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 318.0 with 1 tasks
   [druid] 2019-06-11 09:25:55,049 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 318.0 (TID 212, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:25:55,049 [er for task 212] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 318.0 (TID 212)
   [druid] 2019-06-11 09:25:55,050 [er for task 212] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:25:55,050 [er for task 212] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:55,050 [er for task 212] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:25:55,050 [er for task 212] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:25:55,051 [er for task 212] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 318.0 (TID 212). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:25:55,051 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 318.0 (TID 212) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:25:55,051 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 318.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:25:55,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 318 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:25:55,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 106 finished: print at Transform.scala:36, took 0.006134 s
   [druid] 2019-06-11 09:25:55,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216355000 ms.0 from job set of time 1560216355000 ms
   [druid] 2019-06-11 09:25:55,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.052 s for time 1560216355000 ms (execution: 0.045 s)
   [druid] 2019-06-11 09:25:55,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 365 from persistence list
   [druid] 2019-06-11 09:25:55,052 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 365
   [druid] 2019-06-11 09:25:55,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 360 from persistence list
   [druid] 2019-06-11 09:25:55,053 [-thread-pool-13] INFO  che.spark.storage.BlockManager {1} - Removing RDD 360
   [druid] 2019-06-11 09:25:55,053 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 359 from persistence list
   [druid] 2019-06-11 09:25:55,053 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 359
   [druid] 2019-06-11 09:25:55,053 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[359] at socketTextStream at Transform.scala:18 of time 1560216355000 ms
   [druid] 2019-06-11 09:25:55,053 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216345000 ms
   [druid] 2019-06-11 09:25:55,053 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216345000 ms
   [druid] 2019-06-11 09:26:00,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216360000 ms
   [druid] 2019-06-11 09:26:00,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216360000 ms.0 from job set of time 1560216360000 ms
   [druid] 2019-06-11 09:26:00,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:26:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 374 (map at Transform.scala:20)
   [druid] 2019-06-11 09:26:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 107 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 321 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 320, ShuffleMapStage 319)
   [druid] 2019-06-11 09:26:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 319)
   [druid] 2019-06-11 09:26:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 319 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:26:00,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_160 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:00,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_160_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:26:00,012 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_160_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:00,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 160 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 319 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:26:00,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 319.0 with 2 tasks
   [druid] 2019-06-11 09:26:00,013 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 319.0 (TID 213, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:00,013 [er for task 213] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 319.0 (TID 213)
   [druid] 2019-06-11 09:26:00,022 [er for task 213] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 319.0 (TID 213). 853 bytes result sent to driver
   [druid] 2019-06-11 09:26:00,022 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 319.0 (TID 214, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:00,022 [er for task 214] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 319.0 (TID 214)
   [druid] 2019-06-11 09:26:00,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 319.0 (TID 213) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:26:00,030 [er for task 214] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 319.0 (TID 214). 810 bytes result sent to driver
   [druid] 2019-06-11 09:26:00,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 319.0 (TID 214) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:26:00,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 319.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 319 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:26:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:26:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:26:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 321)
   [druid] 2019-06-11 09:26:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:26:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 321 (MapPartitionsRDD[379] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:00,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_161 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:00,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_161_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:00,032 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_161_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:00,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 161 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:00,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 321 (MapPartitionsRDD[379] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:26:00,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 321.0 with 1 tasks
   [druid] 2019-06-11 09:26:00,034 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 321.0 (TID 215, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:00,034 [er for task 215] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 321.0 (TID 215)
   [druid] 2019-06-11 09:26:00,035 [er for task 215] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:00,035 [er for task 215] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:00,035 [er for task 215] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:00,035 [er for task 215] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:00,036 [er for task 215] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 321.0 (TID 215). 966 bytes result sent to driver
   [druid] 2019-06-11 09:26:00,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 321.0 (TID 215) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:00,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 321.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:00,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 321 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:26:00,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 107 finished: print at Transform.scala:36, took 0.025940 s
   [druid] 2019-06-11 09:26:00,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:00,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 107 is 160 bytes
   [druid] 2019-06-11 09:26:00,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 106 is 83 bytes
   [druid] 2019-06-11 09:26:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 108 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 324 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 322, ShuffleMapStage 323)
   [druid] 2019-06-11 09:26:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:26:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 324 (MapPartitionsRDD[379] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:00,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_162 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:00,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_162_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:00,042 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_162_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:00,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 162 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:00,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 324 (MapPartitionsRDD[379] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:26:00,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 324.0 with 1 tasks
   [druid] 2019-06-11 09:26:00,042 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 324.0 (TID 216, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:00,042 [er for task 216] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 324.0 (TID 216)
   [druid] 2019-06-11 09:26:00,043 [er for task 216] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:00,043 [er for task 216] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:00,044 [er for task 216] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:00,044 [er for task 216] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:26:00,044 [er for task 216] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 324.0 (TID 216). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:00,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 324.0 (TID 216) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:00,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 324.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:00,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 324 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:26:00,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 108 finished: print at Transform.scala:36, took 0.005207 s
   [druid] 2019-06-11 09:26:00,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216360000 ms.0 from job set of time 1560216360000 ms
   [druid] 2019-06-11 09:26:00,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.044 s for time 1560216360000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:26:00,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 372 from persistence list
   [druid] 2019-06-11 09:26:00,045 [-thread-pool-13] INFO  che.spark.storage.BlockManager {1} - Removing RDD 372
   [druid] 2019-06-11 09:26:00,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 367 from persistence list
   [druid] 2019-06-11 09:26:00,045 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 367
   [druid] 2019-06-11 09:26:00,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 366 from persistence list
   [druid] 2019-06-11 09:26:00,045 [-thread-pool-13] INFO  che.spark.storage.BlockManager {1} - Removing RDD 366
   [druid] 2019-06-11 09:26:00,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[366] at socketTextStream at Transform.scala:18 of time 1560216360000 ms
   [druid] 2019-06-11 09:26:00,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216350000 ms
   [druid] 2019-06-11 09:26:00,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216350000 ms
   [druid] 2019-06-11 09:26:05,021 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216365000 ms
   [druid] 2019-06-11 09:26:05,023 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_144_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,023 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216365000 ms.0 from job set of time 1560216365000 ms
   [druid] 2019-06-11 09:26:05,024 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_151_piece0 on 192.168.91.1:63518 in memory (size: 1308.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,025 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 97
   [druid] 2019-06-11 09:26:05,026 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_158_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,026 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 92
   [druid] 2019-06-11 09:26:05,027 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 93
   [druid] 2019-06-11 09:26:05,027 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_148_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,028 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 100
   [druid] 2019-06-11 09:26:05,028 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_145_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,028 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:05,029 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 99
   [druid] 2019-06-11 09:26:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 381 (map at Transform.scala:20)
   [druid] 2019-06-11 09:26:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:26:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 109 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 327 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 325, ShuffleMapStage 326)
   [druid] 2019-06-11 09:26:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 326)
   [druid] 2019-06-11 09:26:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 326 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:26:05,029 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_160_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_163 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:05,030 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_156_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_163_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:26:05,032 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_150_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,032 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_163_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 163 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:05,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 326 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:26:05,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 326.0 with 2 tasks
   [druid] 2019-06-11 09:26:05,033 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_142_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,033 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 326.0 (TID 217, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:05,033 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 104
   [druid] 2019-06-11 09:26:05,033 [er for task 217] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 326.0 (TID 217)
   [druid] 2019-06-11 09:26:05,033 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 105
   [druid] 2019-06-11 09:26:05,034 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 94
   [druid] 2019-06-11 09:26:05,034 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_162_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,035 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_155_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,036 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_152_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,037 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_157_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,037 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 101
   [druid] 2019-06-11 09:26:05,038 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 98
   [druid] 2019-06-11 09:26:05,038 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 96
   [druid] 2019-06-11 09:26:05,039 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_153_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:05,040 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_159_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:05,040 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_161_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:05,041 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_154_piece0 on 192.168.91.1:63518 in memory (size: 1308.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:05,042 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_149_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:05,042 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 103
   [druid] 2019-06-11 09:26:05,042 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 95
   [druid] 2019-06-11 09:26:05,043 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_141_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:05,043 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 90
   [druid] 2019-06-11 09:26:05,043 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 102
   [druid] 2019-06-11 09:26:05,044 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_147_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:05,045 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_143_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:05,045 [er for task 217] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 326.0 (TID 217). 767 bytes result sent to driver
   [druid] 2019-06-11 09:26:05,046 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 326.0 (TID 218, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:05,046 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_146_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:05,046 [er for task 218] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 326.0 (TID 218)
   [druid] 2019-06-11 09:26:05,046 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 326.0 (TID 217) in 13 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:26:05,075 [er for task 218] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 326.0 (TID 218). 853 bytes result sent to driver
   [druid] 2019-06-11 09:26:05,075 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 326.0 (TID 218) in 30 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:26:05,075 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 326.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:05,075 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 326 (parallelize at Transform.scala:16) finished in 0.042 s
   [druid] 2019-06-11 09:26:05,075 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:26:05,075 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:26:05,075 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 327)
   [druid] 2019-06-11 09:26:05,075 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:26:05,076 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 327 (MapPartitionsRDD[386] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:05,076 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_164 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:05,077 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_164_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:05,077 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_164_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:05,078 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 164 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:05,078 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 327 (MapPartitionsRDD[386] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:26:05,078 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 327.0 with 1 tasks
   [druid] 2019-06-11 09:26:05,079 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 327.0 (TID 219, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:05,079 [er for task 219] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 327.0 (TID 219)
   [druid] 2019-06-11 09:26:05,080 [er for task 219] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:05,080 [er for task 219] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:05,080 [er for task 219] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:05,080 [er for task 219] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:05,081 [er for task 219] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 327.0 (TID 219). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:05,081 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 327.0 (TID 219) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:05,081 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 327.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:05,081 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 327 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:26:05,081 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 109 finished: print at Transform.scala:36, took 0.053208 s
   [druid] 2019-06-11 09:26:05,084 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:05,084 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 108 is 83 bytes
   [druid] 2019-06-11 09:26:05,084 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 109 is 160 bytes
   [druid] 2019-06-11 09:26:05,084 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 110 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:05,085 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 330 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:05,085 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 328, ShuffleMapStage 329)
   [druid] 2019-06-11 09:26:05,085 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:26:05,085 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 330 (MapPartitionsRDD[386] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:05,085 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_165 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:05,086 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_165_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:05,087 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_165_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:05,087 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 165 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:05,087 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 330 (MapPartitionsRDD[386] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:26:05,087 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 330.0 with 1 tasks
   [druid] 2019-06-11 09:26:05,087 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 330.0 (TID 220, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:05,088 [er for task 220] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 330.0 (TID 220)
   [druid] 2019-06-11 09:26:05,088 [er for task 220] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:05,088 [er for task 220] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:05,089 [er for task 220] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:05,089 [er for task 220] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:05,089 [er for task 220] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 330.0 (TID 220). 966 bytes result sent to driver
   [druid] 2019-06-11 09:26:05,090 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 330.0 (TID 220) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:05,090 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 330.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:05,090 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 330 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:26:05,090 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 110 finished: print at Transform.scala:36, took 0.006165 s
   [druid] 2019-06-11 09:26:05,090 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216365000 ms.0 from job set of time 1560216365000 ms
   [druid] 2019-06-11 09:26:05,091 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.090 s for time 1560216365000 ms (execution: 0.067 s)
   [druid] 2019-06-11 09:26:05,091 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 379 from persistence list
   [druid] 2019-06-11 09:26:05,091 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 379
   [druid] 2019-06-11 09:26:05,091 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 374 from persistence list
   [druid] 2019-06-11 09:26:05,091 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 374
   [druid] 2019-06-11 09:26:05,091 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 373 from persistence list
   [druid] 2019-06-11 09:26:05,091 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 373
   [druid] 2019-06-11 09:26:05,091 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[373] at socketTextStream at Transform.scala:18 of time 1560216365000 ms
   [druid] 2019-06-11 09:26:05,092 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216355000 ms
   [druid] 2019-06-11 09:26:05,092 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216355000 ms
   [druid] 2019-06-11 09:26:10,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216370000 ms
   [druid] 2019-06-11 09:26:10,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216370000 ms.0 from job set of time 1560216370000 ms
   [druid] 2019-06-11 09:26:10,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 388 (map at Transform.scala:20)
   [druid] 2019-06-11 09:26:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:26:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 111 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 333 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 332, ShuffleMapStage 331)
   [druid] 2019-06-11 09:26:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 332)
   [druid] 2019-06-11 09:26:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 332 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:26:10,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_166 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:10,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_166_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:26:10,011 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_166_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:10,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 166 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:10,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 332 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:26:10,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 332.0 with 2 tasks
   [druid] 2019-06-11 09:26:10,012 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 332.0 (TID 221, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:10,012 [er for task 221] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 332.0 (TID 221)
   [druid] 2019-06-11 09:26:10,022 [er for task 221] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 332.0 (TID 221). 767 bytes result sent to driver
   [druid] 2019-06-11 09:26:10,022 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 332.0 (TID 222, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:10,022 [er for task 222] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 332.0 (TID 222)
   [druid] 2019-06-11 09:26:10,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 332.0 (TID 221) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:26:10,031 [er for task 222] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 332.0 (TID 222). 853 bytes result sent to driver
   [druid] 2019-06-11 09:26:10,032 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 332.0 (TID 222) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:26:10,032 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 332.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 332 (parallelize at Transform.scala:16) finished in 0.021 s
   [druid] 2019-06-11 09:26:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:26:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:26:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 333)
   [druid] 2019-06-11 09:26:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:26:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 333 (MapPartitionsRDD[393] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:10,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_167 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:10,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_167_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:10,035 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_167_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:10,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 167 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:10,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 333 (MapPartitionsRDD[393] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:26:10,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 333.0 with 1 tasks
   [druid] 2019-06-11 09:26:10,036 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 333.0 (TID 223, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:10,036 [er for task 223] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 333.0 (TID 223)
   [druid] 2019-06-11 09:26:10,037 [er for task 223] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:10,037 [er for task 223] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:26:10,037 [er for task 223] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:10,037 [er for task 223] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:10,037 [er for task 223] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 333.0 (TID 223). 966 bytes result sent to driver
   [druid] 2019-06-11 09:26:10,038 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 333.0 (TID 223) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:10,038 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 333.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 333 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:26:10,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 111 finished: print at Transform.scala:36, took 0.029333 s
   [druid] 2019-06-11 09:26:10,040 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:10,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 110 is 83 bytes
   [druid] 2019-06-11 09:26:10,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 111 is 160 bytes
   [druid] 2019-06-11 09:26:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 112 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 336 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 335, ShuffleMapStage 334)
   [druid] 2019-06-11 09:26:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:26:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 336 (MapPartitionsRDD[393] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:10,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_168 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:10,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_168_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:10,043 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_168_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:10,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 168 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 336 (MapPartitionsRDD[393] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:26:10,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 336.0 with 1 tasks
   [druid] 2019-06-11 09:26:10,043 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 336.0 (TID 224, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:10,044 [er for task 224] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 336.0 (TID 224)
   [druid] 2019-06-11 09:26:10,044 [er for task 224] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:10,044 [er for task 224] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:10,044 [er for task 224] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:10,044 [er for task 224] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:10,046 [er for task 224] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 336.0 (TID 224). 966 bytes result sent to driver
   [druid] 2019-06-11 09:26:10,046 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 336.0 (TID 224) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:10,046 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 336.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 336 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:26:10,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 112 finished: print at Transform.scala:36, took 0.005857 s
   [druid] 2019-06-11 09:26:10,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216370000 ms.0 from job set of time 1560216370000 ms
   [druid] 2019-06-11 09:26:10,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.046 s for time 1560216370000 ms (execution: 0.040 s)
   [druid] 2019-06-11 09:26:10,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 386 from persistence list
   [druid] 2019-06-11 09:26:10,047 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 386
   [druid] 2019-06-11 09:26:10,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 381 from persistence list
   [druid] 2019-06-11 09:26:10,047 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 381
   [druid] 2019-06-11 09:26:10,048 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 380 from persistence list
   [druid] 2019-06-11 09:26:10,048 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 380
   [druid] 2019-06-11 09:26:10,048 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[380] at socketTextStream at Transform.scala:18 of time 1560216370000 ms
   [druid] 2019-06-11 09:26:10,048 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216360000 ms
   [druid] 2019-06-11 09:26:10,048 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216360000 ms
   [druid] 2019-06-11 09:26:15,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216375000 ms
   [druid] 2019-06-11 09:26:15,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216375000 ms.0 from job set of time 1560216375000 ms
   [druid] 2019-06-11 09:26:15,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:26:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 395 (map at Transform.scala:20)
   [druid] 2019-06-11 09:26:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 113 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 339 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 338, ShuffleMapStage 337)
   [druid] 2019-06-11 09:26:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 337)
   [druid] 2019-06-11 09:26:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 337 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:26:15,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_169 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:15,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_169_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:26:15,011 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_169_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:15,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 169 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:15,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 337 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:26:15,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 337.0 with 2 tasks
   [druid] 2019-06-11 09:26:15,011 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 337.0 (TID 225, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:15,012 [er for task 225] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 337.0 (TID 225)
   [druid] 2019-06-11 09:26:15,021 [er for task 225] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 337.0 (TID 225). 853 bytes result sent to driver
   [druid] 2019-06-11 09:26:15,022 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 337.0 (TID 226, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:15,022 [er for task 226] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 337.0 (TID 226)
   [druid] 2019-06-11 09:26:15,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 337.0 (TID 225) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:26:15,030 [er for task 226] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 337.0 (TID 226). 767 bytes result sent to driver
   [druid] 2019-06-11 09:26:15,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 337.0 (TID 226) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:26:15,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 337.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 337 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:26:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:26:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:26:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 339)
   [druid] 2019-06-11 09:26:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:26:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 339 (MapPartitionsRDD[400] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:15,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_170 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:15,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_170_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:15,033 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_170_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:15,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 170 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 339 (MapPartitionsRDD[400] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:26:15,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 339.0 with 1 tasks
   [druid] 2019-06-11 09:26:15,033 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 339.0 (TID 227, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:15,034 [er for task 227] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 339.0 (TID 227)
   [druid] 2019-06-11 09:26:15,034 [er for task 227] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:15,034 [er for task 227] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:15,034 [er for task 227] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:15,034 [er for task 227] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:15,035 [er for task 227] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 339.0 (TID 227). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:15,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 339.0 (TID 227) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:15,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 339.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:15,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 339 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:26:15,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 113 finished: print at Transform.scala:36, took 0.027399 s
   [druid] 2019-06-11 09:26:15,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:15,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 113 is 160 bytes
   [druid] 2019-06-11 09:26:15,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 112 is 83 bytes
   [druid] 2019-06-11 09:26:15,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 114 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:15,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 342 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:15,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 340, ShuffleMapStage 341)
   [druid] 2019-06-11 09:26:15,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:26:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 342 (MapPartitionsRDD[400] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:15,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_171 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:15,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_171_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:15,041 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_171_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:15,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 171 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 342 (MapPartitionsRDD[400] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:26:15,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 342.0 with 1 tasks
   [druid] 2019-06-11 09:26:15,042 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 342.0 (TID 228, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:15,042 [er for task 228] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 342.0 (TID 228)
   [druid] 2019-06-11 09:26:15,043 [er for task 228] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:15,043 [er for task 228] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:15,043 [er for task 228] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:15,043 [er for task 228] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:15,044 [er for task 228] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 342.0 (TID 228). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:15,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 342.0 (TID 228) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:15,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 342.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:15,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 342 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:26:15,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 114 finished: print at Transform.scala:36, took 0.005883 s
   [druid] 2019-06-11 09:26:15,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216375000 ms.0 from job set of time 1560216375000 ms
   [druid] 2019-06-11 09:26:15,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.044 s for time 1560216375000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:26:15,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 393 from persistence list
   [druid] 2019-06-11 09:26:15,045 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 393
   [druid] 2019-06-11 09:26:15,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 388 from persistence list
   [druid] 2019-06-11 09:26:15,045 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 388
   [druid] 2019-06-11 09:26:15,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 387 from persistence list
   [druid] 2019-06-11 09:26:15,045 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 387
   [druid] 2019-06-11 09:26:15,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[387] at socketTextStream at Transform.scala:18 of time 1560216375000 ms
   [druid] 2019-06-11 09:26:15,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216365000 ms
   [druid] 2019-06-11 09:26:15,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216365000 ms
   [druid] 2019-06-11 09:26:20,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216380000 ms
   [druid] 2019-06-11 09:26:20,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216380000 ms.0 from job set of time 1560216380000 ms
   [druid] 2019-06-11 09:26:20,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 402 (map at Transform.scala:20)
   [druid] 2019-06-11 09:26:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:26:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 115 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 345 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 343, ShuffleMapStage 344)
   [druid] 2019-06-11 09:26:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 344)
   [druid] 2019-06-11 09:26:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 344 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:26:20,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_172 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:20,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_172_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:26:20,011 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_172_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:20,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 172 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:20,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 344 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:26:20,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 344.0 with 2 tasks
   [druid] 2019-06-11 09:26:20,012 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 344.0 (TID 229, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:20,012 [er for task 229] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 344.0 (TID 229)
   [druid] 2019-06-11 09:26:20,021 [er for task 229] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 344.0 (TID 229). 810 bytes result sent to driver
   [druid] 2019-06-11 09:26:20,021 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 344.0 (TID 230, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:20,021 [er for task 230] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 344.0 (TID 230)
   [druid] 2019-06-11 09:26:20,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 344.0 (TID 229) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:26:20,029 [er for task 230] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 344.0 (TID 230). 810 bytes result sent to driver
   [druid] 2019-06-11 09:26:20,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 344.0 (TID 230) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:26:20,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 344.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 344 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:26:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:26:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:26:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 345)
   [druid] 2019-06-11 09:26:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:26:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 345 (MapPartitionsRDD[407] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:20,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_173 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:20,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_173_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:20,031 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_173_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:20,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 173 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 345 (MapPartitionsRDD[407] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:26:20,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 345.0 with 1 tasks
   [druid] 2019-06-11 09:26:20,032 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 345.0 (TID 231, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:20,032 [er for task 231] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 345.0 (TID 231)
   [druid] 2019-06-11 09:26:20,033 [er for task 231] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:20,033 [er for task 231] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:20,033 [er for task 231] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:20,033 [er for task 231] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:20,034 [er for task 231] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 345.0 (TID 231). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:20,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 345.0 (TID 231) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:20,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 345.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:20,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 345 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:26:20,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 115 finished: print at Transform.scala:36, took 0.025847 s
   [druid] 2019-06-11 09:26:20,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:20,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 114 is 83 bytes
   [druid] 2019-06-11 09:26:20,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 115 is 160 bytes
   [druid] 2019-06-11 09:26:20,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 116 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:20,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 348 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:20,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 347, ShuffleMapStage 346)
   [druid] 2019-06-11 09:26:20,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:26:20,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 348 (MapPartitionsRDD[407] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:20,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_174 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:20,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_174_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:20,040 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_174_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:20,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 174 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 348 (MapPartitionsRDD[407] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:26:20,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 348.0 with 1 tasks
   [druid] 2019-06-11 09:26:20,041 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 348.0 (TID 232, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:20,041 [er for task 232] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 348.0 (TID 232)
   [druid] 2019-06-11 09:26:20,042 [er for task 232] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:20,042 [er for task 232] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:20,042 [er for task 232] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:20,042 [er for task 232] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:20,043 [er for task 232] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 348.0 (TID 232). 966 bytes result sent to driver
   [druid] 2019-06-11 09:26:20,043 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 348.0 (TID 232) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:20,043 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 348.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:20,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 348 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:26:20,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 116 finished: print at Transform.scala:36, took 0.005597 s
   [druid] 2019-06-11 09:26:20,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216380000 ms.0 from job set of time 1560216380000 ms
   [druid] 2019-06-11 09:26:20,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216380000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:26:20,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 400 from persistence list
   [druid] 2019-06-11 09:26:20,044 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 400
   [druid] 2019-06-11 09:26:20,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 395 from persistence list
   [druid] 2019-06-11 09:26:20,044 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 395
   [druid] 2019-06-11 09:26:20,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 394 from persistence list
   [druid] 2019-06-11 09:26:20,044 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 394
   [druid] 2019-06-11 09:26:20,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[394] at socketTextStream at Transform.scala:18 of time 1560216380000 ms
   [druid] 2019-06-11 09:26:20,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216370000 ms
   [druid] 2019-06-11 09:26:20,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216370000 ms
   [druid] 2019-06-11 09:26:25,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216385000 ms
   [druid] 2019-06-11 09:26:25,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216385000 ms.0 from job set of time 1560216385000 ms
   [druid] 2019-06-11 09:26:25,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:26:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 409 (map at Transform.scala:20)
   [druid] 2019-06-11 09:26:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 117 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 351 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 350, ShuffleMapStage 349)
   [druid] 2019-06-11 09:26:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 349)
   [druid] 2019-06-11 09:26:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 349 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:26:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_175 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:25,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_175_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:26:25,010 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_175_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:25,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 175 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 349 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:26:25,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 349.0 with 2 tasks
   [druid] 2019-06-11 09:26:25,011 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 349.0 (TID 233, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:25,011 [er for task 233] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 349.0 (TID 233)
   [druid] 2019-06-11 09:26:25,020 [er for task 233] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 349.0 (TID 233). 810 bytes result sent to driver
   [druid] 2019-06-11 09:26:25,020 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 349.0 (TID 234, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:25,020 [er for task 234] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 349.0 (TID 234)
   [druid] 2019-06-11 09:26:25,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 349.0 (TID 233) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:26:25,028 [er for task 234] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 349.0 (TID 234). 810 bytes result sent to driver
   [druid] 2019-06-11 09:26:25,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 349.0 (TID 234) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:26:25,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 349.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 349 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:26:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:26:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:26:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 351)
   [druid] 2019-06-11 09:26:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:26:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 351 (MapPartitionsRDD[414] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:25,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_176 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:25,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_176_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:25,030 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_176_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:25,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 176 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:25,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 351 (MapPartitionsRDD[414] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:26:25,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 351.0 with 1 tasks
   [druid] 2019-06-11 09:26:25,031 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 351.0 (TID 235, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:25,031 [er for task 235] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 351.0 (TID 235)
   [druid] 2019-06-11 09:26:25,032 [er for task 235] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:25,032 [er for task 235] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:25,032 [er for task 235] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:25,032 [er for task 235] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:25,032 [er for task 235] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 351.0 (TID 235). 966 bytes result sent to driver
   [druid] 2019-06-11 09:26:25,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 351.0 (TID 235) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:25,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 351.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:25,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 351 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:26:25,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 117 finished: print at Transform.scala:36, took 0.025416 s
   [druid] 2019-06-11 09:26:25,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:25,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 117 is 160 bytes
   [druid] 2019-06-11 09:26:25,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 116 is 83 bytes
   [druid] 2019-06-11 09:26:25,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 118 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:25,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 354 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:25,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 353, ShuffleMapStage 352)
   [druid] 2019-06-11 09:26:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:26:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 354 (MapPartitionsRDD[414] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:25,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_177 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:25,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_177_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:25,039 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_177_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:25,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 177 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 354 (MapPartitionsRDD[414] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:26:25,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 354.0 with 1 tasks
   [druid] 2019-06-11 09:26:25,040 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 354.0 (TID 236, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:25,040 [er for task 236] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 354.0 (TID 236)
   [druid] 2019-06-11 09:26:25,041 [er for task 236] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:25,041 [er for task 236] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:25,041 [er for task 236] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:25,041 [er for task 236] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:25,042 [er for task 236] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 354.0 (TID 236). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:25,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 354.0 (TID 236) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:25,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 354.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:25,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 354 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:26:25,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 118 finished: print at Transform.scala:36, took 0.005635 s
   [druid] 2019-06-11 09:26:25,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216385000 ms.0 from job set of time 1560216385000 ms
   [druid] 2019-06-11 09:26:25,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 407 from persistence list
   [druid] 2019-06-11 09:26:25,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216385000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:26:25,043 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 407
   [druid] 2019-06-11 09:26:25,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 402 from persistence list
   [druid] 2019-06-11 09:26:25,043 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 402
   [druid] 2019-06-11 09:26:25,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 401 from persistence list
   [druid] 2019-06-11 09:26:25,044 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 401
   [druid] 2019-06-11 09:26:25,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[401] at socketTextStream at Transform.scala:18 of time 1560216385000 ms
   [druid] 2019-06-11 09:26:25,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216375000 ms
   [druid] 2019-06-11 09:26:25,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216375000 ms
   [druid] 2019-06-11 09:26:30,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216390000 ms
   [druid] 2019-06-11 09:26:30,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216390000 ms.0 from job set of time 1560216390000 ms
   [druid] 2019-06-11 09:26:30,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:26:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 416 (map at Transform.scala:20)
   [druid] 2019-06-11 09:26:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 119 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 357 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 356, ShuffleMapStage 355)
   [druid] 2019-06-11 09:26:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 355)
   [druid] 2019-06-11 09:26:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 355 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:26:30,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_178 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:30,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_178_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:26:30,011 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_178_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:30,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 178 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:30,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 355 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:26:30,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 355.0 with 2 tasks
   [druid] 2019-06-11 09:26:30,012 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 355.0 (TID 237, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:30,012 [er for task 237] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 355.0 (TID 237)
   [druid] 2019-06-11 09:26:30,021 [er for task 237] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 355.0 (TID 237). 853 bytes result sent to driver
   [druid] 2019-06-11 09:26:30,022 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 355.0 (TID 238, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:30,022 [er for task 238] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 355.0 (TID 238)
   [druid] 2019-06-11 09:26:30,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 355.0 (TID 237) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:26:30,030 [er for task 238] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 355.0 (TID 238). 810 bytes result sent to driver
   [druid] 2019-06-11 09:26:30,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 355.0 (TID 238) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:26:30,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 355.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 355 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:26:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:26:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:26:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 357)
   [druid] 2019-06-11 09:26:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:26:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 357 (MapPartitionsRDD[421] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:30,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_179 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:30,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_179_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:30,032 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_179_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:30,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 179 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 357 (MapPartitionsRDD[421] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:26:30,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 357.0 with 1 tasks
   [druid] 2019-06-11 09:26:30,033 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 357.0 (TID 239, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:30,033 [er for task 239] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 357.0 (TID 239)
   [druid] 2019-06-11 09:26:30,034 [er for task 239] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:30,034 [er for task 239] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:30,034 [er for task 239] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:30,034 [er for task 239] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:30,035 [er for task 239] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 357.0 (TID 239). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:30,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 357.0 (TID 239) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:30,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 357.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:30,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 357 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:26:30,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 119 finished: print at Transform.scala:36, took 0.026098 s
   [druid] 2019-06-11 09:26:30,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:30,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 119 is 160 bytes
   [druid] 2019-06-11 09:26:30,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 118 is 83 bytes
   [druid] 2019-06-11 09:26:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 120 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 360 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 358, ShuffleMapStage 359)
   [druid] 2019-06-11 09:26:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:26:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 360 (MapPartitionsRDD[421] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:30,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_180 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:30,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_180_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:30,041 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_180_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:30,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 180 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:30,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 360 (MapPartitionsRDD[421] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:26:30,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 360.0 with 1 tasks
   [druid] 2019-06-11 09:26:30,042 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 360.0 (TID 240, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:30,042 [er for task 240] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 360.0 (TID 240)
   [druid] 2019-06-11 09:26:30,043 [er for task 240] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:30,043 [er for task 240] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:30,043 [er for task 240] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:30,043 [er for task 240] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:30,044 [er for task 240] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 360.0 (TID 240). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:30,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 360.0 (TID 240) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:30,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 360.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:30,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 360 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:26:30,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 120 finished: print at Transform.scala:36, took 0.006811 s
   [druid] 2019-06-11 09:26:30,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216390000 ms.0 from job set of time 1560216390000 ms
   [druid] 2019-06-11 09:26:30,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560216390000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:26:30,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 414 from persistence list
   [druid] 2019-06-11 09:26:30,045 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 414
   [druid] 2019-06-11 09:26:30,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 409 from persistence list
   [druid] 2019-06-11 09:26:30,045 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 409
   [druid] 2019-06-11 09:26:30,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 408 from persistence list
   [druid] 2019-06-11 09:26:30,045 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 408
   [druid] 2019-06-11 09:26:30,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[408] at socketTextStream at Transform.scala:18 of time 1560216390000 ms
   [druid] 2019-06-11 09:26:30,046 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216380000 ms
   [druid] 2019-06-11 09:26:30,046 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216380000 ms
   [druid] 2019-06-11 09:26:35,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216395000 ms
   [druid] 2019-06-11 09:26:35,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216395000 ms.0 from job set of time 1560216395000 ms
   [druid] 2019-06-11 09:26:35,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:26:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 423 (map at Transform.scala:20)
   [druid] 2019-06-11 09:26:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 121 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 363 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 361, ShuffleMapStage 362)
   [druid] 2019-06-11 09:26:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 361)
   [druid] 2019-06-11 09:26:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 361 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:26:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_181 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:35,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_181_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:26:35,010 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_181_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:35,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 181 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 361 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:26:35,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 361.0 with 2 tasks
   [druid] 2019-06-11 09:26:35,011 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 361.0 (TID 241, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:35,011 [er for task 241] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 361.0 (TID 241)
   [druid] 2019-06-11 09:26:35,025 [er for task 241] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 361.0 (TID 241). 810 bytes result sent to driver
   [druid] 2019-06-11 09:26:35,025 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 361.0 (TID 242, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:35,026 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 361.0 (TID 241) in 15 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:26:35,026 [er for task 242] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 361.0 (TID 242)
   [druid] 2019-06-11 09:26:35,035 [er for task 242] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 361.0 (TID 242). 810 bytes result sent to driver
   [druid] 2019-06-11 09:26:35,035 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 361.0 (TID 242) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:26:35,035 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 361.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 361 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:26:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:26:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:26:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 363)
   [druid] 2019-06-11 09:26:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:26:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 363 (MapPartitionsRDD[428] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:35,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_182 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:35,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_182_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:35,037 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_182_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:35,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 182 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 363 (MapPartitionsRDD[428] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:26:35,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 363.0 with 1 tasks
   [druid] 2019-06-11 09:26:35,038 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 363.0 (TID 243, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:35,038 [er for task 243] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 363.0 (TID 243)
   [druid] 2019-06-11 09:26:35,039 [er for task 243] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:35,039 [er for task 243] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:35,039 [er for task 243] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:35,039 [er for task 243] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:35,040 [er for task 243] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 363.0 (TID 243). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:35,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 363.0 (TID 243) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:35,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 363.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 363 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:26:35,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 121 finished: print at Transform.scala:36, took 0.031637 s
   [druid] 2019-06-11 09:26:35,042 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:35,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 121 is 160 bytes
   [druid] 2019-06-11 09:26:35,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 120 is 83 bytes
   [druid] 2019-06-11 09:26:35,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 122 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:35,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 366 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:35,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 365, ShuffleMapStage 364)
   [druid] 2019-06-11 09:26:35,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:26:35,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 366 (MapPartitionsRDD[428] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:35,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_183 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:35,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_183_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:35,045 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_183_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:35,045 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 183 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:35,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 366 (MapPartitionsRDD[428] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:26:35,045 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 366.0 with 1 tasks
   [druid] 2019-06-11 09:26:35,046 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 366.0 (TID 244, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:35,046 [er for task 244] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 366.0 (TID 244)
   [druid] 2019-06-11 09:26:35,047 [er for task 244] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:35,047 [er for task 244] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:35,047 [er for task 244] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:35,047 [er for task 244] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:35,048 [er for task 244] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 366.0 (TID 244). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:35,048 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 366.0 (TID 244) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:35,048 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 366.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:35,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 366 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:26:35,048 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 122 finished: print at Transform.scala:36, took 0.006355 s
   [druid] 2019-06-11 09:26:35,049 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216395000 ms.0 from job set of time 1560216395000 ms
   [druid] 2019-06-11 09:26:35,049 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.049 s for time 1560216395000 ms (execution: 0.043 s)
   [druid] 2019-06-11 09:26:35,049 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 421 from persistence list
   [druid] 2019-06-11 09:26:35,049 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 421
   [druid] 2019-06-11 09:26:35,049 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 416 from persistence list
   [druid] 2019-06-11 09:26:35,049 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 416
   [druid] 2019-06-11 09:26:35,049 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 415 from persistence list
   [druid] 2019-06-11 09:26:35,049 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 415
   [druid] 2019-06-11 09:26:35,049 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[415] at socketTextStream at Transform.scala:18 of time 1560216395000 ms
   [druid] 2019-06-11 09:26:35,049 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216385000 ms
   [druid] 2019-06-11 09:26:35,049 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216385000 ms
   [druid] 2019-06-11 09:26:40,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216400000 ms
   [druid] 2019-06-11 09:26:40,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216400000 ms.0 from job set of time 1560216400000 ms
   [druid] 2019-06-11 09:26:40,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 430 (map at Transform.scala:20)
   [druid] 2019-06-11 09:26:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:26:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 123 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 369 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 368, ShuffleMapStage 367)
   [druid] 2019-06-11 09:26:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 368)
   [druid] 2019-06-11 09:26:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 368 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:26:40,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_184 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:40,019 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_184_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:26:40,020 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_184_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:40,022 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_173_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:40,022 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 184 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:40,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 109
   [druid] 2019-06-11 09:26:40,024 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 368 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:26:40,024 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 368.0 with 2 tasks
   [druid] 2019-06-11 09:26:40,024 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 368.0 (TID 245, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:40,024 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_171_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:40,024 [er for task 245] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 368.0 (TID 245)
   [druid] 2019-06-11 09:26:40,025 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 116
   [druid] 2019-06-11 09:26:40,026 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_170_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:40,026 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 117
   [druid] 2019-06-11 09:26:40,027 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_178_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:40,028 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_175_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:40,029 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_169_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:40,029 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 107
   [druid] 2019-06-11 09:26:40,030 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 119
   [druid] 2019-06-11 09:26:40,031 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_183_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:40,032 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_179_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:40,033 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_176_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:40,034 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 113
   [druid] 2019-06-11 09:26:40,034 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_172_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:40,035 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 115
   [druid] 2019-06-11 09:26:40,035 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 112
   [druid] 2019-06-11 09:26:40,037 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_181_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:40,037 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_167_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:40,038 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_174_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:40,039 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 108
   [druid] 2019-06-11 09:26:40,039 [er for task 245] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 368.0 (TID 245). 810 bytes result sent to driver
   [druid] 2019-06-11 09:26:40,039 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 368.0 (TID 246, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:40,039 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_166_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:40,039 [er for task 246] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 368.0 (TID 246)
   [druid] 2019-06-11 09:26:40,039 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 368.0 (TID 245) in 15 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:26:40,041 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 118
   [druid] 2019-06-11 09:26:40,042 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_163_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:40,043 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_164_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:40,043 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 114
   [druid] 2019-06-11 09:26:40,043 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 106
   [druid] 2019-06-11 09:26:40,044 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_180_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:40,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 110
   [druid] 2019-06-11 09:26:40,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 111
   [druid] 2019-06-11 09:26:40,045 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_165_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:40,045 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_168_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:40,047 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_177_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:40,047 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_182_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:40,052 [er for task 246] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 368.0 (TID 246). 853 bytes result sent to driver
   [druid] 2019-06-11 09:26:40,053 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 368.0 (TID 246) in 14 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:26:40,053 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 368.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:40,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 368 (parallelize at Transform.scala:16) finished in 0.029 s
   [druid] 2019-06-11 09:26:40,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:26:40,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:26:40,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 369)
   [druid] 2019-06-11 09:26:40,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:26:40,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 369 (MapPartitionsRDD[435] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:40,054 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_185 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:40,054 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_185_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:40,055 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_185_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:40,055 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 185 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:40,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 369 (MapPartitionsRDD[435] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:26:40,055 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 369.0 with 1 tasks
   [druid] 2019-06-11 09:26:40,056 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 369.0 (TID 247, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:40,056 [er for task 247] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 369.0 (TID 247)
   [druid] 2019-06-11 09:26:40,057 [er for task 247] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:40,057 [er for task 247] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:40,057 [er for task 247] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:40,057 [er for task 247] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:40,058 [er for task 247] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 369.0 (TID 247). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:40,058 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 369.0 (TID 247) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:40,058 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 369.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:40,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 369 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:26:40,058 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 123 finished: print at Transform.scala:36, took 0.048939 s
   [druid] 2019-06-11 09:26:40,061 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:40,062 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 122 is 83 bytes
   [druid] 2019-06-11 09:26:40,062 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 123 is 160 bytes
   [druid] 2019-06-11 09:26:40,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 124 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:40,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 372 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:40,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 371, ShuffleMapStage 370)
   [druid] 2019-06-11 09:26:40,063 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:26:40,063 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 372 (MapPartitionsRDD[435] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:40,063 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_186 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:40,064 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_186_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:40,064 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_186_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:40,065 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 186 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:40,065 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 372 (MapPartitionsRDD[435] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:26:40,065 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 372.0 with 1 tasks
   [druid] 2019-06-11 09:26:40,065 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 372.0 (TID 248, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:40,065 [er for task 248] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 372.0 (TID 248)
   [druid] 2019-06-11 09:26:40,066 [er for task 248] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:40,066 [er for task 248] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:40,066 [er for task 248] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:40,066 [er for task 248] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:40,067 [er for task 248] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 372.0 (TID 248). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:40,068 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 372.0 (TID 248) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:40,068 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 372.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:40,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 372 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:26:40,068 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 124 finished: print at Transform.scala:36, took 0.006745 s
   [druid] 2019-06-11 09:26:40,069 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216400000 ms.0 from job set of time 1560216400000 ms
   [druid] 2019-06-11 09:26:40,069 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.069 s for time 1560216400000 ms (execution: 0.061 s)
   [druid] 2019-06-11 09:26:40,069 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 428 from persistence list
   [druid] 2019-06-11 09:26:40,069 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 428
   [druid] 2019-06-11 09:26:40,069 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 423 from persistence list
   [druid] 2019-06-11 09:26:40,069 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 423
   [druid] 2019-06-11 09:26:40,069 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 422 from persistence list
   [druid] 2019-06-11 09:26:40,069 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 422
   [druid] 2019-06-11 09:26:40,069 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[422] at socketTextStream at Transform.scala:18 of time 1560216400000 ms
   [druid] 2019-06-11 09:26:40,070 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216390000 ms
   [druid] 2019-06-11 09:26:40,070 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216390000 ms
   [druid] 2019-06-11 09:26:45,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216405000 ms
   [druid] 2019-06-11 09:26:45,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216405000 ms.0 from job set of time 1560216405000 ms
   [druid] 2019-06-11 09:26:45,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 437 (map at Transform.scala:20)
   [druid] 2019-06-11 09:26:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:26:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 125 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 375 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 374, ShuffleMapStage 373)
   [druid] 2019-06-11 09:26:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 374)
   [druid] 2019-06-11 09:26:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 374 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:26:45,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_187 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:45,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_187_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:26:45,011 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_187_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:45,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 187 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:45,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 374 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:26:45,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 374.0 with 2 tasks
   [druid] 2019-06-11 09:26:45,012 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 374.0 (TID 249, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:45,012 [er for task 249] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 374.0 (TID 249)
   [druid] 2019-06-11 09:26:45,025 [er for task 249] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 374.0 (TID 249). 810 bytes result sent to driver
   [druid] 2019-06-11 09:26:45,025 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 374.0 (TID 250, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:45,025 [er for task 250] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 374.0 (TID 250)
   [druid] 2019-06-11 09:26:45,025 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 374.0 (TID 249) in 13 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:26:45,035 [er for task 250] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 374.0 (TID 250). 853 bytes result sent to driver
   [druid] 2019-06-11 09:26:45,035 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 374.0 (TID 250) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:26:45,035 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 374.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 374 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:26:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:26:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:26:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 375)
   [druid] 2019-06-11 09:26:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:26:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 375 (MapPartitionsRDD[442] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:45,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_188 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:45,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_188_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:45,037 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_188_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:45,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 188 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 375 (MapPartitionsRDD[442] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:26:45,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 375.0 with 1 tasks
   [druid] 2019-06-11 09:26:45,038 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 375.0 (TID 251, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:45,038 [er for task 251] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 375.0 (TID 251)
   [druid] 2019-06-11 09:26:45,039 [er for task 251] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:45,039 [er for task 251] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:45,040 [er for task 251] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:45,040 [er for task 251] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:26:45,040 [er for task 251] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 375.0 (TID 251). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:45,041 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 375.0 (TID 251) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:45,041 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 375.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:45,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 375 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:26:45,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 125 finished: print at Transform.scala:36, took 0.032386 s
   [druid] 2019-06-11 09:26:45,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:45,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 124 is 83 bytes
   [druid] 2019-06-11 09:26:45,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 125 is 160 bytes
   [druid] 2019-06-11 09:26:45,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 126 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:45,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 378 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:45,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 376, ShuffleMapStage 377)
   [druid] 2019-06-11 09:26:45,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:26:45,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 378 (MapPartitionsRDD[442] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:45,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_189 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:45,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_189_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:45,046 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_189_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:45,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 189 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:45,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 378 (MapPartitionsRDD[442] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:26:45,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 378.0 with 1 tasks
   [druid] 2019-06-11 09:26:45,047 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 378.0 (TID 252, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:45,047 [er for task 252] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 378.0 (TID 252)
   [druid] 2019-06-11 09:26:45,048 [er for task 252] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:45,048 [er for task 252] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:45,048 [er for task 252] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:45,048 [er for task 252] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:45,049 [er for task 252] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 378.0 (TID 252). 966 bytes result sent to driver
   [druid] 2019-06-11 09:26:45,050 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 378.0 (TID 252) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:45,050 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 378.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 378 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:26:45,050 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 126 finished: print at Transform.scala:36, took 0.006474 s
   [druid] 2019-06-11 09:26:45,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216405000 ms.0 from job set of time 1560216405000 ms
   [druid] 2019-06-11 09:26:45,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.050 s for time 1560216405000 ms (execution: 0.043 s)
   [druid] 2019-06-11 09:26:45,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 435 from persistence list
   [druid] 2019-06-11 09:26:45,051 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 435
   [druid] 2019-06-11 09:26:45,051 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 430 from persistence list
   [druid] 2019-06-11 09:26:45,051 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 430
   [druid] 2019-06-11 09:26:45,051 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 429 from persistence list
   [druid] 2019-06-11 09:26:45,051 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 429
   [druid] 2019-06-11 09:26:45,051 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[429] at socketTextStream at Transform.scala:18 of time 1560216405000 ms
   [druid] 2019-06-11 09:26:45,051 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216395000 ms
   [druid] 2019-06-11 09:26:45,051 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216395000 ms
   [druid] 2019-06-11 09:26:50,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216410000 ms
   [druid] 2019-06-11 09:26:50,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216410000 ms.0 from job set of time 1560216410000 ms
   [druid] 2019-06-11 09:26:50,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:26:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 444 (map at Transform.scala:20)
   [druid] 2019-06-11 09:26:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 127 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 381 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 379, ShuffleMapStage 380)
   [druid] 2019-06-11 09:26:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 379)
   [druid] 2019-06-11 09:26:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 379 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:26:50,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_190 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:50,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_190_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:26:50,011 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_190_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:50,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 190 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:50,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 379 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:26:50,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 379.0 with 2 tasks
   [druid] 2019-06-11 09:26:50,012 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 379.0 (TID 253, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:50,012 [er for task 253] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 379.0 (TID 253)
   [druid] 2019-06-11 09:26:50,022 [er for task 253] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 379.0 (TID 253). 853 bytes result sent to driver
   [druid] 2019-06-11 09:26:50,023 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 379.0 (TID 254, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:50,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 379.0 (TID 253) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:26:50,023 [er for task 254] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 379.0 (TID 254)
   [druid] 2019-06-11 09:26:50,037 [er for task 254] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 379.0 (TID 254). 810 bytes result sent to driver
   [druid] 2019-06-11 09:26:50,037 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 379.0 (TID 254) in 15 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:26:50,037 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 379.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:50,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 379 (parallelize at Transform.scala:16) finished in 0.026 s
   [druid] 2019-06-11 09:26:50,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:26:50,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:26:50,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 381)
   [druid] 2019-06-11 09:26:50,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:26:50,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 381 (MapPartitionsRDD[449] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:50,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_191 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:26:50,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_191_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:50,041 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_191_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:50,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 191 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:50,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 381 (MapPartitionsRDD[449] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:26:50,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 381.0 with 1 tasks
   [druid] 2019-06-11 09:26:50,042 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 381.0 (TID 255, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:50,042 [er for task 255] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 381.0 (TID 255)
   [druid] 2019-06-11 09:26:50,043 [er for task 255] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:50,043 [er for task 255] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:50,043 [er for task 255] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:50,043 [er for task 255] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:50,043 [er for task 255] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 381.0 (TID 255). 966 bytes result sent to driver
   [druid] 2019-06-11 09:26:50,044 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 381.0 (TID 255) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:50,044 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 381.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 381 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:26:50,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 127 finished: print at Transform.scala:36, took 0.035413 s
   [druid] 2019-06-11 09:26:50,047 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:50,048 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 127 is 160 bytes
   [druid] 2019-06-11 09:26:50,048 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 126 is 83 bytes
   [druid] 2019-06-11 09:26:50,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 128 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:50,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 384 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:50,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 383, ShuffleMapStage 382)
   [druid] 2019-06-11 09:26:50,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:26:50,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 384 (MapPartitionsRDD[449] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:50,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_192 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:50,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_192_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:50,050 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_192_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:26:50,051 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 192 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:50,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 384 (MapPartitionsRDD[449] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:26:50,051 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 384.0 with 1 tasks
   [druid] 2019-06-11 09:26:50,051 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 384.0 (TID 256, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:50,051 [er for task 256] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 384.0 (TID 256)
   [druid] 2019-06-11 09:26:50,053 [er for task 256] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:50,053 [er for task 256] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:50,053 [er for task 256] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:50,053 [er for task 256] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:50,055 [er for task 256] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 384.0 (TID 256). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:50,055 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 384.0 (TID 256) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:50,055 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 384.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:50,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 384 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:26:50,056 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 128 finished: print at Transform.scala:36, took 0.008395 s
   [druid] 2019-06-11 09:26:50,056 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216410000 ms.0 from job set of time 1560216410000 ms
   [druid] 2019-06-11 09:26:50,056 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.056 s for time 1560216410000 ms (execution: 0.050 s)
   [druid] 2019-06-11 09:26:50,056 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 442 from persistence list
   [druid] 2019-06-11 09:26:50,056 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 442
   [druid] 2019-06-11 09:26:50,056 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 437 from persistence list
   [druid] 2019-06-11 09:26:50,056 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 437
   [druid] 2019-06-11 09:26:50,056 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 436 from persistence list
   [druid] 2019-06-11 09:26:50,057 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 436
   [druid] 2019-06-11 09:26:50,057 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[436] at socketTextStream at Transform.scala:18 of time 1560216410000 ms
   [druid] 2019-06-11 09:26:50,057 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216400000 ms
   [druid] 2019-06-11 09:26:50,057 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216400000 ms
   [druid] 2019-06-11 09:26:55,011 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216415000 ms
   [druid] 2019-06-11 09:26:55,011 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216415000 ms.0 from job set of time 1560216415000 ms
   [druid] 2019-06-11 09:26:55,013 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 451 (map at Transform.scala:20)
   [druid] 2019-06-11 09:26:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:26:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 129 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 387 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 386, ShuffleMapStage 385)
   [druid] 2019-06-11 09:26:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 386)
   [druid] 2019-06-11 09:26:55,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 386 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:26:55,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_193 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:55,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_193_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:26:55,015 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_193_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:55,016 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 193 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:55,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 386 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:26:55,016 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 386.0 with 2 tasks
   [druid] 2019-06-11 09:26:55,016 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 386.0 (TID 257, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:55,016 [er for task 257] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 386.0 (TID 257)
   [druid] 2019-06-11 09:26:55,026 [er for task 257] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 386.0 (TID 257). 810 bytes result sent to driver
   [druid] 2019-06-11 09:26:55,026 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 386.0 (TID 258, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:26:55,026 [er for task 258] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 386.0 (TID 258)
   [druid] 2019-06-11 09:26:55,026 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 386.0 (TID 257) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:26:55,034 [er for task 258] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 386.0 (TID 258). 853 bytes result sent to driver
   [druid] 2019-06-11 09:26:55,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 386.0 (TID 258) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:26:55,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 386.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 386 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:26:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:26:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:26:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 387)
   [druid] 2019-06-11 09:26:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:26:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 387 (MapPartitionsRDD[456] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:55,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_194 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:55,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_194_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:55,037 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_194_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:55,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 194 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 387 (MapPartitionsRDD[456] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:26:55,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 387.0 with 1 tasks
   [druid] 2019-06-11 09:26:55,038 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 387.0 (TID 259, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:55,038 [er for task 259] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 387.0 (TID 259)
   [druid] 2019-06-11 09:26:55,039 [er for task 259] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:55,039 [er for task 259] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:55,039 [er for task 259] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:55,039 [er for task 259] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:55,040 [er for task 259] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 387.0 (TID 259). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:26:55,041 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 387.0 (TID 259) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:55,041 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 387.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:55,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 387 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:26:55,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 129 finished: print at Transform.scala:36, took 0.027964 s
   [druid] 2019-06-11 09:26:55,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:26:55,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 128 is 83 bytes
   [druid] 2019-06-11 09:26:55,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 129 is 160 bytes
   [druid] 2019-06-11 09:26:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 130 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:26:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 390 (print at Transform.scala:36)
   [druid] 2019-06-11 09:26:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 389, ShuffleMapStage 388)
   [druid] 2019-06-11 09:26:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:26:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 390 (MapPartitionsRDD[456] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:26:55,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_195 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:55,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_195_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:26:55,046 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_195_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:26:55,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 195 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:26:55,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 390 (MapPartitionsRDD[456] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:26:55,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 390.0 with 1 tasks
   [druid] 2019-06-11 09:26:55,046 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 390.0 (TID 260, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:26:55,046 [er for task 260] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 390.0 (TID 260)
   [druid] 2019-06-11 09:26:55,047 [er for task 260] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:26:55,047 [er for task 260] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:55,047 [er for task 260] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:26:55,047 [er for task 260] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:26:55,049 [er for task 260] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 390.0 (TID 260). 966 bytes result sent to driver
   [druid] 2019-06-11 09:26:55,049 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 390.0 (TID 260) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:26:55,049 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 390.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:26:55,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 390 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:26:55,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 130 finished: print at Transform.scala:36, took 0.005941 s
   [druid] 2019-06-11 09:26:55,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216415000 ms.0 from job set of time 1560216415000 ms
   [druid] 2019-06-11 09:26:55,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.050 s for time 1560216415000 ms (execution: 0.039 s)
   [druid] 2019-06-11 09:26:55,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 449 from persistence list
   [druid] 2019-06-11 09:26:55,050 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 449
   [druid] 2019-06-11 09:26:55,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 444 from persistence list
   [druid] 2019-06-11 09:26:55,050 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 444
   [druid] 2019-06-11 09:26:55,050 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 443 from persistence list
   [druid] 2019-06-11 09:26:55,050 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 443
   [druid] 2019-06-11 09:26:55,050 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[443] at socketTextStream at Transform.scala:18 of time 1560216415000 ms
   [druid] 2019-06-11 09:26:55,051 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216405000 ms
   [druid] 2019-06-11 09:26:55,051 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216405000 ms
   [druid] 2019-06-11 09:27:00,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216420000 ms
   [druid] 2019-06-11 09:27:00,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216420000 ms.0 from job set of time 1560216420000 ms
   [druid] 2019-06-11 09:27:00,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:27:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 458 (map at Transform.scala:20)
   [druid] 2019-06-11 09:27:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 131 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 393 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 391, ShuffleMapStage 392)
   [druid] 2019-06-11 09:27:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 391)
   [druid] 2019-06-11 09:27:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 391 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:27:00,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_196 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:00,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_196_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:27:00,012 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_196_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:00,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 196 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 391 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:27:00,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 391.0 with 2 tasks
   [druid] 2019-06-11 09:27:00,014 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 391.0 (TID 261, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:00,014 [er for task 261] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 391.0 (TID 261)
   [druid] 2019-06-11 09:27:00,022 [er for task 261] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 391.0 (TID 261). 810 bytes result sent to driver
   [druid] 2019-06-11 09:27:00,022 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 391.0 (TID 262, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:00,023 [er for task 262] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 391.0 (TID 262)
   [druid] 2019-06-11 09:27:00,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 391.0 (TID 261) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:27:00,030 [er for task 262] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 391.0 (TID 262). 810 bytes result sent to driver
   [druid] 2019-06-11 09:27:00,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 391.0 (TID 262) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:27:00,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 391.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 391 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:27:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:27:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:27:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 393)
   [druid] 2019-06-11 09:27:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:27:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 393 (MapPartitionsRDD[463] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:00,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_197 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:00,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_197_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:00,033 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_197_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:00,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 197 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 393 (MapPartitionsRDD[463] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:27:00,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 393.0 with 1 tasks
   [druid] 2019-06-11 09:27:00,033 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 393.0 (TID 263, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:00,033 [er for task 263] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 393.0 (TID 263)
   [druid] 2019-06-11 09:27:00,034 [er for task 263] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:00,034 [er for task 263] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:00,034 [er for task 263] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:00,034 [er for task 263] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:00,035 [er for task 263] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 393.0 (TID 263). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:27:00,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 393.0 (TID 263) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:00,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 393.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:00,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 393 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:27:00,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 131 finished: print at Transform.scala:36, took 0.026352 s
   [druid] 2019-06-11 09:27:00,038 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:00,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 131 is 160 bytes
   [druid] 2019-06-11 09:27:00,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 130 is 83 bytes
   [druid] 2019-06-11 09:27:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 132 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 396 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 394, ShuffleMapStage 395)
   [druid] 2019-06-11 09:27:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:27:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 396 (MapPartitionsRDD[463] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:00,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_198 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:00,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_198_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:00,041 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_198_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:00,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 198 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:00,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 396 (MapPartitionsRDD[463] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:27:00,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 396.0 with 1 tasks
   [druid] 2019-06-11 09:27:00,042 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 396.0 (TID 264, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:00,042 [er for task 264] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 396.0 (TID 264)
   [druid] 2019-06-11 09:27:00,042 [er for task 264] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:00,042 [er for task 264] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:00,042 [er for task 264] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:00,042 [er for task 264] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:00,043 [er for task 264] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 396.0 (TID 264). 966 bytes result sent to driver
   [druid] 2019-06-11 09:27:00,043 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 396.0 (TID 264) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:00,043 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 396.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:00,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 396 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:27:00,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 132 finished: print at Transform.scala:36, took 0.005610 s
   [druid] 2019-06-11 09:27:00,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216420000 ms.0 from job set of time 1560216420000 ms
   [druid] 2019-06-11 09:27:00,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.044 s for time 1560216420000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:27:00,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 456 from persistence list
   [druid] 2019-06-11 09:27:00,044 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 456
   [druid] 2019-06-11 09:27:00,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 451 from persistence list
   [druid] 2019-06-11 09:27:00,044 [c-thread-pool-7] INFO  che.spark.storage.BlockManager {1} - Removing RDD 451
   [druid] 2019-06-11 09:27:00,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 450 from persistence list
   [druid] 2019-06-11 09:27:00,045 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 450
   [druid] 2019-06-11 09:27:00,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[450] at socketTextStream at Transform.scala:18 of time 1560216420000 ms
   [druid] 2019-06-11 09:27:00,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216410000 ms
   [druid] 2019-06-11 09:27:00,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216410000 ms
   [druid] 2019-06-11 09:27:05,009 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216425000 ms
   [druid] 2019-06-11 09:27:05,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216425000 ms.0 from job set of time 1560216425000 ms
   [druid] 2019-06-11 09:27:05,011 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:05,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:27:05,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 465 (map at Transform.scala:20)
   [druid] 2019-06-11 09:27:05,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 133 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:05,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 399 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:05,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 397, ShuffleMapStage 398)
   [druid] 2019-06-11 09:27:05,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 397)
   [druid] 2019-06-11 09:27:05,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 397 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:27:05,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_199 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:05,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_199_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:27:05,014 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_199_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:05,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 199 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:05,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 397 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:27:05,014 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 397.0 with 2 tasks
   [druid] 2019-06-11 09:27:05,014 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 397.0 (TID 265, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:05,015 [er for task 265] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 397.0 (TID 265)
   [druid] 2019-06-11 09:27:05,023 [er for task 265] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 397.0 (TID 265). 810 bytes result sent to driver
   [druid] 2019-06-11 09:27:05,023 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 397.0 (TID 266, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:05,023 [er for task 266] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 397.0 (TID 266)
   [druid] 2019-06-11 09:27:05,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 397.0 (TID 265) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:27:05,031 [er for task 266] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 397.0 (TID 266). 853 bytes result sent to driver
   [druid] 2019-06-11 09:27:05,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 397.0 (TID 266) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:27:05,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 397.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 397 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:27:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:27:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:27:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 399)
   [druid] 2019-06-11 09:27:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:27:05,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 399 (MapPartitionsRDD[470] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:05,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_200 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:05,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_200_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:05,033 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_200_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:05,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 200 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:05,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 399 (MapPartitionsRDD[470] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:27:05,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 399.0 with 1 tasks
   [druid] 2019-06-11 09:27:05,034 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 399.0 (TID 267, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:05,034 [er for task 267] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 399.0 (TID 267)
   [druid] 2019-06-11 09:27:05,034 [er for task 267] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:05,034 [er for task 267] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:05,034 [er for task 267] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:05,034 [er for task 267] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:05,035 [er for task 267] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 399.0 (TID 267). 966 bytes result sent to driver
   [druid] 2019-06-11 09:27:05,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 399.0 (TID 267) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:05,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 399.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 399 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:27:05,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 133 finished: print at Transform.scala:36, took 0.024050 s
   [druid] 2019-06-11 09:27:05,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:05,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 133 is 160 bytes
   [druid] 2019-06-11 09:27:05,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 132 is 83 bytes
   [druid] 2019-06-11 09:27:05,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 134 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:05,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 402 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:05,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 401, ShuffleMapStage 400)
   [druid] 2019-06-11 09:27:05,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:27:05,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 402 (MapPartitionsRDD[470] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:05,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_201 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:05,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_201_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:05,042 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_201_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:05,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 201 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:05,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 402 (MapPartitionsRDD[470] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:27:05,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 402.0 with 1 tasks
   [druid] 2019-06-11 09:27:05,042 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 402.0 (TID 268, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:05,042 [er for task 268] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 402.0 (TID 268)
   [druid] 2019-06-11 09:27:05,043 [er for task 268] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:05,043 [er for task 268] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:05,043 [er for task 268] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:05,043 [er for task 268] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:05,044 [er for task 268] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 402.0 (TID 268). 966 bytes result sent to driver
   [druid] 2019-06-11 09:27:05,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 402.0 (TID 268) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:05,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 402.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:05,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 402 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:27:05,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 134 finished: print at Transform.scala:36, took 0.005075 s
   [druid] 2019-06-11 09:27:05,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216425000 ms.0 from job set of time 1560216425000 ms
   [druid] 2019-06-11 09:27:05,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560216425000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:27:05,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 463 from persistence list
   [druid] 2019-06-11 09:27:05,045 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 463
   [druid] 2019-06-11 09:27:05,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 458 from persistence list
   [druid] 2019-06-11 09:27:05,045 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 458
   [druid] 2019-06-11 09:27:05,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 457 from persistence list
   [druid] 2019-06-11 09:27:05,046 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 457
   [druid] 2019-06-11 09:27:05,046 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[457] at socketTextStream at Transform.scala:18 of time 1560216425000 ms
   [druid] 2019-06-11 09:27:05,046 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216415000 ms
   [druid] 2019-06-11 09:27:05,046 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216415000 ms
   [druid] 2019-06-11 09:27:10,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216430000 ms
   [druid] 2019-06-11 09:27:10,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216430000 ms.0 from job set of time 1560216430000 ms
   [druid] 2019-06-11 09:27:10,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 472 (map at Transform.scala:20)
   [druid] 2019-06-11 09:27:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:27:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 135 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 405 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 404, ShuffleMapStage 403)
   [druid] 2019-06-11 09:27:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 404)
   [druid] 2019-06-11 09:27:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 404 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:27:10,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_202 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:10,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_202_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:27:10,010 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_202_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:10,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 202 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:10,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 404 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:27:10,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 404.0 with 2 tasks
   [druid] 2019-06-11 09:27:10,011 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 404.0 (TID 269, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:10,012 [er for task 269] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 404.0 (TID 269)
   [druid] 2019-06-11 09:27:10,025 [er for task 269] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 404.0 (TID 269). 767 bytes result sent to driver
   [druid] 2019-06-11 09:27:10,025 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 404.0 (TID 270, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:10,025 [er for task 270] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 404.0 (TID 270)
   [druid] 2019-06-11 09:27:10,025 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 404.0 (TID 269) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:27:10,033 [er for task 270] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 404.0 (TID 270). 853 bytes result sent to driver
   [druid] 2019-06-11 09:27:10,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 404.0 (TID 270) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:27:10,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 404.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 404 (parallelize at Transform.scala:16) finished in 0.022 s
   [druid] 2019-06-11 09:27:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:27:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:27:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 405)
   [druid] 2019-06-11 09:27:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:27:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 405 (MapPartitionsRDD[477] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:10,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_203 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:10,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_203_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:10,036 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_203_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:10,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 203 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:10,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 405 (MapPartitionsRDD[477] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:27:10,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 405.0 with 1 tasks
   [druid] 2019-06-11 09:27:10,037 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 405.0 (TID 271, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:10,037 [er for task 271] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 405.0 (TID 271)
   [druid] 2019-06-11 09:27:10,038 [er for task 271] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:10,038 [er for task 271] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:10,038 [er for task 271] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:10,038 [er for task 271] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:10,038 [er for task 271] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 405.0 (TID 271). 966 bytes result sent to driver
   [druid] 2019-06-11 09:27:10,039 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 405.0 (TID 271) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:10,039 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 405.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:10,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 405 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:27:10,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 135 finished: print at Transform.scala:36, took 0.030598 s
   [druid] 2019-06-11 09:27:10,041 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:10,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 134 is 83 bytes
   [druid] 2019-06-11 09:27:10,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 135 is 160 bytes
   [druid] 2019-06-11 09:27:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 136 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 408 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 407, ShuffleMapStage 406)
   [druid] 2019-06-11 09:27:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:27:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 408 (MapPartitionsRDD[477] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:10,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_204 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:10,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_204_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:10,044 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_204_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:10,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 204 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:10,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 408 (MapPartitionsRDD[477] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:27:10,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 408.0 with 1 tasks
   [druid] 2019-06-11 09:27:10,044 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 408.0 (TID 272, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:10,045 [er for task 272] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 408.0 (TID 272)
   [druid] 2019-06-11 09:27:10,045 [er for task 272] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:10,045 [er for task 272] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:10,045 [er for task 272] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:10,045 [er for task 272] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:10,046 [er for task 272] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 408.0 (TID 272). 923 bytes result sent to driver
   [druid] 2019-06-11 09:27:10,046 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 408.0 (TID 272) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:10,046 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 408.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:10,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 408 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:27:10,047 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 136 finished: print at Transform.scala:36, took 0.005507 s
   [druid] 2019-06-11 09:27:10,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216430000 ms.0 from job set of time 1560216430000 ms
   [druid] 2019-06-11 09:27:10,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.047 s for time 1560216430000 ms (execution: 0.041 s)
   [druid] 2019-06-11 09:27:10,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 470 from persistence list
   [druid] 2019-06-11 09:27:10,047 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 470
   [druid] 2019-06-11 09:27:10,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 465 from persistence list
   [druid] 2019-06-11 09:27:10,047 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 465
   [druid] 2019-06-11 09:27:10,047 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 464 from persistence list
   [druid] 2019-06-11 09:27:10,048 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 464
   [druid] 2019-06-11 09:27:10,048 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[464] at socketTextStream at Transform.scala:18 of time 1560216430000 ms
   [druid] 2019-06-11 09:27:10,048 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216420000 ms
   [druid] 2019-06-11 09:27:10,048 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216420000 ms
   [druid] 2019-06-11 09:27:15,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216435000 ms
   [druid] 2019-06-11 09:27:15,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216435000 ms.0 from job set of time 1560216435000 ms
   [druid] 2019-06-11 09:27:15,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 479 (map at Transform.scala:20)
   [druid] 2019-06-11 09:27:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:27:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 137 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 411 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 409, ShuffleMapStage 410)
   [druid] 2019-06-11 09:27:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 410)
   [druid] 2019-06-11 09:27:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 410 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:27:15,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_205 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:15,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_205_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:27:15,015 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_205_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:15,017 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 129
   [druid] 2019-06-11 09:27:15,018 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 205 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:15,020 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_191_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:15,020 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 410 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:27:15,020 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 410.0 with 2 tasks
   [druid] 2019-06-11 09:27:15,021 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 133
   [druid] 2019-06-11 09:27:15,021 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 410.0 (TID 273, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:15,021 [er for task 273] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 410.0 (TID 273)
   [druid] 2019-06-11 09:27:15,022 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_204_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:15,022 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 125
   [druid] 2019-06-11 09:27:15,022 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 124
   [druid] 2019-06-11 09:27:15,023 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_195_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:15,024 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_194_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:15,025 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 130
   [druid] 2019-06-11 09:27:15,025 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_184_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:15,026 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_190_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:15,027 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_198_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:15,028 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_202_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:15,029 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_185_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:15,030 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 131
   [druid] 2019-06-11 09:27:15,031 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_199_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:15,031 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 123
   [druid] 2019-06-11 09:27:15,032 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_201_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:15,032 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 122
   [druid] 2019-06-11 09:27:15,033 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 120
   [druid] 2019-06-11 09:27:15,033 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_193_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:15,034 [er for task 273] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 410.0 (TID 273). 810 bytes result sent to driver
   [druid] 2019-06-11 09:27:15,034 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 410.0 (TID 274, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:15,034 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_200_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:15,034 [er for task 274] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 410.0 (TID 274)
   [druid] 2019-06-11 09:27:15,034 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 410.0 (TID 273) in 13 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:27:15,034 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 132
   [druid] 2019-06-11 09:27:15,035 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_203_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:15,036 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_192_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:15,036 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 128
   [druid] 2019-06-11 09:27:15,037 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_189_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:15,038 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_188_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:15,039 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_196_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:15,039 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 121
   [druid] 2019-06-11 09:27:15,040 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 127
   [druid] 2019-06-11 09:27:15,041 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 126
   [druid] 2019-06-11 09:27:15,041 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_187_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:15,042 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_197_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:15,043 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_186_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:15,049 [er for task 274] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 410.0 (TID 274). 810 bytes result sent to driver
   [druid] 2019-06-11 09:27:15,050 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 410.0 (TID 274) in 16 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:27:15,050 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 410.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:15,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 410 (parallelize at Transform.scala:16) finished in 0.029 s
   [druid] 2019-06-11 09:27:15,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:27:15,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:27:15,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 411)
   [druid] 2019-06-11 09:27:15,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:27:15,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 411 (MapPartitionsRDD[484] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:15,051 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_206 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:27:15,052 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_206_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:27:15,053 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_206_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:15,053 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 206 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:15,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 411 (MapPartitionsRDD[484] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:27:15,053 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 411.0 with 1 tasks
   [druid] 2019-06-11 09:27:15,053 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 411.0 (TID 275, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:15,053 [er for task 275] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 411.0 (TID 275)
   [druid] 2019-06-11 09:27:15,054 [er for task 275] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:15,054 [er for task 275] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:15,054 [er for task 275] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:15,054 [er for task 275] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:15,055 [er for task 275] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 411.0 (TID 275). 966 bytes result sent to driver
   [druid] 2019-06-11 09:27:15,055 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 411.0 (TID 275) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:15,055 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 411.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:15,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 411 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:27:15,055 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 137 finished: print at Transform.scala:36, took 0.048264 s
   [druid] 2019-06-11 09:27:15,058 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:15,058 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 136 is 83 bytes
   [druid] 2019-06-11 09:27:15,058 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 137 is 160 bytes
   [druid] 2019-06-11 09:27:15,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 138 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:15,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 414 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:15,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 412, ShuffleMapStage 413)
   [druid] 2019-06-11 09:27:15,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:27:15,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 414 (MapPartitionsRDD[484] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:15,059 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_207 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:27:15,059 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_207_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:27:15,060 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_207_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:15,060 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 207 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:15,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 414 (MapPartitionsRDD[484] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:27:15,060 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 414.0 with 1 tasks
   [druid] 2019-06-11 09:27:15,060 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 414.0 (TID 276, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:15,060 [er for task 276] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 414.0 (TID 276)
   [druid] 2019-06-11 09:27:15,061 [er for task 276] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:15,061 [er for task 276] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:15,061 [er for task 276] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:15,061 [er for task 276] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:15,062 [er for task 276] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 414.0 (TID 276). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:27:15,062 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 414.0 (TID 276) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:15,062 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 414.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:15,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 414 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:27:15,063 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 138 finished: print at Transform.scala:36, took 0.004874 s
   [druid] 2019-06-11 09:27:15,063 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216435000 ms.0 from job set of time 1560216435000 ms
   [druid] 2019-06-11 09:27:15,063 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.063 s for time 1560216435000 ms (execution: 0.058 s)
   [druid] 2019-06-11 09:27:15,063 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 477 from persistence list
   [druid] 2019-06-11 09:27:15,063 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 472 from persistence list
   [druid] 2019-06-11 09:27:15,063 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 477
   [druid] 2019-06-11 09:27:15,063 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 471 from persistence list
   [druid] 2019-06-11 09:27:15,063 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 472
   [druid] 2019-06-11 09:27:15,064 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 471
   [druid] 2019-06-11 09:27:15,064 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[471] at socketTextStream at Transform.scala:18 of time 1560216435000 ms
   [druid] 2019-06-11 09:27:15,064 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216425000 ms
   [druid] 2019-06-11 09:27:15,064 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216425000 ms
   [druid] 2019-06-11 09:27:20,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216440000 ms
   [druid] 2019-06-11 09:27:20,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216440000 ms.0 from job set of time 1560216440000 ms
   [druid] 2019-06-11 09:27:20,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:27:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 486 (map at Transform.scala:20)
   [druid] 2019-06-11 09:27:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 139 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 417 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 416, ShuffleMapStage 415)
   [druid] 2019-06-11 09:27:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 415)
   [druid] 2019-06-11 09:27:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 415 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:27:20,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_208 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:27:20,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_208_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:27:20,010 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_208_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:20,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 208 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:20,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 415 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:27:20,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 415.0 with 2 tasks
   [druid] 2019-06-11 09:27:20,011 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 415.0 (TID 277, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:20,011 [er for task 277] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 415.0 (TID 277)
   [druid] 2019-06-11 09:27:20,020 [er for task 277] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 415.0 (TID 277). 810 bytes result sent to driver
   [druid] 2019-06-11 09:27:20,021 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 415.0 (TID 278, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:20,021 [er for task 278] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 415.0 (TID 278)
   [druid] 2019-06-11 09:27:20,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 415.0 (TID 277) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:27:20,029 [er for task 278] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 415.0 (TID 278). 767 bytes result sent to driver
   [druid] 2019-06-11 09:27:20,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 415.0 (TID 278) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:27:20,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 415.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 415 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:27:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:27:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:27:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 417)
   [druid] 2019-06-11 09:27:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:27:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 417 (MapPartitionsRDD[491] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:20,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_209 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:27:20,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_209_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:27:20,031 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_209_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:20,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 209 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 417 (MapPartitionsRDD[491] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:27:20,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 417.0 with 1 tasks
   [druid] 2019-06-11 09:27:20,031 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 417.0 (TID 279, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:20,032 [er for task 279] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 417.0 (TID 279)
   [druid] 2019-06-11 09:27:20,033 [er for task 279] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:20,033 [er for task 279] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:20,033 [er for task 279] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:20,033 [er for task 279] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:20,034 [er for task 279] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 417.0 (TID 279). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:27:20,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 417.0 (TID 279) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:20,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 417.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:20,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 417 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:27:20,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 139 finished: print at Transform.scala:36, took 0.026491 s
   [druid] 2019-06-11 09:27:20,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:20,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 139 is 160 bytes
   [druid] 2019-06-11 09:27:20,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 138 is 83 bytes
   [druid] 2019-06-11 09:27:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 140 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 420 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 419, ShuffleMapStage 418)
   [druid] 2019-06-11 09:27:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:27:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 420 (MapPartitionsRDD[491] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:20,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_210 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:27:20,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_210_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:27:20,039 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_210_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:20,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 210 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 420 (MapPartitionsRDD[491] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:27:20,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 420.0 with 1 tasks
   [druid] 2019-06-11 09:27:20,040 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 420.0 (TID 280, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:20,040 [er for task 280] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 420.0 (TID 280)
   [druid] 2019-06-11 09:27:20,041 [er for task 280] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:20,041 [er for task 280] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:20,041 [er for task 280] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:20,041 [er for task 280] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:20,042 [er for task 280] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 420.0 (TID 280). 966 bytes result sent to driver
   [druid] 2019-06-11 09:27:20,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 420.0 (TID 280) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:20,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 420.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:20,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 420 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:27:20,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 140 finished: print at Transform.scala:36, took 0.005288 s
   [druid] 2019-06-11 09:27:20,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216440000 ms.0 from job set of time 1560216440000 ms
   [druid] 2019-06-11 09:27:20,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560216440000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:27:20,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 484 from persistence list
   [druid] 2019-06-11 09:27:20,043 [c-thread-pool-3] INFO  che.spark.storage.BlockManager {1} - Removing RDD 484
   [druid] 2019-06-11 09:27:20,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 479 from persistence list
   [druid] 2019-06-11 09:27:20,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 478 from persistence list
   [druid] 2019-06-11 09:27:20,043 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 479
   [druid] 2019-06-11 09:27:20,043 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 478
   [druid] 2019-06-11 09:27:20,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[478] at socketTextStream at Transform.scala:18 of time 1560216440000 ms
   [druid] 2019-06-11 09:27:20,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216430000 ms
   [druid] 2019-06-11 09:27:20,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216430000 ms
   [druid] 2019-06-11 09:27:25,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216445000 ms
   [druid] 2019-06-11 09:27:25,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216445000 ms.0 from job set of time 1560216445000 ms
   [druid] 2019-06-11 09:27:25,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:27:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 493 (map at Transform.scala:20)
   [druid] 2019-06-11 09:27:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 141 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 423 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 422, ShuffleMapStage 421)
   [druid] 2019-06-11 09:27:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 421)
   [druid] 2019-06-11 09:27:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 421 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:27:25,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_211 stored as values in memory (estimated size 2.0 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:27:25,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_211_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.5 MB)
   [druid] 2019-06-11 09:27:25,011 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_211_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:25,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 211 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 421 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:27:25,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 421.0 with 2 tasks
   [druid] 2019-06-11 09:27:25,012 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 421.0 (TID 281, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:25,012 [er for task 281] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 421.0 (TID 281)
   [druid] 2019-06-11 09:27:25,020 [er for task 281] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 421.0 (TID 281). 853 bytes result sent to driver
   [druid] 2019-06-11 09:27:25,021 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 421.0 (TID 282, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:25,021 [er for task 282] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 421.0 (TID 282)
   [druid] 2019-06-11 09:27:25,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 421.0 (TID 281) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:27:25,029 [er for task 282] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 421.0 (TID 282). 767 bytes result sent to driver
   [druid] 2019-06-11 09:27:25,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 421.0 (TID 282) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:27:25,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 421.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 421 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:27:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:27:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:27:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 423)
   [druid] 2019-06-11 09:27:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:27:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 423 (MapPartitionsRDD[498] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:25,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_212 stored as values in memory (estimated size 3.7 KB, free 1989.5 MB)
   [druid] 2019-06-11 09:27:25,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_212_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:25,031 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_212_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:25,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 212 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 423 (MapPartitionsRDD[498] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:27:25,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 423.0 with 1 tasks
   [druid] 2019-06-11 09:27:25,032 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 423.0 (TID 283, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:25,032 [er for task 283] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 423.0 (TID 283)
   [druid] 2019-06-11 09:27:25,033 [er for task 283] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:25,033 [er for task 283] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:25,033 [er for task 283] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:25,033 [er for task 283] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:25,034 [er for task 283] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 423.0 (TID 283). 966 bytes result sent to driver
   [druid] 2019-06-11 09:27:25,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 423.0 (TID 283) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:25,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 423.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 423 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:27:25,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 141 finished: print at Transform.scala:36, took 0.025372 s
   [druid] 2019-06-11 09:27:25,038 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:25,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 141 is 160 bytes
   [druid] 2019-06-11 09:27:25,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 140 is 83 bytes
   [druid] 2019-06-11 09:27:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 142 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 426 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:25,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 425, ShuffleMapStage 424)
   [druid] 2019-06-11 09:27:25,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:27:25,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 426 (MapPartitionsRDD[498] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:25,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_213 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:25,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_213_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:25,040 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_213_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.6 MB)
   [druid] 2019-06-11 09:27:25,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 213 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:25,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 426 (MapPartitionsRDD[498] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:27:25,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 426.0 with 1 tasks
   [druid] 2019-06-11 09:27:25,041 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 426.0 (TID 284, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:25,041 [er for task 284] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 426.0 (TID 284)
   [druid] 2019-06-11 09:27:25,041 [er for task 284] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:25,041 [er for task 284] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:25,041 [er for task 284] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:25,041 [er for task 284] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:25,042 [er for task 284] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 426.0 (TID 284). 966 bytes result sent to driver
   [druid] 2019-06-11 09:27:25,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 426.0 (TID 284) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:25,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 426.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:25,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 426 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:27:25,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 142 finished: print at Transform.scala:36, took 0.005509 s
   [druid] 2019-06-11 09:27:25,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216445000 ms.0 from job set of time 1560216445000 ms
   [druid] 2019-06-11 09:27:25,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216445000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:27:25,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 491 from persistence list
   [druid] 2019-06-11 09:27:25,043 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 491
   [druid] 2019-06-11 09:27:25,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 486 from persistence list
   [druid] 2019-06-11 09:27:25,044 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 486
   [druid] 2019-06-11 09:27:25,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 485 from persistence list
   [druid] 2019-06-11 09:27:25,044 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 485
   [druid] 2019-06-11 09:27:25,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[485] at socketTextStream at Transform.scala:18 of time 1560216445000 ms
   [druid] 2019-06-11 09:27:25,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216435000 ms
   [druid] 2019-06-11 09:27:25,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216435000 ms
   [druid] 2019-06-11 09:27:30,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216450000 ms
   [druid] 2019-06-11 09:27:30,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216450000 ms.0 from job set of time 1560216450000 ms
   [druid] 2019-06-11 09:27:30,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:27:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 500 (map at Transform.scala:20)
   [druid] 2019-06-11 09:27:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 143 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 429 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 427, ShuffleMapStage 428)
   [druid] 2019-06-11 09:27:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 427)
   [druid] 2019-06-11 09:27:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 427 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:27:30,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_214 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:30,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_214_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:27:30,011 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_214_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:30,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 214 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 427 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:27:30,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 427.0 with 2 tasks
   [druid] 2019-06-11 09:27:30,011 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 427.0 (TID 285, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:30,011 [er for task 285] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 427.0 (TID 285)
   [druid] 2019-06-11 09:27:30,021 [er for task 285] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 427.0 (TID 285). 853 bytes result sent to driver
   [druid] 2019-06-11 09:27:30,021 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 427.0 (TID 286, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:30,021 [er for task 286] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 427.0 (TID 286)
   [druid] 2019-06-11 09:27:30,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 427.0 (TID 285) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:27:30,029 [er for task 286] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 427.0 (TID 286). 810 bytes result sent to driver
   [druid] 2019-06-11 09:27:30,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 427.0 (TID 286) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:27:30,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 427.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 427 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:27:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:27:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:27:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 429)
   [druid] 2019-06-11 09:27:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:27:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 429 (MapPartitionsRDD[505] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:30,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_215 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:30,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_215_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:30,032 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_215_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:30,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 215 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:30,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 429 (MapPartitionsRDD[505] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:27:30,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 429.0 with 1 tasks
   [druid] 2019-06-11 09:27:30,032 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 429.0 (TID 287, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:30,032 [er for task 287] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 429.0 (TID 287)
   [druid] 2019-06-11 09:27:30,033 [er for task 287] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:30,033 [er for task 287] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:30,033 [er for task 287] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:30,033 [er for task 287] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:30,034 [er for task 287] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 429.0 (TID 287). 966 bytes result sent to driver
   [druid] 2019-06-11 09:27:30,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 429.0 (TID 287) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:30,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 429.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:30,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 429 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:27:30,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 143 finished: print at Transform.scala:36, took 0.025423 s
   [druid] 2019-06-11 09:27:30,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:30,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 143 is 160 bytes
   [druid] 2019-06-11 09:27:30,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 142 is 83 bytes
   [druid] 2019-06-11 09:27:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 144 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 432 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 430, ShuffleMapStage 431)
   [druid] 2019-06-11 09:27:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:27:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 432 (MapPartitionsRDD[505] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:30,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_216 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:30,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_216_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:30,040 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_216_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:30,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 216 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:30,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 432 (MapPartitionsRDD[505] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:27:30,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 432.0 with 1 tasks
   [druid] 2019-06-11 09:27:30,040 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 432.0 (TID 288, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:30,040 [er for task 288] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 432.0 (TID 288)
   [druid] 2019-06-11 09:27:30,041 [er for task 288] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:30,041 [er for task 288] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:30,041 [er for task 288] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:30,041 [er for task 288] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:30,042 [er for task 288] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 432.0 (TID 288). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:27:30,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 432.0 (TID 288) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:30,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 432.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:30,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 432 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:27:30,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 144 finished: print at Transform.scala:36, took 0.006141 s
   [druid] 2019-06-11 09:27:30,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216450000 ms.0 from job set of time 1560216450000 ms
   [druid] 2019-06-11 09:27:30,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216450000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:27:30,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 498 from persistence list
   [druid] 2019-06-11 09:27:30,043 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 498
   [druid] 2019-06-11 09:27:30,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 493 from persistence list
   [druid] 2019-06-11 09:27:30,043 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 493
   [druid] 2019-06-11 09:27:30,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 492 from persistence list
   [druid] 2019-06-11 09:27:30,044 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 492
   [druid] 2019-06-11 09:27:30,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[492] at socketTextStream at Transform.scala:18 of time 1560216450000 ms
   [druid] 2019-06-11 09:27:30,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216440000 ms
   [druid] 2019-06-11 09:27:30,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216440000 ms
   [druid] 2019-06-11 09:27:35,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216455000 ms
   [druid] 2019-06-11 09:27:35,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216455000 ms.0 from job set of time 1560216455000 ms
   [druid] 2019-06-11 09:27:35,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 507 (map at Transform.scala:20)
   [druid] 2019-06-11 09:27:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:27:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 145 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 435 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 434, ShuffleMapStage 433)
   [druid] 2019-06-11 09:27:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 434)
   [druid] 2019-06-11 09:27:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 434 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:27:35,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_217 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:35,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_217_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:27:35,011 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_217_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:35,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 217 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 434 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:27:35,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 434.0 with 2 tasks
   [druid] 2019-06-11 09:27:35,012 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 434.0 (TID 289, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:35,012 [er for task 289] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 434.0 (TID 289)
   [druid] 2019-06-11 09:27:35,021 [er for task 289] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 434.0 (TID 289). 810 bytes result sent to driver
   [druid] 2019-06-11 09:27:35,021 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 434.0 (TID 290, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:35,021 [er for task 290] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 434.0 (TID 290)
   [druid] 2019-06-11 09:27:35,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 434.0 (TID 289) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:27:35,029 [er for task 290] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 434.0 (TID 290). 810 bytes result sent to driver
   [druid] 2019-06-11 09:27:35,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 434.0 (TID 290) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:27:35,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 434.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:35,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 434 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:27:35,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:27:35,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:27:35,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 435)
   [druid] 2019-06-11 09:27:35,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:27:35,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 435 (MapPartitionsRDD[512] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:35,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_218 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:35,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_218_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:35,031 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_218_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:35,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 218 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:35,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 435 (MapPartitionsRDD[512] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:27:35,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 435.0 with 1 tasks
   [druid] 2019-06-11 09:27:35,032 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 435.0 (TID 291, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:35,032 [er for task 291] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 435.0 (TID 291)
   [druid] 2019-06-11 09:27:35,033 [er for task 291] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:35,033 [er for task 291] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:35,033 [er for task 291] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:35,033 [er for task 291] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:35,034 [er for task 291] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 435.0 (TID 291). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:27:35,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 435.0 (TID 291) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:35,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 435.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 435 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:27:35,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 145 finished: print at Transform.scala:36, took 0.025844 s
   [druid] 2019-06-11 09:27:35,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:35,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 144 is 83 bytes
   [druid] 2019-06-11 09:27:35,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 145 is 160 bytes
   [druid] 2019-06-11 09:27:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 146 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 438 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 437, ShuffleMapStage 436)
   [druid] 2019-06-11 09:27:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:27:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 438 (MapPartitionsRDD[512] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:35,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_219 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:35,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_219_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:35,039 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_219_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:35,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 219 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 438 (MapPartitionsRDD[512] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:27:35,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 438.0 with 1 tasks
   [druid] 2019-06-11 09:27:35,040 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 438.0 (TID 292, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:35,040 [er for task 292] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 438.0 (TID 292)
   [druid] 2019-06-11 09:27:35,040 [er for task 292] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:35,040 [er for task 292] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:35,040 [er for task 292] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:35,040 [er for task 292] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:35,041 [er for task 292] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 438.0 (TID 292). 966 bytes result sent to driver
   [druid] 2019-06-11 09:27:35,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 438.0 (TID 292) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:35,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 438.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:35,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 438 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:27:35,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 146 finished: print at Transform.scala:36, took 0.004975 s
   [druid] 2019-06-11 09:27:35,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216455000 ms.0 from job set of time 1560216455000 ms
   [druid] 2019-06-11 09:27:35,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560216455000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:27:35,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 505 from persistence list
   [druid] 2019-06-11 09:27:35,042 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 505
   [druid] 2019-06-11 09:27:35,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 500 from persistence list
   [druid] 2019-06-11 09:27:35,043 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 500
   [druid] 2019-06-11 09:27:35,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 499 from persistence list
   [druid] 2019-06-11 09:27:35,043 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 499
   [druid] 2019-06-11 09:27:35,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[499] at socketTextStream at Transform.scala:18 of time 1560216455000 ms
   [druid] 2019-06-11 09:27:35,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216445000 ms
   [druid] 2019-06-11 09:27:35,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216445000 ms
   [druid] 2019-06-11 09:27:40,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216460000 ms
   [druid] 2019-06-11 09:27:40,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216460000 ms.0 from job set of time 1560216460000 ms
   [druid] 2019-06-11 09:27:40,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 514 (map at Transform.scala:20)
   [druid] 2019-06-11 09:27:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:27:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 147 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 441 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 440, ShuffleMapStage 439)
   [druid] 2019-06-11 09:27:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 440)
   [druid] 2019-06-11 09:27:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 440 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:27:40,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_220 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:40,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_220_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:27:40,010 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_220_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:40,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 220 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 440 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:27:40,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 440.0 with 2 tasks
   [druid] 2019-06-11 09:27:40,010 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 440.0 (TID 293, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:40,011 [er for task 293] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 440.0 (TID 293)
   [druid] 2019-06-11 09:27:40,024 [er for task 293] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 440.0 (TID 293). 810 bytes result sent to driver
   [druid] 2019-06-11 09:27:40,024 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 440.0 (TID 294, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:40,024 [er for task 294] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 440.0 (TID 294)
   [druid] 2019-06-11 09:27:40,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 440.0 (TID 293) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:27:40,032 [er for task 294] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 440.0 (TID 294). 810 bytes result sent to driver
   [druid] 2019-06-11 09:27:40,032 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 440.0 (TID 294) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:27:40,032 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 440.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 440 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:27:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:27:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:27:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 441)
   [druid] 2019-06-11 09:27:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:27:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 441 (MapPartitionsRDD[519] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:40,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_221 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:40,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_221_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:40,034 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_221_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:40,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 221 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 441 (MapPartitionsRDD[519] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:27:40,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 441.0 with 1 tasks
   [druid] 2019-06-11 09:27:40,035 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 441.0 (TID 295, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:40,035 [er for task 295] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 441.0 (TID 295)
   [druid] 2019-06-11 09:27:40,036 [er for task 295] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:40,036 [er for task 295] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:40,036 [er for task 295] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:40,036 [er for task 295] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:40,037 [er for task 295] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 441.0 (TID 295). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:27:40,037 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 441.0 (TID 295) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:40,037 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 441.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:40,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 441 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:27:40,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 147 finished: print at Transform.scala:36, took 0.029320 s
   [druid] 2019-06-11 09:27:40,040 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:40,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 146 is 83 bytes
   [druid] 2019-06-11 09:27:40,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 147 is 160 bytes
   [druid] 2019-06-11 09:27:40,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 148 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:40,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 444 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:40,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 442, ShuffleMapStage 443)
   [druid] 2019-06-11 09:27:40,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:27:40,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 444 (MapPartitionsRDD[519] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:40,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_222 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:40,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_222_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:40,042 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_222_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:40,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 222 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:40,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 444 (MapPartitionsRDD[519] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:27:40,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 444.0 with 1 tasks
   [druid] 2019-06-11 09:27:40,043 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 444.0 (TID 296, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:40,043 [er for task 296] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 444.0 (TID 296)
   [druid] 2019-06-11 09:27:40,044 [er for task 296] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:40,044 [er for task 296] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:40,044 [er for task 296] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:40,044 [er for task 296] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:40,045 [er for task 296] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 444.0 (TID 296). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:27:40,045 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 444.0 (TID 296) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:40,045 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 444.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:40,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 444 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:27:40,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 148 finished: print at Transform.scala:36, took 0.005088 s
   [druid] 2019-06-11 09:27:40,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216460000 ms.0 from job set of time 1560216460000 ms
   [druid] 2019-06-11 09:27:40,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560216460000 ms (execution: 0.039 s)
   [druid] 2019-06-11 09:27:40,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 512 from persistence list
   [druid] 2019-06-11 09:27:40,046 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 512
   [druid] 2019-06-11 09:27:40,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 507 from persistence list
   [druid] 2019-06-11 09:27:40,046 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 507
   [druid] 2019-06-11 09:27:40,046 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 506 from persistence list
   [druid] 2019-06-11 09:27:40,046 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 506
   [druid] 2019-06-11 09:27:40,047 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[506] at socketTextStream at Transform.scala:18 of time 1560216460000 ms
   [druid] 2019-06-11 09:27:40,047 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216450000 ms
   [druid] 2019-06-11 09:27:40,047 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216450000 ms
   [druid] 2019-06-11 09:27:45,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216465000 ms
   [druid] 2019-06-11 09:27:45,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216465000 ms.0 from job set of time 1560216465000 ms
   [druid] 2019-06-11 09:27:45,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 521 (map at Transform.scala:20)
   [druid] 2019-06-11 09:27:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:27:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 149 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 447 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 445, ShuffleMapStage 446)
   [druid] 2019-06-11 09:27:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 446)
   [druid] 2019-06-11 09:27:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 446 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:27:45,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_223 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:45,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_223_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:27:45,010 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_223_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:45,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 223 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:45,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 446 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:27:45,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 446.0 with 2 tasks
   [druid] 2019-06-11 09:27:45,010 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 446.0 (TID 297, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:45,011 [er for task 297] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 446.0 (TID 297)
   [druid] 2019-06-11 09:27:45,020 [er for task 297] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 446.0 (TID 297). 810 bytes result sent to driver
   [druid] 2019-06-11 09:27:45,020 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 446.0 (TID 298, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:45,020 [er for task 298] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 446.0 (TID 298)
   [druid] 2019-06-11 09:27:45,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 446.0 (TID 297) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:27:45,028 [er for task 298] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 446.0 (TID 298). 767 bytes result sent to driver
   [druid] 2019-06-11 09:27:45,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 446.0 (TID 298) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:27:45,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 446.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 446 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:27:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:27:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:27:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 447)
   [druid] 2019-06-11 09:27:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:27:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 447 (MapPartitionsRDD[526] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:45,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_224 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:45,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_224_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:45,030 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_224_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:45,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 224 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 447 (MapPartitionsRDD[526] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:27:45,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 447.0 with 1 tasks
   [druid] 2019-06-11 09:27:45,030 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 447.0 (TID 299, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:45,031 [er for task 299] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 447.0 (TID 299)
   [druid] 2019-06-11 09:27:45,031 [er for task 299] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:45,031 [er for task 299] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:45,032 [er for task 299] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:45,032 [er for task 299] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:27:45,033 [er for task 299] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 447.0 (TID 299). 966 bytes result sent to driver
   [druid] 2019-06-11 09:27:45,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 447.0 (TID 299) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:45,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 447.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 447 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:27:45,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 149 finished: print at Transform.scala:36, took 0.025378 s
   [druid] 2019-06-11 09:27:45,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:45,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 148 is 83 bytes
   [druid] 2019-06-11 09:27:45,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 149 is 160 bytes
   [druid] 2019-06-11 09:27:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 150 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 450 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 448, ShuffleMapStage 449)
   [druid] 2019-06-11 09:27:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:27:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 450 (MapPartitionsRDD[526] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:45,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_225 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:45,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_225_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:45,038 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_225_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:45,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 225 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 450 (MapPartitionsRDD[526] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:27:45,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 450.0 with 1 tasks
   [druid] 2019-06-11 09:27:45,039 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 450.0 (TID 300, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:45,039 [er for task 300] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 450.0 (TID 300)
   [druid] 2019-06-11 09:27:45,040 [er for task 300] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:45,040 [er for task 300] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:45,040 [er for task 300] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:45,040 [er for task 300] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:45,041 [er for task 300] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 450.0 (TID 300). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:27:45,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 450.0 (TID 300) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:45,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 450.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:45,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 450 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:27:45,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 150 finished: print at Transform.scala:36, took 0.005605 s
   [druid] 2019-06-11 09:27:45,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216465000 ms.0 from job set of time 1560216465000 ms
   [druid] 2019-06-11 09:27:45,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560216465000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:27:45,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 519 from persistence list
   [druid] 2019-06-11 09:27:45,042 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 519
   [druid] 2019-06-11 09:27:45,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 514 from persistence list
   [druid] 2019-06-11 09:27:45,042 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 514
   [druid] 2019-06-11 09:27:45,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 513 from persistence list
   [druid] 2019-06-11 09:27:45,043 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 513
   [druid] 2019-06-11 09:27:45,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[513] at socketTextStream at Transform.scala:18 of time 1560216465000 ms
   [druid] 2019-06-11 09:27:45,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216455000 ms
   [druid] 2019-06-11 09:27:45,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216455000 ms
   [druid] 2019-06-11 09:27:50,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216470000 ms
   [druid] 2019-06-11 09:27:50,010 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216470000 ms.0 from job set of time 1560216470000 ms
   [druid] 2019-06-11 09:27:50,012 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 140
   [druid] 2019-06-11 09:27:50,015 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_205_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:50,017 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_222_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:50,018 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:50,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:27:50,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 528 (map at Transform.scala:20)
   [druid] 2019-06-11 09:27:50,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 151 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:50,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 453 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:50,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 452, ShuffleMapStage 451)
   [druid] 2019-06-11 09:27:50,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 451)
   [druid] 2019-06-11 09:27:50,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 451 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:27:50,019 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_226 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:50,020 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_226_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:27:50,020 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_226_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:50,020 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 226 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:50,020 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 451 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:27:50,021 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 451.0 with 2 tasks
   [druid] 2019-06-11 09:27:50,021 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 451.0 (TID 301, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:50,021 [er for task 301] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 451.0 (TID 301)
   [druid] 2019-06-11 09:27:50,034 [er for task 301] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 451.0 (TID 301). 853 bytes result sent to driver
   [druid] 2019-06-11 09:27:50,035 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 451.0 (TID 302, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:50,035 [er for task 302] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 451.0 (TID 302)
   [druid] 2019-06-11 09:27:50,035 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 451.0 (TID 301) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:27:50,044 [er for task 302] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 451.0 (TID 302). 810 bytes result sent to driver
   [druid] 2019-06-11 09:27:50,044 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 451.0 (TID 302) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:27:50,044 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 451.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 451 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:27:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:27:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:27:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 453)
   [druid] 2019-06-11 09:27:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:27:50,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 453 (MapPartitionsRDD[533] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:50,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_227 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:50,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_227_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:50,046 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_227_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:50,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 227 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:50,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 453 (MapPartitionsRDD[533] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:27:50,047 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 453.0 with 1 tasks
   [druid] 2019-06-11 09:27:50,047 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 453.0 (TID 303, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:50,047 [er for task 303] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 453.0 (TID 303)
   [druid] 2019-06-11 09:27:50,048 [er for task 303] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:50,048 [er for task 303] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:50,048 [er for task 303] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:50,048 [er for task 303] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:50,049 [er for task 303] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 453.0 (TID 303). 966 bytes result sent to driver
   [druid] 2019-06-11 09:27:50,050 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 453.0 (TID 303) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:50,050 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 453.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:50,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 453 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:27:50,050 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 151 finished: print at Transform.scala:36, took 0.031494 s
   [druid] 2019-06-11 09:27:50,052 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:50,053 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 151 is 160 bytes
   [druid] 2019-06-11 09:27:50,053 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 150 is 83 bytes
   [druid] 2019-06-11 09:27:50,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 152 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:50,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 456 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:50,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 455, ShuffleMapStage 454)
   [druid] 2019-06-11 09:27:50,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:27:50,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 456 (MapPartitionsRDD[533] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:50,053 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_228 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:50,054 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_228_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:50,055 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_228_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:50,055 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 228 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:50,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 456 (MapPartitionsRDD[533] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:27:50,055 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 456.0 with 1 tasks
   [druid] 2019-06-11 09:27:50,055 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 456.0 (TID 304, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:50,055 [er for task 304] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 456.0 (TID 304)
   [druid] 2019-06-11 09:27:50,056 [er for task 304] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:50,056 [er for task 304] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:50,056 [er for task 304] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:50,056 [er for task 304] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:50,057 [er for task 304] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 456.0 (TID 304). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:27:50,057 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 456.0 (TID 304) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:50,057 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 456.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:50,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 456 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:27:50,057 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 152 finished: print at Transform.scala:36, took 0.005611 s
   [druid] 2019-06-11 09:27:50,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216470000 ms.0 from job set of time 1560216470000 ms
   [druid] 2019-06-11 09:27:50,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.058 s for time 1560216470000 ms (execution: 0.048 s)
   [druid] 2019-06-11 09:27:50,058 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 526 from persistence list
   [druid] 2019-06-11 09:27:50,058 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 526
   [druid] 2019-06-11 09:27:50,058 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 521 from persistence list
   [druid] 2019-06-11 09:27:50,058 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 521
   [druid] 2019-06-11 09:27:50,058 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 520 from persistence list
   [druid] 2019-06-11 09:27:50,059 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 520
   [druid] 2019-06-11 09:27:50,059 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[520] at socketTextStream at Transform.scala:18 of time 1560216470000 ms
   [druid] 2019-06-11 09:27:50,059 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216460000 ms
   [druid] 2019-06-11 09:27:50,059 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216460000 ms
   [druid] 2019-06-11 09:27:55,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216475000 ms
   [druid] 2019-06-11 09:27:55,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216475000 ms.0 from job set of time 1560216475000 ms
   [druid] 2019-06-11 09:27:55,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 535 (map at Transform.scala:20)
   [druid] 2019-06-11 09:27:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:27:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 153 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 459 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 458, ShuffleMapStage 457)
   [druid] 2019-06-11 09:27:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 458)
   [druid] 2019-06-11 09:27:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 458 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:27:55,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_229 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:55,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_229_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:27:55,011 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_229_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:55,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 229 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:55,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 458 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:27:55,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 458.0 with 2 tasks
   [druid] 2019-06-11 09:27:55,011 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 458.0 (TID 305, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:55,012 [er for task 305] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 458.0 (TID 305)
   [druid] 2019-06-11 09:27:55,023 [er for task 305] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 458.0 (TID 305). 853 bytes result sent to driver
   [druid] 2019-06-11 09:27:55,023 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 458.0 (TID 306, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:27:55,023 [er for task 306] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 458.0 (TID 306)
   [druid] 2019-06-11 09:27:55,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 458.0 (TID 305) in 12 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:27:55,039 [er for task 306] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 458.0 (TID 306). 853 bytes result sent to driver
   [druid] 2019-06-11 09:27:55,039 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 458.0 (TID 306) in 16 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:27:55,039 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 458.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 458 (parallelize at Transform.scala:16) finished in 0.028 s
   [druid] 2019-06-11 09:27:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:27:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:27:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 459)
   [druid] 2019-06-11 09:27:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:27:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 459 (MapPartitionsRDD[540] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:55,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_230 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:55,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_230_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:55,041 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_230_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:55,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 230 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:55,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 459 (MapPartitionsRDD[540] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:27:55,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 459.0 with 1 tasks
   [druid] 2019-06-11 09:27:55,042 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 459.0 (TID 307, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:55,042 [er for task 307] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 459.0 (TID 307)
   [druid] 2019-06-11 09:27:55,042 [er for task 307] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:55,042 [er for task 307] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:55,043 [er for task 307] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:55,043 [er for task 307] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:55,043 [er for task 307] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 459.0 (TID 307). 966 bytes result sent to driver
   [druid] 2019-06-11 09:27:55,044 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 459.0 (TID 307) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:55,044 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 459.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 459 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:27:55,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 153 finished: print at Transform.scala:36, took 0.035697 s
   [druid] 2019-06-11 09:27:55,047 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:27:55,047 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 152 is 83 bytes
   [druid] 2019-06-11 09:27:55,047 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 153 is 160 bytes
   [druid] 2019-06-11 09:27:55,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 154 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:27:55,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 462 (print at Transform.scala:36)
   [druid] 2019-06-11 09:27:55,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 460, ShuffleMapStage 461)
   [druid] 2019-06-11 09:27:55,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:27:55,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 462 (MapPartitionsRDD[540] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:27:55,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_231 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:55,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_231_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:27:55,049 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_231_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:27:55,050 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 231 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:27:55,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 462 (MapPartitionsRDD[540] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:27:55,051 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 462.0 with 1 tasks
   [druid] 2019-06-11 09:27:55,051 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 462.0 (TID 308, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:27:55,051 [er for task 308] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 462.0 (TID 308)
   [druid] 2019-06-11 09:27:55,052 [er for task 308] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:27:55,052 [er for task 308] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:55,052 [er for task 308] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:27:55,052 [er for task 308] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:27:55,053 [er for task 308] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 462.0 (TID 308). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:27:55,053 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 462.0 (TID 308) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:27:55,054 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 462.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:27:55,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 462 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:27:55,054 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 154 finished: print at Transform.scala:36, took 0.006920 s
   [druid] 2019-06-11 09:27:55,054 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216475000 ms.0 from job set of time 1560216475000 ms
   [druid] 2019-06-11 09:27:55,054 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.054 s for time 1560216475000 ms (execution: 0.048 s)
   [druid] 2019-06-11 09:27:55,054 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 533 from persistence list
   [druid] 2019-06-11 09:27:55,055 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 533
   [druid] 2019-06-11 09:27:55,055 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 528 from persistence list
   [druid] 2019-06-11 09:27:55,055 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 528
   [druid] 2019-06-11 09:27:55,055 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 527 from persistence list
   [druid] 2019-06-11 09:27:55,055 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 527
   [druid] 2019-06-11 09:27:55,055 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[527] at socketTextStream at Transform.scala:18 of time 1560216475000 ms
   [druid] 2019-06-11 09:27:55,055 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216465000 ms
   [druid] 2019-06-11 09:27:55,055 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216465000 ms
   [druid] 2019-06-11 09:28:00,009 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216480000 ms
   [druid] 2019-06-11 09:28:00,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216480000 ms.0 from job set of time 1560216480000 ms
   [druid] 2019-06-11 09:28:00,012 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:28:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 542 (map at Transform.scala:20)
   [druid] 2019-06-11 09:28:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 155 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 465 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 463, ShuffleMapStage 464)
   [druid] 2019-06-11 09:28:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 463)
   [druid] 2019-06-11 09:28:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 463 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:28:00,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_232 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:00,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_232_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:28:00,014 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_232_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:00,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 232 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:00,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 463 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:28:00,015 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 463.0 with 2 tasks
   [druid] 2019-06-11 09:28:00,015 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 463.0 (TID 309, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:00,015 [er for task 309] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 463.0 (TID 309)
   [druid] 2019-06-11 09:28:00,030 [er for task 309] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 463.0 (TID 309). 810 bytes result sent to driver
   [druid] 2019-06-11 09:28:00,030 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 463.0 (TID 310, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:00,030 [er for task 310] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 463.0 (TID 310)
   [druid] 2019-06-11 09:28:00,030 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 463.0 (TID 309) in 15 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:28:00,040 [er for task 310] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 463.0 (TID 310). 810 bytes result sent to driver
   [druid] 2019-06-11 09:28:00,040 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 463.0 (TID 310) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:28:00,040 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 463.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 463 (parallelize at Transform.scala:16) finished in 0.025 s
   [druid] 2019-06-11 09:28:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:28:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:28:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 465)
   [druid] 2019-06-11 09:28:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:28:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 465 (MapPartitionsRDD[547] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:00,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_233 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:00,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_233_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:00,042 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_233_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:00,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 233 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 465 (MapPartitionsRDD[547] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:28:00,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 465.0 with 1 tasks
   [druid] 2019-06-11 09:28:00,043 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 465.0 (TID 311, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:00,043 [er for task 311] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 465.0 (TID 311)
   [druid] 2019-06-11 09:28:00,044 [er for task 311] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:00,044 [er for task 311] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:00,044 [er for task 311] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:00,044 [er for task 311] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:00,045 [er for task 311] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 465.0 (TID 311). 966 bytes result sent to driver
   [druid] 2019-06-11 09:28:00,045 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 465.0 (TID 311) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:00,045 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 465.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:00,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 465 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:00,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 155 finished: print at Transform.scala:36, took 0.032873 s
   [druid] 2019-06-11 09:28:00,047 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:00,048 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 155 is 160 bytes
   [druid] 2019-06-11 09:28:00,048 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 154 is 83 bytes
   [druid] 2019-06-11 09:28:00,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 156 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:00,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 468 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:00,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 466, ShuffleMapStage 467)
   [druid] 2019-06-11 09:28:00,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:28:00,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 468 (MapPartitionsRDD[547] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:00,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_234 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:00,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_234_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:00,050 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_234_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:00,050 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 234 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:00,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 468 (MapPartitionsRDD[547] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:28:00,051 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 468.0 with 1 tasks
   [druid] 2019-06-11 09:28:00,051 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 468.0 (TID 312, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:00,051 [er for task 312] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 468.0 (TID 312)
   [druid] 2019-06-11 09:28:00,052 [er for task 312] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:00,052 [er for task 312] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:00,052 [er for task 312] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:00,052 [er for task 312] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:00,052 [er for task 312] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 468.0 (TID 312). 966 bytes result sent to driver
   [druid] 2019-06-11 09:28:00,053 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 468.0 (TID 312) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:00,053 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 468.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:00,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 468 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:00,053 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 156 finished: print at Transform.scala:36, took 0.005542 s
   [druid] 2019-06-11 09:28:00,053 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216480000 ms.0 from job set of time 1560216480000 ms
   [druid] 2019-06-11 09:28:00,053 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.053 s for time 1560216480000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:28:00,053 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 540 from persistence list
   [druid] 2019-06-11 09:28:00,053 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 540
   [druid] 2019-06-11 09:28:00,053 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 535 from persistence list
   [druid] 2019-06-11 09:28:00,054 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 535
   [druid] 2019-06-11 09:28:00,054 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 534 from persistence list
   [druid] 2019-06-11 09:28:00,054 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 534
   [druid] 2019-06-11 09:28:00,054 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[534] at socketTextStream at Transform.scala:18 of time 1560216480000 ms
   [druid] 2019-06-11 09:28:00,054 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216470000 ms
   [druid] 2019-06-11 09:28:00,055 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216470000 ms
   [druid] 2019-06-11 09:28:05,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216485000 ms
   [druid] 2019-06-11 09:28:05,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216485000 ms.0 from job set of time 1560216485000 ms
   [druid] 2019-06-11 09:28:05,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 549 (map at Transform.scala:20)
   [druid] 2019-06-11 09:28:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:28:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 157 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 471 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 470, ShuffleMapStage 469)
   [druid] 2019-06-11 09:28:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 470)
   [druid] 2019-06-11 09:28:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 470 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:28:05,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_235 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:05,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_235_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:28:05,011 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_235_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:05,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 235 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 470 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:28:05,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 470.0 with 2 tasks
   [druid] 2019-06-11 09:28:05,011 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 470.0 (TID 313, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:05,012 [er for task 313] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 470.0 (TID 313)
   [druid] 2019-06-11 09:28:05,025 [er for task 313] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 470.0 (TID 313). 767 bytes result sent to driver
   [druid] 2019-06-11 09:28:05,025 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 470.0 (TID 314, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:05,025 [er for task 314] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 470.0 (TID 314)
   [druid] 2019-06-11 09:28:05,025 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 470.0 (TID 313) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:28:05,033 [er for task 314] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 470.0 (TID 314). 810 bytes result sent to driver
   [druid] 2019-06-11 09:28:05,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 470.0 (TID 314) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:28:05,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 470.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:05,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 470 (parallelize at Transform.scala:16) finished in 0.022 s
   [druid] 2019-06-11 09:28:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:28:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:28:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 471)
   [druid] 2019-06-11 09:28:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:28:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 471 (MapPartitionsRDD[554] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:05,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_236 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:05,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_236_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:05,036 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_236_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:05,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 236 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:05,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 471 (MapPartitionsRDD[554] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:28:05,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 471.0 with 1 tasks
   [druid] 2019-06-11 09:28:05,037 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 471.0 (TID 315, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:05,037 [er for task 315] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 471.0 (TID 315)
   [druid] 2019-06-11 09:28:05,037 [er for task 315] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:05,038 [er for task 315] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:28:05,038 [er for task 315] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:05,038 [er for task 315] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:05,038 [er for task 315] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 471.0 (TID 315). 966 bytes result sent to driver
   [druid] 2019-06-11 09:28:05,039 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 471.0 (TID 315) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:05,039 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 471.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 471 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:05,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 157 finished: print at Transform.scala:36, took 0.029791 s
   [druid] 2019-06-11 09:28:05,041 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:05,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 156 is 83 bytes
   [druid] 2019-06-11 09:28:05,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 157 is 160 bytes
   [druid] 2019-06-11 09:28:05,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 158 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:05,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 474 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:05,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 473, ShuffleMapStage 472)
   [druid] 2019-06-11 09:28:05,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:28:05,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 474 (MapPartitionsRDD[554] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:05,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_237 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:05,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_237_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:05,043 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_237_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:05,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 237 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:05,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 474 (MapPartitionsRDD[554] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:28:05,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 474.0 with 1 tasks
   [druid] 2019-06-11 09:28:05,044 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 474.0 (TID 316, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:05,044 [er for task 316] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 474.0 (TID 316)
   [druid] 2019-06-11 09:28:05,044 [er for task 316] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:05,044 [er for task 316] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:05,044 [er for task 316] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:05,044 [er for task 316] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:05,045 [er for task 316] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 474.0 (TID 316). 966 bytes result sent to driver
   [druid] 2019-06-11 09:28:05,045 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 474.0 (TID 316) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:05,045 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 474.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:05,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 474 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:28:05,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 158 finished: print at Transform.scala:36, took 0.004752 s
   [druid] 2019-06-11 09:28:05,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216485000 ms.0 from job set of time 1560216485000 ms
   [druid] 2019-06-11 09:28:05,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.046 s for time 1560216485000 ms (execution: 0.039 s)
   [druid] 2019-06-11 09:28:05,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 547 from persistence list
   [druid] 2019-06-11 09:28:05,046 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 547
   [druid] 2019-06-11 09:28:05,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 542 from persistence list
   [druid] 2019-06-11 09:28:05,047 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 542
   [druid] 2019-06-11 09:28:05,047 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 541 from persistence list
   [druid] 2019-06-11 09:28:05,047 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 541
   [druid] 2019-06-11 09:28:05,047 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[541] at socketTextStream at Transform.scala:18 of time 1560216485000 ms
   [druid] 2019-06-11 09:28:05,047 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216475000 ms
   [druid] 2019-06-11 09:28:05,047 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216475000 ms
   [druid] 2019-06-11 09:28:10,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216490000 ms
   [druid] 2019-06-11 09:28:10,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216490000 ms.0 from job set of time 1560216490000 ms
   [druid] 2019-06-11 09:28:10,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 556 (map at Transform.scala:20)
   [druid] 2019-06-11 09:28:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:28:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 159 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 477 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 476, ShuffleMapStage 475)
   [druid] 2019-06-11 09:28:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 476)
   [druid] 2019-06-11 09:28:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 476 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:28:10,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_238 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:10,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_238_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:28:10,010 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_238_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:10,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 238 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:10,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 476 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:28:10,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 476.0 with 2 tasks
   [druid] 2019-06-11 09:28:10,011 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 476.0 (TID 317, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:10,011 [er for task 317] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 476.0 (TID 317)
   [druid] 2019-06-11 09:28:10,024 [er for task 317] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 476.0 (TID 317). 810 bytes result sent to driver
   [druid] 2019-06-11 09:28:10,024 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 476.0 (TID 318, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:10,024 [er for task 318] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 476.0 (TID 318)
   [druid] 2019-06-11 09:28:10,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 476.0 (TID 317) in 13 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:28:10,032 [er for task 318] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 476.0 (TID 318). 853 bytes result sent to driver
   [druid] 2019-06-11 09:28:10,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 476.0 (TID 318) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:28:10,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 476.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 476 (parallelize at Transform.scala:16) finished in 0.022 s
   [druid] 2019-06-11 09:28:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:28:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:28:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 477)
   [druid] 2019-06-11 09:28:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:28:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 477 (MapPartitionsRDD[561] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:10,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_239 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:10,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_239_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:10,035 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_239_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:10,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 239 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:10,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 477 (MapPartitionsRDD[561] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:28:10,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 477.0 with 1 tasks
   [druid] 2019-06-11 09:28:10,036 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 477.0 (TID 319, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:10,036 [er for task 319] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 477.0 (TID 319)
   [druid] 2019-06-11 09:28:10,037 [er for task 319] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:10,037 [er for task 319] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:10,037 [er for task 319] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:10,037 [er for task 319] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:10,038 [er for task 319] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 477.0 (TID 319). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:28:10,038 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 477.0 (TID 319) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:10,038 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 477.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 477 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:10,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 159 finished: print at Transform.scala:36, took 0.030009 s
   [druid] 2019-06-11 09:28:10,041 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:10,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 158 is 83 bytes
   [druid] 2019-06-11 09:28:10,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 159 is 160 bytes
   [druid] 2019-06-11 09:28:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 160 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 480 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 478, ShuffleMapStage 479)
   [druid] 2019-06-11 09:28:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:28:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 480 (MapPartitionsRDD[561] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:10,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_240 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:10,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_240_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:10,044 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_240_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:10,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 240 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:10,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 480 (MapPartitionsRDD[561] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:28:10,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 480.0 with 1 tasks
   [druid] 2019-06-11 09:28:10,044 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 480.0 (TID 320, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:10,044 [er for task 320] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 480.0 (TID 320)
   [druid] 2019-06-11 09:28:10,045 [er for task 320] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:10,045 [er for task 320] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:10,045 [er for task 320] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:10,045 [er for task 320] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:10,046 [er for task 320] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 480.0 (TID 320). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:28:10,046 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 480.0 (TID 320) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:10,046 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 480.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:10,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 480 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:28:10,047 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 160 finished: print at Transform.scala:36, took 0.006468 s
   [druid] 2019-06-11 09:28:10,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216490000 ms.0 from job set of time 1560216490000 ms
   [druid] 2019-06-11 09:28:10,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.047 s for time 1560216490000 ms (execution: 0.042 s)
   [druid] 2019-06-11 09:28:10,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 554 from persistence list
   [druid] 2019-06-11 09:28:10,047 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 554
   [druid] 2019-06-11 09:28:10,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 549 from persistence list
   [druid] 2019-06-11 09:28:10,048 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 549
   [druid] 2019-06-11 09:28:10,048 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 548 from persistence list
   [druid] 2019-06-11 09:28:10,048 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 548
   [druid] 2019-06-11 09:28:10,048 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[548] at socketTextStream at Transform.scala:18 of time 1560216490000 ms
   [druid] 2019-06-11 09:28:10,048 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216480000 ms
   [druid] 2019-06-11 09:28:10,048 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216480000 ms
   [druid] 2019-06-11 09:28:15,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216495000 ms
   [druid] 2019-06-11 09:28:15,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216495000 ms.0 from job set of time 1560216495000 ms
   [druid] 2019-06-11 09:28:15,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 563 (map at Transform.scala:20)
   [druid] 2019-06-11 09:28:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:28:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 161 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 483 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 481, ShuffleMapStage 482)
   [druid] 2019-06-11 09:28:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 482)
   [druid] 2019-06-11 09:28:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 482 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:28:15,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_241 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:15,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_241_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:28:15,009 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_241_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:15,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 241 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 482 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:28:15,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 482.0 with 2 tasks
   [druid] 2019-06-11 09:28:15,009 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 482.0 (TID 321, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:15,010 [er for task 321] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 482.0 (TID 321)
   [druid] 2019-06-11 09:28:15,019 [er for task 321] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 482.0 (TID 321). 810 bytes result sent to driver
   [druid] 2019-06-11 09:28:15,019 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 482.0 (TID 322, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:15,019 [er for task 322] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 482.0 (TID 322)
   [druid] 2019-06-11 09:28:15,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 482.0 (TID 321) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:28:15,028 [er for task 322] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 482.0 (TID 322). 767 bytes result sent to driver
   [druid] 2019-06-11 09:28:15,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 482.0 (TID 322) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:28:15,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 482.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 482 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:28:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:28:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:28:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 483)
   [druid] 2019-06-11 09:28:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:28:15,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 483 (MapPartitionsRDD[568] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:15,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_242 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:15,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_242_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:15,030 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_242_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:15,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 242 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 483 (MapPartitionsRDD[568] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:28:15,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 483.0 with 1 tasks
   [druid] 2019-06-11 09:28:15,031 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 483.0 (TID 323, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:15,031 [er for task 323] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 483.0 (TID 323)
   [druid] 2019-06-11 09:28:15,032 [er for task 323] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:15,032 [er for task 323] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:15,032 [er for task 323] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:15,032 [er for task 323] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:15,033 [er for task 323] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 483.0 (TID 323). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:28:15,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 483.0 (TID 323) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:15,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 483.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 483 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:28:15,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 161 finished: print at Transform.scala:36, took 0.026728 s
   [druid] 2019-06-11 09:28:15,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:15,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 160 is 83 bytes
   [druid] 2019-06-11 09:28:15,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 161 is 160 bytes
   [druid] 2019-06-11 09:28:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 162 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 486 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 484, ShuffleMapStage 485)
   [druid] 2019-06-11 09:28:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:28:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 486 (MapPartitionsRDD[568] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:15,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_243 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:15,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_243_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:15,038 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_243_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:15,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 243 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:15,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 486 (MapPartitionsRDD[568] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:28:15,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 486.0 with 1 tasks
   [druid] 2019-06-11 09:28:15,039 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 486.0 (TID 324, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:15,039 [er for task 324] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 486.0 (TID 324)
   [druid] 2019-06-11 09:28:15,040 [er for task 324] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:15,040 [er for task 324] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:15,040 [er for task 324] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:15,040 [er for task 324] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:15,040 [er for task 324] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 486.0 (TID 324). 966 bytes result sent to driver
   [druid] 2019-06-11 09:28:15,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 486.0 (TID 324) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:15,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 486.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 486 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:15,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 162 finished: print at Transform.scala:36, took 0.004677 s
   [druid] 2019-06-11 09:28:15,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216495000 ms.0 from job set of time 1560216495000 ms
   [druid] 2019-06-11 09:28:15,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560216495000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:28:15,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 561 from persistence list
   [druid] 2019-06-11 09:28:15,041 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 561
   [druid] 2019-06-11 09:28:15,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 556 from persistence list
   [druid] 2019-06-11 09:28:15,041 [-thread-pool-11] INFO  che.spark.storage.BlockManager {1} - Removing RDD 556
   [druid] 2019-06-11 09:28:15,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 555 from persistence list
   [druid] 2019-06-11 09:28:15,041 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 555
   [druid] 2019-06-11 09:28:15,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[555] at socketTextStream at Transform.scala:18 of time 1560216495000 ms
   [druid] 2019-06-11 09:28:15,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216485000 ms
   [druid] 2019-06-11 09:28:15,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216485000 ms
   [druid] 2019-06-11 09:28:20,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216500000 ms
   [druid] 2019-06-11 09:28:20,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216500000 ms.0 from job set of time 1560216500000 ms
   [druid] 2019-06-11 09:28:20,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:28:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 570 (map at Transform.scala:20)
   [druid] 2019-06-11 09:28:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 163 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 489 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 488, ShuffleMapStage 487)
   [druid] 2019-06-11 09:28:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 487)
   [druid] 2019-06-11 09:28:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 487 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:28:20,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_244 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:20,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_244_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:28:20,011 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_244_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 244 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:20,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 487 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:28:20,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 487.0 with 2 tasks
   [druid] 2019-06-11 09:28:20,011 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 487.0 (TID 325, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:20,012 [er for task 325] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 487.0 (TID 325)
   [druid] 2019-06-11 09:28:20,021 [er for task 325] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 487.0 (TID 325). 853 bytes result sent to driver
   [druid] 2019-06-11 09:28:20,021 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 487.0 (TID 326, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:20,021 [er for task 326] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 487.0 (TID 326)
   [druid] 2019-06-11 09:28:20,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 487.0 (TID 325) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:28:20,029 [er for task 326] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 487.0 (TID 326). 767 bytes result sent to driver
   [druid] 2019-06-11 09:28:20,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 487.0 (TID 326) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:28:20,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 487.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 487 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:28:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:28:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:28:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 489)
   [druid] 2019-06-11 09:28:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:28:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 489 (MapPartitionsRDD[575] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:20,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_245 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:20,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_245_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:20,031 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_245_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 245 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 489 (MapPartitionsRDD[575] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:28:20,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 489.0 with 1 tasks
   [druid] 2019-06-11 09:28:20,032 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 489.0 (TID 327, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:20,032 [er for task 327] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 489.0 (TID 327)
   [druid] 2019-06-11 09:28:20,033 [er for task 327] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:20,033 [er for task 327] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:20,033 [er for task 327] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:20,033 [er for task 327] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:20,034 [er for task 327] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 489.0 (TID 327). 966 bytes result sent to driver
   [druid] 2019-06-11 09:28:20,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 489.0 (TID 327) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:20,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 489.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:20,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 489 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:20,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 163 finished: print at Transform.scala:36, took 0.025058 s
   [druid] 2019-06-11 09:28:20,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:20,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 163 is 160 bytes
   [druid] 2019-06-11 09:28:20,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 162 is 83 bytes
   [druid] 2019-06-11 09:28:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 164 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 492 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 491, ShuffleMapStage 490)
   [druid] 2019-06-11 09:28:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:28:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 492 (MapPartitionsRDD[575] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:20,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_246 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:20,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_246_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:20,039 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_246_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 246 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 492 (MapPartitionsRDD[575] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:28:20,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 492.0 with 1 tasks
   [druid] 2019-06-11 09:28:20,040 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 492.0 (TID 328, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:20,040 [er for task 328] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 492.0 (TID 328)
   [druid] 2019-06-11 09:28:20,040 [er for task 328] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:20,041 [er for task 328] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:28:20,041 [er for task 328] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:20,046 [er for task 328] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 5 ms
   [druid] 2019-06-11 09:28:20,049 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_241_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,052 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_227_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,053 [er for task 328] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 492.0 (TID 328). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:28:20,053 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 492.0 (TID 328) in 14 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:20,053 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 492.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:20,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 157
   [druid] 2019-06-11 09:28:20,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 492 (print at Transform.scala:36) finished in 0.015 s
   [druid] 2019-06-11 09:28:20,054 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 164 finished: print at Transform.scala:36, took 0.017562 s
   [druid] 2019-06-11 09:28:20,054 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_235_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,054 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216500000 ms.0 from job set of time 1560216500000 ms
   [druid] 2019-06-11 09:28:20,054 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.054 s for time 1560216500000 ms (execution: 0.048 s)
   [druid] 2019-06-11 09:28:20,054 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 568 from persistence list
   [druid] 2019-06-11 09:28:20,054 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 568
   [druid] 2019-06-11 09:28:20,054 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 563 from persistence list
   [druid] 2019-06-11 09:28:20,055 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 563
   [druid] 2019-06-11 09:28:20,055 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 562 from persistence list
   [druid] 2019-06-11 09:28:20,055 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 562
   [druid] 2019-06-11 09:28:20,055 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[562] at socketTextStream at Transform.scala:18 of time 1560216500000 ms
   [druid] 2019-06-11 09:28:20,055 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216490000 ms
   [druid] 2019-06-11 09:28:20,055 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216490000 ms
   [druid] 2019-06-11 09:28:20,055 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_243_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,056 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 159
   [druid] 2019-06-11 09:28:20,057 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_231_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,057 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_229_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,058 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_242_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,059 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_237_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,060 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 155
   [druid] 2019-06-11 09:28:20,060 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_239_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,061 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 154
   [druid] 2019-06-11 09:28:20,062 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_238_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,063 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_234_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,063 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 153
   [druid] 2019-06-11 09:28:20,063 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 156
   [druid] 2019-06-11 09:28:20,064 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_230_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,065 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_244_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,066 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_245_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,066 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_240_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,067 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_228_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,067 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 151
   [druid] 2019-06-11 09:28:20,068 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 152
   [druid] 2019-06-11 09:28:20,068 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_236_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,069 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_226_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,070 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 158
   [druid] 2019-06-11 09:28:20,070 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_232_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,071 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_233_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:20,071 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 150
   [druid] 2019-06-11 09:28:25,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216505000 ms
   [druid] 2019-06-11 09:28:25,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216505000 ms.0 from job set of time 1560216505000 ms
   [druid] 2019-06-11 09:28:25,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 577 (map at Transform.scala:20)
   [druid] 2019-06-11 09:28:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:28:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 165 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 495 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 494, ShuffleMapStage 493)
   [druid] 2019-06-11 09:28:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 494)
   [druid] 2019-06-11 09:28:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 494 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:28:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_247 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:25,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_247_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:28:25,010 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_247_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:25,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 247 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 494 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:28:25,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 494.0 with 2 tasks
   [druid] 2019-06-11 09:28:25,011 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 494.0 (TID 329, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:25,011 [er for task 329] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 494.0 (TID 329)
   [druid] 2019-06-11 09:28:25,020 [er for task 329] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 494.0 (TID 329). 810 bytes result sent to driver
   [druid] 2019-06-11 09:28:25,020 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 494.0 (TID 330, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:25,020 [er for task 330] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 494.0 (TID 330)
   [druid] 2019-06-11 09:28:25,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 494.0 (TID 329) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:28:25,028 [er for task 330] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 494.0 (TID 330). 810 bytes result sent to driver
   [druid] 2019-06-11 09:28:25,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 494.0 (TID 330) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:28:25,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 494.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 494 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:28:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:28:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:28:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 495)
   [druid] 2019-06-11 09:28:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:28:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 495 (MapPartitionsRDD[582] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:25,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_248 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:25,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_248_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:25,031 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_248_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:25,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 248 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:25,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 495 (MapPartitionsRDD[582] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:28:25,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 495.0 with 1 tasks
   [druid] 2019-06-11 09:28:25,031 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 495.0 (TID 331, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:25,031 [er for task 331] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 495.0 (TID 331)
   [druid] 2019-06-11 09:28:25,032 [er for task 331] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:25,032 [er for task 331] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:25,032 [er for task 331] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:25,032 [er for task 331] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:25,032 [er for task 331] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 495.0 (TID 331). 966 bytes result sent to driver
   [druid] 2019-06-11 09:28:25,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 495.0 (TID 331) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:25,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 495.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 495 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:28:25,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 165 finished: print at Transform.scala:36, took 0.027847 s
   [druid] 2019-06-11 09:28:25,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:25,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 164 is 83 bytes
   [druid] 2019-06-11 09:28:25,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 165 is 160 bytes
   [druid] 2019-06-11 09:28:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 166 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 498 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 496, ShuffleMapStage 497)
   [druid] 2019-06-11 09:28:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:28:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 498 (MapPartitionsRDD[582] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:25,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_249 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:25,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_249_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:25,041 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_249_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:25,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 249 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:25,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 498 (MapPartitionsRDD[582] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:28:25,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 498.0 with 1 tasks
   [druid] 2019-06-11 09:28:25,042 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 498.0 (TID 332, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:25,042 [er for task 332] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 498.0 (TID 332)
   [druid] 2019-06-11 09:28:25,043 [er for task 332] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:25,043 [er for task 332] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:25,043 [er for task 332] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:25,043 [er for task 332] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:25,044 [er for task 332] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 498.0 (TID 332). 966 bytes result sent to driver
   [druid] 2019-06-11 09:28:25,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 498.0 (TID 332) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:25,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 498.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 498 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:25,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 166 finished: print at Transform.scala:36, took 0.005776 s
   [druid] 2019-06-11 09:28:25,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216505000 ms.0 from job set of time 1560216505000 ms
   [druid] 2019-06-11 09:28:25,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560216505000 ms (execution: 0.039 s)
   [druid] 2019-06-11 09:28:25,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 575 from persistence list
   [druid] 2019-06-11 09:28:25,045 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 575
   [druid] 2019-06-11 09:28:25,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 570 from persistence list
   [druid] 2019-06-11 09:28:25,045 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 570
   [druid] 2019-06-11 09:28:25,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 569 from persistence list
   [druid] 2019-06-11 09:28:25,046 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 569
   [druid] 2019-06-11 09:28:25,046 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[569] at socketTextStream at Transform.scala:18 of time 1560216505000 ms
   [druid] 2019-06-11 09:28:25,046 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216495000 ms
   [druid] 2019-06-11 09:28:25,046 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216495000 ms
   [druid] 2019-06-11 09:28:30,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216510000 ms
   [druid] 2019-06-11 09:28:30,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216510000 ms.0 from job set of time 1560216510000 ms
   [druid] 2019-06-11 09:28:30,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 584 (map at Transform.scala:20)
   [druid] 2019-06-11 09:28:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:28:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 167 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 501 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 499, ShuffleMapStage 500)
   [druid] 2019-06-11 09:28:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 500)
   [druid] 2019-06-11 09:28:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 500 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:28:30,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_250 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:30,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_250_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:28:30,011 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_250_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:30,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 250 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 500 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:28:30,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 500.0 with 2 tasks
   [druid] 2019-06-11 09:28:30,011 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 500.0 (TID 333, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:30,012 [er for task 333] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 500.0 (TID 333)
   [druid] 2019-06-11 09:28:30,020 [er for task 333] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 500.0 (TID 333). 810 bytes result sent to driver
   [druid] 2019-06-11 09:28:30,020 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 500.0 (TID 334, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:30,020 [er for task 334] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 500.0 (TID 334)
   [druid] 2019-06-11 09:28:30,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 500.0 (TID 333) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:28:30,028 [er for task 334] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 500.0 (TID 334). 810 bytes result sent to driver
   [druid] 2019-06-11 09:28:30,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 500.0 (TID 334) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:28:30,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 500.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 500 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:28:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:28:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:28:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 501)
   [druid] 2019-06-11 09:28:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:28:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 501 (MapPartitionsRDD[589] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:30,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_251 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:30,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_251_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:30,031 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_251_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:30,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 251 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 501 (MapPartitionsRDD[589] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:28:30,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 501.0 with 1 tasks
   [druid] 2019-06-11 09:28:30,031 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 501.0 (TID 335, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:30,032 [er for task 335] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 501.0 (TID 335)
   [druid] 2019-06-11 09:28:30,032 [er for task 335] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:30,033 [er for task 335] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:28:30,033 [er for task 335] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:30,033 [er for task 335] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:30,034 [er for task 335] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 501.0 (TID 335). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:28:30,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 501.0 (TID 335) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:30,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 501.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:30,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 501 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:28:30,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 167 finished: print at Transform.scala:36, took 0.026348 s
   [druid] 2019-06-11 09:28:30,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:30,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 166 is 83 bytes
   [druid] 2019-06-11 09:28:30,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 167 is 160 bytes
   [druid] 2019-06-11 09:28:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 168 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 504 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 502, ShuffleMapStage 503)
   [druid] 2019-06-11 09:28:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:28:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 504 (MapPartitionsRDD[589] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:30,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_252 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:30,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_252_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:30,039 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_252_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:30,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 252 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:30,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 504 (MapPartitionsRDD[589] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:28:30,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 504.0 with 1 tasks
   [druid] 2019-06-11 09:28:30,040 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 504.0 (TID 336, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:30,040 [er for task 336] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 504.0 (TID 336)
   [druid] 2019-06-11 09:28:30,041 [er for task 336] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:30,041 [er for task 336] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:30,041 [er for task 336] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:30,041 [er for task 336] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:30,042 [er for task 336] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 504.0 (TID 336). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:28:30,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 504.0 (TID 336) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:30,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 504.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:30,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 504 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:30,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 168 finished: print at Transform.scala:36, took 0.005746 s
   [druid] 2019-06-11 09:28:30,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216510000 ms.0 from job set of time 1560216510000 ms
   [druid] 2019-06-11 09:28:30,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216510000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:28:30,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 582 from persistence list
   [druid] 2019-06-11 09:28:30,043 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 582
   [druid] 2019-06-11 09:28:30,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 577 from persistence list
   [druid] 2019-06-11 09:28:30,043 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 577
   [druid] 2019-06-11 09:28:30,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 576 from persistence list
   [druid] 2019-06-11 09:28:30,044 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 576
   [druid] 2019-06-11 09:28:30,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[576] at socketTextStream at Transform.scala:18 of time 1560216510000 ms
   [druid] 2019-06-11 09:28:30,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216500000 ms
   [druid] 2019-06-11 09:28:30,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216500000 ms
   [druid] 2019-06-11 09:28:35,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216515000 ms
   [druid] 2019-06-11 09:28:35,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216515000 ms.0 from job set of time 1560216515000 ms
   [druid] 2019-06-11 09:28:35,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 591 (map at Transform.scala:20)
   [druid] 2019-06-11 09:28:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:28:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 169 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 507 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 506, ShuffleMapStage 505)
   [druid] 2019-06-11 09:28:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 506)
   [druid] 2019-06-11 09:28:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 506 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:28:35,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_253 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:35,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_253_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:28:35,012 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_253_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:35,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 253 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:35,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 506 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:28:35,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 506.0 with 2 tasks
   [druid] 2019-06-11 09:28:35,012 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 506.0 (TID 337, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:35,013 [er for task 337] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 506.0 (TID 337)
   [druid] 2019-06-11 09:28:35,022 [er for task 337] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 506.0 (TID 337). 853 bytes result sent to driver
   [druid] 2019-06-11 09:28:35,022 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 506.0 (TID 338, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:35,022 [er for task 338] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 506.0 (TID 338)
   [druid] 2019-06-11 09:28:35,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 506.0 (TID 337) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:28:35,031 [er for task 338] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 506.0 (TID 338). 810 bytes result sent to driver
   [druid] 2019-06-11 09:28:35,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 506.0 (TID 338) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:28:35,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 506.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 506 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:28:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:28:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:28:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 507)
   [druid] 2019-06-11 09:28:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:28:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 507 (MapPartitionsRDD[596] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:35,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_254 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:35,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_254_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:28:35,034 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_254_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:35,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 254 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 507 (MapPartitionsRDD[596] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:28:35,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 507.0 with 1 tasks
   [druid] 2019-06-11 09:28:35,035 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 507.0 (TID 339, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:35,035 [er for task 339] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 507.0 (TID 339)
   [druid] 2019-06-11 09:28:35,036 [er for task 339] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:35,036 [er for task 339] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:35,036 [er for task 339] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:35,036 [er for task 339] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:35,037 [er for task 339] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 507.0 (TID 339). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:28:35,037 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 507.0 (TID 339) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:35,037 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 507.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 507 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:35,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 169 finished: print at Transform.scala:36, took 0.030173 s
   [druid] 2019-06-11 09:28:35,040 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:35,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 168 is 83 bytes
   [druid] 2019-06-11 09:28:35,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 169 is 160 bytes
   [druid] 2019-06-11 09:28:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 170 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 510 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 509, ShuffleMapStage 508)
   [druid] 2019-06-11 09:28:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:28:35,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 510 (MapPartitionsRDD[596] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:35,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_255 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:35,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_255_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:35,043 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_255_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:35,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 255 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:35,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 510 (MapPartitionsRDD[596] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:28:35,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 510.0 with 1 tasks
   [druid] 2019-06-11 09:28:35,044 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 510.0 (TID 340, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:35,044 [er for task 340] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 510.0 (TID 340)
   [druid] 2019-06-11 09:28:35,045 [er for task 340] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:35,045 [er for task 340] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:35,045 [er for task 340] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:35,045 [er for task 340] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:35,045 [er for task 340] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 510.0 (TID 340). 966 bytes result sent to driver
   [druid] 2019-06-11 09:28:35,046 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 510.0 (TID 340) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:35,046 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 510.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 510 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:35,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 170 finished: print at Transform.scala:36, took 0.006358 s
   [druid] 2019-06-11 09:28:35,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216515000 ms.0 from job set of time 1560216515000 ms
   [druid] 2019-06-11 09:28:35,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.046 s for time 1560216515000 ms (execution: 0.041 s)
   [druid] 2019-06-11 09:28:35,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 589 from persistence list
   [druid] 2019-06-11 09:28:35,047 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 589
   [druid] 2019-06-11 09:28:35,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 584 from persistence list
   [druid] 2019-06-11 09:28:35,047 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 584
   [druid] 2019-06-11 09:28:35,047 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 583 from persistence list
   [druid] 2019-06-11 09:28:35,047 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 583
   [druid] 2019-06-11 09:28:35,047 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[583] at socketTextStream at Transform.scala:18 of time 1560216515000 ms
   [druid] 2019-06-11 09:28:35,047 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216505000 ms
   [druid] 2019-06-11 09:28:35,047 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216505000 ms
   [druid] 2019-06-11 09:28:40,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216520000 ms
   [druid] 2019-06-11 09:28:40,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216520000 ms.0 from job set of time 1560216520000 ms
   [druid] 2019-06-11 09:28:40,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:28:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 598 (map at Transform.scala:20)
   [druid] 2019-06-11 09:28:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 171 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 513 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 511, ShuffleMapStage 512)
   [druid] 2019-06-11 09:28:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 511)
   [druid] 2019-06-11 09:28:40,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 511 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:28:40,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_256 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:40,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_256_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:28:40,012 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_256_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:40,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 256 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:40,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 511 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:28:40,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 511.0 with 2 tasks
   [druid] 2019-06-11 09:28:40,013 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 511.0 (TID 341, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:40,013 [er for task 341] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 511.0 (TID 341)
   [druid] 2019-06-11 09:28:40,022 [er for task 341] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 511.0 (TID 341). 810 bytes result sent to driver
   [druid] 2019-06-11 09:28:40,022 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 511.0 (TID 342, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:40,022 [er for task 342] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 511.0 (TID 342)
   [druid] 2019-06-11 09:28:40,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 511.0 (TID 341) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:28:40,031 [er for task 342] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 511.0 (TID 342). 810 bytes result sent to driver
   [druid] 2019-06-11 09:28:40,032 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 511.0 (TID 342) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:28:40,032 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 511.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 511 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:28:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:28:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:28:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 513)
   [druid] 2019-06-11 09:28:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:28:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 513 (MapPartitionsRDD[603] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:40,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_257 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:40,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_257_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:40,035 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_257_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:40,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 257 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 513 (MapPartitionsRDD[603] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:28:40,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 513.0 with 1 tasks
   [druid] 2019-06-11 09:28:40,036 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 513.0 (TID 343, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:40,036 [er for task 343] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 513.0 (TID 343)
   [druid] 2019-06-11 09:28:40,037 [er for task 343] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:40,037 [er for task 343] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:40,037 [er for task 343] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:40,037 [er for task 343] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:40,038 [er for task 343] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 513.0 (TID 343). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:28:40,038 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 513.0 (TID 343) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:40,038 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 513.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 513 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:40,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 171 finished: print at Transform.scala:36, took 0.028899 s
   [druid] 2019-06-11 09:28:40,041 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:40,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 171 is 160 bytes
   [druid] 2019-06-11 09:28:40,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 170 is 83 bytes
   [druid] 2019-06-11 09:28:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 172 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 516 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 514, ShuffleMapStage 515)
   [druid] 2019-06-11 09:28:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:28:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 516 (MapPartitionsRDD[603] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:40,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_258 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:40,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_258_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:40,043 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_258_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:40,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 258 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:40,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 516 (MapPartitionsRDD[603] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:28:40,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 516.0 with 1 tasks
   [druid] 2019-06-11 09:28:40,044 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 516.0 (TID 344, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:40,044 [er for task 344] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 516.0 (TID 344)
   [druid] 2019-06-11 09:28:40,045 [er for task 344] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:40,045 [er for task 344] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:40,045 [er for task 344] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:40,045 [er for task 344] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:40,045 [er for task 344] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 516.0 (TID 344). 966 bytes result sent to driver
   [druid] 2019-06-11 09:28:40,046 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 516.0 (TID 344) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:40,046 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 516.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:40,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 516 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:40,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 172 finished: print at Transform.scala:36, took 0.005442 s
   [druid] 2019-06-11 09:28:40,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216520000 ms.0 from job set of time 1560216520000 ms
   [druid] 2019-06-11 09:28:40,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.046 s for time 1560216520000 ms (execution: 0.040 s)
   [druid] 2019-06-11 09:28:40,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 596 from persistence list
   [druid] 2019-06-11 09:28:40,047 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 596
   [druid] 2019-06-11 09:28:40,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 591 from persistence list
   [druid] 2019-06-11 09:28:40,047 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 591
   [druid] 2019-06-11 09:28:40,047 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 590 from persistence list
   [druid] 2019-06-11 09:28:40,047 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 590
   [druid] 2019-06-11 09:28:40,047 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[590] at socketTextStream at Transform.scala:18 of time 1560216520000 ms
   [druid] 2019-06-11 09:28:40,048 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216510000 ms
   [druid] 2019-06-11 09:28:40,048 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216510000 ms
   [druid] 2019-06-11 09:28:45,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216525000 ms
   [druid] 2019-06-11 09:28:45,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216525000 ms.0 from job set of time 1560216525000 ms
   [druid] 2019-06-11 09:28:45,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 605 (map at Transform.scala:20)
   [druid] 2019-06-11 09:28:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:28:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 173 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 519 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 517, ShuffleMapStage 518)
   [druid] 2019-06-11 09:28:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 518)
   [druid] 2019-06-11 09:28:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 518 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:28:45,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_259 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:45,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_259_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:28:45,010 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_259_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:45,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 259 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:45,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 518 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:28:45,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 518.0 with 2 tasks
   [druid] 2019-06-11 09:28:45,011 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 518.0 (TID 345, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:45,011 [er for task 345] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 518.0 (TID 345)
   [druid] 2019-06-11 09:28:45,021 [er for task 345] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 518.0 (TID 345). 853 bytes result sent to driver
   [druid] 2019-06-11 09:28:45,021 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 518.0 (TID 346, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:45,021 [er for task 346] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 518.0 (TID 346)
   [druid] 2019-06-11 09:28:45,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 518.0 (TID 345) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:28:45,029 [er for task 346] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 518.0 (TID 346). 767 bytes result sent to driver
   [druid] 2019-06-11 09:28:45,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 518.0 (TID 346) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:28:45,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 518.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 518 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:28:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:28:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:28:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 519)
   [druid] 2019-06-11 09:28:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:28:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 519 (MapPartitionsRDD[610] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:45,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_260 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:45,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_260_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:45,032 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_260_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:45,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 260 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:45,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 519 (MapPartitionsRDD[610] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:28:45,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 519.0 with 1 tasks
   [druid] 2019-06-11 09:28:45,032 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 519.0 (TID 347, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:45,032 [er for task 347] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 519.0 (TID 347)
   [druid] 2019-06-11 09:28:45,033 [er for task 347] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:45,033 [er for task 347] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:45,033 [er for task 347] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:45,033 [er for task 347] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:45,034 [er for task 347] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 519.0 (TID 347). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:28:45,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 519.0 (TID 347) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:45,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 519.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 519 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:45,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 173 finished: print at Transform.scala:36, took 0.026220 s
   [druid] 2019-06-11 09:28:45,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:45,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 172 is 83 bytes
   [druid] 2019-06-11 09:28:45,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 173 is 160 bytes
   [druid] 2019-06-11 09:28:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 174 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 522 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 520, ShuffleMapStage 521)
   [druid] 2019-06-11 09:28:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:28:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 522 (MapPartitionsRDD[610] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:45,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_261 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:45,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_261_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:45,040 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_261_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:45,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 261 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 522 (MapPartitionsRDD[610] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:28:45,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 522.0 with 1 tasks
   [druid] 2019-06-11 09:28:45,041 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 522.0 (TID 348, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:45,041 [er for task 348] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 522.0 (TID 348)
   [druid] 2019-06-11 09:28:45,041 [er for task 348] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:45,041 [er for task 348] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:45,041 [er for task 348] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:45,041 [er for task 348] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:45,042 [er for task 348] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 522.0 (TID 348). 966 bytes result sent to driver
   [druid] 2019-06-11 09:28:45,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 522.0 (TID 348) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:45,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 522.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 522 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:28:45,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 174 finished: print at Transform.scala:36, took 0.005657 s
   [druid] 2019-06-11 09:28:45,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216525000 ms.0 from job set of time 1560216525000 ms
   [druid] 2019-06-11 09:28:45,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216525000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:28:45,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 603 from persistence list
   [druid] 2019-06-11 09:28:45,043 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 603
   [druid] 2019-06-11 09:28:45,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 598 from persistence list
   [druid] 2019-06-11 09:28:45,044 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 598
   [druid] 2019-06-11 09:28:45,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 597 from persistence list
   [druid] 2019-06-11 09:28:45,044 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 597
   [druid] 2019-06-11 09:28:45,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[597] at socketTextStream at Transform.scala:18 of time 1560216525000 ms
   [druid] 2019-06-11 09:28:45,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216515000 ms
   [druid] 2019-06-11 09:28:45,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216515000 ms
   [druid] 2019-06-11 09:28:50,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216530000 ms
   [druid] 2019-06-11 09:28:50,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216530000 ms.0 from job set of time 1560216530000 ms
   [druid] 2019-06-11 09:28:50,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 612 (map at Transform.scala:20)
   [druid] 2019-06-11 09:28:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:28:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 175 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 525 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 524, ShuffleMapStage 523)
   [druid] 2019-06-11 09:28:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 524)
   [druid] 2019-06-11 09:28:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 524 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:28:50,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_262 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:50,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_262_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:28:50,010 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_262_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:50,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 262 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:50,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 524 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:28:50,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 524.0 with 2 tasks
   [druid] 2019-06-11 09:28:50,011 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 524.0 (TID 349, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:50,011 [er for task 349] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 524.0 (TID 349)
   [druid] 2019-06-11 09:28:50,025 [er for task 349] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 524.0 (TID 349). 767 bytes result sent to driver
   [druid] 2019-06-11 09:28:50,025 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 524.0 (TID 350, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:50,025 [er for task 350] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 524.0 (TID 350)
   [druid] 2019-06-11 09:28:50,025 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 524.0 (TID 349) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:28:50,034 [er for task 350] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 524.0 (TID 350). 853 bytes result sent to driver
   [druid] 2019-06-11 09:28:50,035 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 524.0 (TID 350) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:28:50,035 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 524.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 524 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:28:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:28:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:28:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 525)
   [druid] 2019-06-11 09:28:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:28:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 525 (MapPartitionsRDD[617] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:50,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_263 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:50,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_263_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:50,037 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_263_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:50,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 263 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 525 (MapPartitionsRDD[617] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:28:50,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 525.0 with 1 tasks
   [druid] 2019-06-11 09:28:50,038 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 525.0 (TID 351, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:50,038 [er for task 351] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 525.0 (TID 351)
   [druid] 2019-06-11 09:28:50,039 [er for task 351] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:50,039 [er for task 351] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:50,039 [er for task 351] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:50,039 [er for task 351] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:50,040 [er for task 351] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 525.0 (TID 351). 966 bytes result sent to driver
   [druid] 2019-06-11 09:28:50,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 525.0 (TID 351) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:50,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 525.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:50,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 525 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:28:50,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 175 finished: print at Transform.scala:36, took 0.031723 s
   [druid] 2019-06-11 09:28:50,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:50,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 174 is 83 bytes
   [druid] 2019-06-11 09:28:50,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 175 is 160 bytes
   [druid] 2019-06-11 09:28:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 176 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 528 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 527, ShuffleMapStage 526)
   [druid] 2019-06-11 09:28:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:28:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 528 (MapPartitionsRDD[617] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:50,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_264 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:50,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_264_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:50,046 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_264_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:50,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 264 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:50,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 528 (MapPartitionsRDD[617] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:28:50,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 528.0 with 1 tasks
   [druid] 2019-06-11 09:28:50,047 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 528.0 (TID 352, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:50,047 [er for task 352] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 528.0 (TID 352)
   [druid] 2019-06-11 09:28:50,048 [er for task 352] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:50,048 [er for task 352] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:50,048 [er for task 352] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:50,048 [er for task 352] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:50,049 [er for task 352] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 528.0 (TID 352). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:28:50,049 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 528.0 (TID 352) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:50,049 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 528.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 528 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:28:50,050 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 176 finished: print at Transform.scala:36, took 0.006243 s
   [druid] 2019-06-11 09:28:50,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216530000 ms.0 from job set of time 1560216530000 ms
   [druid] 2019-06-11 09:28:50,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.050 s for time 1560216530000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:28:50,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 610 from persistence list
   [druid] 2019-06-11 09:28:50,050 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 610
   [druid] 2019-06-11 09:28:50,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 605 from persistence list
   [druid] 2019-06-11 09:28:50,050 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 605
   [druid] 2019-06-11 09:28:50,050 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 604 from persistence list
   [druid] 2019-06-11 09:28:50,051 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 604
   [druid] 2019-06-11 09:28:50,051 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[604] at socketTextStream at Transform.scala:18 of time 1560216530000 ms
   [druid] 2019-06-11 09:28:50,051 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216520000 ms
   [druid] 2019-06-11 09:28:50,051 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216520000 ms
   [druid] 2019-06-11 09:28:55,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216535000 ms
   [druid] 2019-06-11 09:28:55,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216535000 ms.0 from job set of time 1560216535000 ms
   [druid] 2019-06-11 09:28:55,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:28:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 619 (map at Transform.scala:20)
   [druid] 2019-06-11 09:28:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 177 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 531 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 529, ShuffleMapStage 530)
   [druid] 2019-06-11 09:28:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 529)
   [druid] 2019-06-11 09:28:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 529 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:28:55,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_265 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:55,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_265_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:28:55,010 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_265_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 265 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 529 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:28:55,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 529.0 with 2 tasks
   [druid] 2019-06-11 09:28:55,010 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 529.0 (TID 353, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:55,010 [er for task 353] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 529.0 (TID 353)
   [druid] 2019-06-11 09:28:55,025 [er for task 353] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 529.0 (TID 353). 810 bytes result sent to driver
   [druid] 2019-06-11 09:28:55,025 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 529.0 (TID 354, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:28:55,025 [er for task 354] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 529.0 (TID 354)
   [druid] 2019-06-11 09:28:55,025 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 529.0 (TID 353) in 15 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:28:55,033 [er for task 354] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 529.0 (TID 354). 853 bytes result sent to driver
   [druid] 2019-06-11 09:28:55,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 529.0 (TID 354) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:28:55,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 529.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 529 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:28:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:28:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:28:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 531)
   [druid] 2019-06-11 09:28:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:28:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 531 (MapPartitionsRDD[624] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:55,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_266 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:55,041 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 162
   [druid] 2019-06-11 09:28:55,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_266_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:55,042 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 163
   [druid] 2019-06-11 09:28:55,042 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_266_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 170
   [druid] 2019-06-11 09:28:55,045 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 266 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:55,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 531 (MapPartitionsRDD[624] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:28:55,047 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_251_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,047 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 531.0 with 1 tasks
   [druid] 2019-06-11 09:28:55,047 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 531.0 (TID 355, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:55,048 [er for task 355] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 531.0 (TID 355)
   [druid] 2019-06-11 09:28:55,048 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_263_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,048 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 173
   [druid] 2019-06-11 09:28:55,048 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 172
   [druid] 2019-06-11 09:28:55,049 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 171
   [druid] 2019-06-11 09:28:55,050 [er for task 355] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:55,050 [er for task 355] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:55,050 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_253_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,050 [er for task 355] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:55,050 [er for task 355] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:55,051 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_247_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,051 [er for task 355] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 531.0 (TID 355). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:28:55,052 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 531.0 (TID 355) in 5 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:55,052 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 531.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:55,052 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_248_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 531 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:28:55,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 177 finished: print at Transform.scala:36, took 0.044591 s
   [druid] 2019-06-11 09:28:55,053 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_262_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,054 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_254_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,055 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_249_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,055 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_260_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,056 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_257_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,056 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:28:55,057 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 177 is 160 bytes
   [druid] 2019-06-11 09:28:55,057 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 167
   [druid] 2019-06-11 09:28:55,057 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 176 is 83 bytes
   [druid] 2019-06-11 09:28:55,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 178 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:28:55,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 534 (print at Transform.scala:36)
   [druid] 2019-06-11 09:28:55,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 532, ShuffleMapStage 533)
   [druid] 2019-06-11 09:28:55,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:28:55,058 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_256_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 534 (MapPartitionsRDD[624] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:28:55,058 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_267 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:55,059 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_258_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,060 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_267_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:28:55,060 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_267_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,060 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_246_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,060 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 267 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:28:55,061 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 160
   [druid] 2019-06-11 09:28:55,061 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 534 (MapPartitionsRDD[624] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:28:55,061 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 534.0 with 1 tasks
   [druid] 2019-06-11 09:28:55,061 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 165
   [druid] 2019-06-11 09:28:55,062 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 169
   [druid] 2019-06-11 09:28:55,062 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 534.0 (TID 356, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:28:55,063 [er for task 356] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 534.0 (TID 356)
   [druid] 2019-06-11 09:28:55,063 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_250_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,063 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 161
   [druid] 2019-06-11 09:28:55,064 [er for task 356] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:28:55,064 [er for task 356] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:55,064 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_255_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,064 [er for task 356] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:28:55,064 [er for task 356] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:28:55,065 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 166
   [druid] 2019-06-11 09:28:55,065 [er for task 356] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 534.0 (TID 356). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:28:55,066 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_264_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,066 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 534.0 (TID 356) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:28:55,066 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 534.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:28:55,066 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 534 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:28:55,066 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 178 finished: print at Transform.scala:36, took 0.009832 s
   [druid] 2019-06-11 09:28:55,066 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216535000 ms.0 from job set of time 1560216535000 ms
   [druid] 2019-06-11 09:28:55,066 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.066 s for time 1560216535000 ms (execution: 0.061 s)
   [druid] 2019-06-11 09:28:55,067 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_252_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,067 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 617 from persistence list
   [druid] 2019-06-11 09:28:55,067 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 612 from persistence list
   [druid] 2019-06-11 09:28:55,067 [-thread-pool-17] INFO  che.spark.storage.BlockManager {1} - Removing RDD 617
   [druid] 2019-06-11 09:28:55,067 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 168
   [druid] 2019-06-11 09:28:55,067 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 611 from persistence list
   [druid] 2019-06-11 09:28:55,067 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 612
   [druid] 2019-06-11 09:28:55,067 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[611] at socketTextStream at Transform.scala:18 of time 1560216535000 ms
   [druid] 2019-06-11 09:28:55,067 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216525000 ms
   [druid] 2019-06-11 09:28:55,067 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216525000 ms
   [druid] 2019-06-11 09:28:55,068 [-thread-pool-17] INFO  che.spark.storage.BlockManager {1} - Removing RDD 611
   [druid] 2019-06-11 09:28:55,068 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_261_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:28:55,068 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 164
   [druid] 2019-06-11 09:28:55,069 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_259_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:00,010 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216540000 ms
   [druid] 2019-06-11 09:29:00,010 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216540000 ms.0 from job set of time 1560216540000 ms
   [druid] 2019-06-11 09:29:00,013 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:29:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 626 (map at Transform.scala:20)
   [druid] 2019-06-11 09:29:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 179 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:00,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 537 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:00,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 535, ShuffleMapStage 536)
   [druid] 2019-06-11 09:29:00,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 535)
   [druid] 2019-06-11 09:29:00,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 535 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:29:00,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_268 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:00,016 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_268_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:29:00,016 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_268_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:00,016 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 268 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:00,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 535 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:29:00,017 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 535.0 with 2 tasks
   [druid] 2019-06-11 09:29:00,017 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 535.0 (TID 357, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:00,017 [er for task 357] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 535.0 (TID 357)
   [druid] 2019-06-11 09:29:00,033 [er for task 357] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 535.0 (TID 357). 853 bytes result sent to driver
   [druid] 2019-06-11 09:29:00,033 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 535.0 (TID 358, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:00,033 [er for task 358] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 535.0 (TID 358)
   [druid] 2019-06-11 09:29:00,033 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 535.0 (TID 357) in 16 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:29:00,045 [er for task 358] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 535.0 (TID 358). 767 bytes result sent to driver
   [druid] 2019-06-11 09:29:00,045 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 535.0 (TID 358) in 12 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:29:00,045 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 535.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:00,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 535 (parallelize at Transform.scala:16) finished in 0.028 s
   [druid] 2019-06-11 09:29:00,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:29:00,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:29:00,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 537)
   [druid] 2019-06-11 09:29:00,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:29:00,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 537 (MapPartitionsRDD[631] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:00,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_269 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:00,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_269_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:00,049 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_269_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:00,049 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 269 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:00,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 537 (MapPartitionsRDD[631] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:29:00,049 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 537.0 with 1 tasks
   [druid] 2019-06-11 09:29:00,049 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 537.0 (TID 359, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:00,050 [er for task 359] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 537.0 (TID 359)
   [druid] 2019-06-11 09:29:00,050 [er for task 359] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:00,051 [er for task 359] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:29:00,051 [er for task 359] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:00,051 [er for task 359] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:00,052 [er for task 359] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 537.0 (TID 359). 966 bytes result sent to driver
   [druid] 2019-06-11 09:29:00,052 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 537.0 (TID 359) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:00,052 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 537.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:00,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 537 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:29:00,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 179 finished: print at Transform.scala:36, took 0.039197 s
   [druid] 2019-06-11 09:29:00,055 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:00,056 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 179 is 160 bytes
   [druid] 2019-06-11 09:29:00,056 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 178 is 83 bytes
   [druid] 2019-06-11 09:29:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 180 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 540 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 538, ShuffleMapStage 539)
   [druid] 2019-06-11 09:29:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:29:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 540 (MapPartitionsRDD[631] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:00,057 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_270 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:00,058 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_270_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:00,059 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_270_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:00,059 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 270 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:00,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 540 (MapPartitionsRDD[631] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:29:00,059 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 540.0 with 1 tasks
   [druid] 2019-06-11 09:29:00,059 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 540.0 (TID 360, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:00,059 [er for task 360] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 540.0 (TID 360)
   [druid] 2019-06-11 09:29:00,060 [er for task 360] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:00,060 [er for task 360] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:00,061 [er for task 360] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:00,061 [er for task 360] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:00,062 [er for task 360] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 540.0 (TID 360). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:29:00,062 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 540.0 (TID 360) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:00,062 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 540.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:00,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 540 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:29:00,062 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 180 finished: print at Transform.scala:36, took 0.007673 s
   [druid] 2019-06-11 09:29:00,063 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216540000 ms.0 from job set of time 1560216540000 ms
   [druid] 2019-06-11 09:29:00,063 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.063 s for time 1560216540000 ms (execution: 0.053 s)
   [druid] 2019-06-11 09:29:00,063 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 624 from persistence list
   [druid] 2019-06-11 09:29:00,063 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 624
   [druid] 2019-06-11 09:29:00,063 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 619 from persistence list
   [druid] 2019-06-11 09:29:00,063 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 619
   [druid] 2019-06-11 09:29:00,063 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 618 from persistence list
   [druid] 2019-06-11 09:29:00,064 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 618
   [druid] 2019-06-11 09:29:00,064 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[618] at socketTextStream at Transform.scala:18 of time 1560216540000 ms
   [druid] 2019-06-11 09:29:00,064 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216530000 ms
   [druid] 2019-06-11 09:29:00,064 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216530000 ms
   [druid] 2019-06-11 09:29:05,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216545000 ms
   [druid] 2019-06-11 09:29:05,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216545000 ms.0 from job set of time 1560216545000 ms
   [druid] 2019-06-11 09:29:05,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:29:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 633 (map at Transform.scala:20)
   [druid] 2019-06-11 09:29:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 181 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 543 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 542, ShuffleMapStage 541)
   [druid] 2019-06-11 09:29:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 541)
   [druid] 2019-06-11 09:29:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 541 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:29:05,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_271 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:05,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_271_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:29:05,011 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_271_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:05,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 271 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:05,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 541 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:29:05,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 541.0 with 2 tasks
   [druid] 2019-06-11 09:29:05,012 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 541.0 (TID 361, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:05,012 [er for task 361] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 541.0 (TID 361)
   [druid] 2019-06-11 09:29:05,021 [er for task 361] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 541.0 (TID 361). 853 bytes result sent to driver
   [druid] 2019-06-11 09:29:05,021 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 541.0 (TID 362, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:05,021 [er for task 362] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 541.0 (TID 362)
   [druid] 2019-06-11 09:29:05,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 541.0 (TID 361) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:29:05,035 [er for task 362] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 541.0 (TID 362). 853 bytes result sent to driver
   [druid] 2019-06-11 09:29:05,036 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 541.0 (TID 362) in 15 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:29:05,036 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 541.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 541 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:29:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:29:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:29:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 543)
   [druid] 2019-06-11 09:29:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:29:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 543 (MapPartitionsRDD[638] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:05,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_272 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:05,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_272_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:05,037 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_272_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:05,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 272 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:05,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 543 (MapPartitionsRDD[638] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:29:05,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 543.0 with 1 tasks
   [druid] 2019-06-11 09:29:05,038 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 543.0 (TID 363, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:05,038 [er for task 363] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 543.0 (TID 363)
   [druid] 2019-06-11 09:29:05,039 [er for task 363] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:05,039 [er for task 363] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:05,039 [er for task 363] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:05,039 [er for task 363] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:05,040 [er for task 363] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 543.0 (TID 363). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:29:05,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 543.0 (TID 363) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:05,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 543.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:05,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 543 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:05,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 181 finished: print at Transform.scala:36, took 0.030953 s
   [druid] 2019-06-11 09:29:05,042 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:05,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 181 is 160 bytes
   [druid] 2019-06-11 09:29:05,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 180 is 83 bytes
   [druid] 2019-06-11 09:29:05,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 182 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:05,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 546 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:05,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 545, ShuffleMapStage 544)
   [druid] 2019-06-11 09:29:05,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:29:05,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 546 (MapPartitionsRDD[638] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:05,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_273 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:05,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_273_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:05,044 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_273_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:05,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 273 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:05,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 546 (MapPartitionsRDD[638] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:29:05,045 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 546.0 with 1 tasks
   [druid] 2019-06-11 09:29:05,045 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 546.0 (TID 364, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:05,045 [er for task 364] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 546.0 (TID 364)
   [druid] 2019-06-11 09:29:05,046 [er for task 364] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:05,046 [er for task 364] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:05,046 [er for task 364] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:05,046 [er for task 364] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:05,046 [er for task 364] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 546.0 (TID 364). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:29:05,047 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 546.0 (TID 364) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:05,047 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 546.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:05,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 546 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:05,047 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 182 finished: print at Transform.scala:36, took 0.004655 s
   [druid] 2019-06-11 09:29:05,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216545000 ms.0 from job set of time 1560216545000 ms
   [druid] 2019-06-11 09:29:05,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.047 s for time 1560216545000 ms (execution: 0.039 s)
   [druid] 2019-06-11 09:29:05,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 631 from persistence list
   [druid] 2019-06-11 09:29:05,048 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 631
   [druid] 2019-06-11 09:29:05,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 626 from persistence list
   [druid] 2019-06-11 09:29:05,048 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 626
   [druid] 2019-06-11 09:29:05,048 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 625 from persistence list
   [druid] 2019-06-11 09:29:05,048 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 625
   [druid] 2019-06-11 09:29:05,048 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[625] at socketTextStream at Transform.scala:18 of time 1560216545000 ms
   [druid] 2019-06-11 09:29:05,048 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216535000 ms
   [druid] 2019-06-11 09:29:05,048 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216535000 ms
   [druid] 2019-06-11 09:29:10,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216550000 ms
   [druid] 2019-06-11 09:29:10,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216550000 ms.0 from job set of time 1560216550000 ms
   [druid] 2019-06-11 09:29:10,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 640 (map at Transform.scala:20)
   [druid] 2019-06-11 09:29:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:29:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 183 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 549 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 547, ShuffleMapStage 548)
   [druid] 2019-06-11 09:29:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 548)
   [druid] 2019-06-11 09:29:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 548 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:29:10,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_274 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:10,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_274_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:29:10,011 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_274_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:10,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 274 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:10,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 548 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:29:10,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 548.0 with 2 tasks
   [druid] 2019-06-11 09:29:10,012 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 548.0 (TID 365, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:10,012 [er for task 365] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 548.0 (TID 365)
   [druid] 2019-06-11 09:29:10,021 [er for task 365] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 548.0 (TID 365). 810 bytes result sent to driver
   [druid] 2019-06-11 09:29:10,021 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 548.0 (TID 366, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:10,021 [er for task 366] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 548.0 (TID 366)
   [druid] 2019-06-11 09:29:10,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 548.0 (TID 365) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:29:10,035 [er for task 366] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 548.0 (TID 366). 853 bytes result sent to driver
   [druid] 2019-06-11 09:29:10,035 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 548.0 (TID 366) in 14 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:29:10,035 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 548.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:10,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 548 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:29:10,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:29:10,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:29:10,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 549)
   [druid] 2019-06-11 09:29:10,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:29:10,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 549 (MapPartitionsRDD[645] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:10,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_275 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:10,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_275_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:10,037 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_275_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:10,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 275 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 549 (MapPartitionsRDD[645] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:29:10,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 549.0 with 1 tasks
   [druid] 2019-06-11 09:29:10,038 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 549.0 (TID 367, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:10,038 [er for task 367] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 549.0 (TID 367)
   [druid] 2019-06-11 09:29:10,039 [er for task 367] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:10,039 [er for task 367] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:10,039 [er for task 367] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:10,039 [er for task 367] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:10,040 [er for task 367] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 549.0 (TID 367). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:29:10,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 549.0 (TID 367) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:10,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 549.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 549 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:10,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 183 finished: print at Transform.scala:36, took 0.031085 s
   [druid] 2019-06-11 09:29:10,042 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:10,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 182 is 83 bytes
   [druid] 2019-06-11 09:29:10,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 183 is 160 bytes
   [druid] 2019-06-11 09:29:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 184 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 552 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 550, ShuffleMapStage 551)
   [druid] 2019-06-11 09:29:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:29:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 552 (MapPartitionsRDD[645] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:10,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_276 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:10,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_276_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:10,044 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_276_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:10,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 276 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:10,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 552 (MapPartitionsRDD[645] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:29:10,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 552.0 with 1 tasks
   [druid] 2019-06-11 09:29:10,045 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 552.0 (TID 368, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:10,045 [er for task 368] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 552.0 (TID 368)
   [druid] 2019-06-11 09:29:10,046 [er for task 368] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:10,046 [er for task 368] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:10,046 [er for task 368] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:10,046 [er for task 368] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:10,047 [er for task 368] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 552.0 (TID 368). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:29:10,047 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 552.0 (TID 368) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:10,047 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 552.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:10,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 552 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:10,048 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 184 finished: print at Transform.scala:36, took 0.005664 s
   [druid] 2019-06-11 09:29:10,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216550000 ms.0 from job set of time 1560216550000 ms
   [druid] 2019-06-11 09:29:10,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.048 s for time 1560216550000 ms (execution: 0.042 s)
   [druid] 2019-06-11 09:29:10,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 638 from persistence list
   [druid] 2019-06-11 09:29:10,048 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 638
   [druid] 2019-06-11 09:29:10,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 633 from persistence list
   [druid] 2019-06-11 09:29:10,048 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 633
   [druid] 2019-06-11 09:29:10,048 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 632 from persistence list
   [druid] 2019-06-11 09:29:10,049 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 632
   [druid] 2019-06-11 09:29:10,049 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[632] at socketTextStream at Transform.scala:18 of time 1560216550000 ms
   [druid] 2019-06-11 09:29:10,049 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216540000 ms
   [druid] 2019-06-11 09:29:10,049 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216540000 ms
   [druid] 2019-06-11 09:29:15,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216555000 ms
   [druid] 2019-06-11 09:29:15,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216555000 ms.0 from job set of time 1560216555000 ms
   [druid] 2019-06-11 09:29:15,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 647 (map at Transform.scala:20)
   [druid] 2019-06-11 09:29:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:29:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 185 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 555 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 553, ShuffleMapStage 554)
   [druid] 2019-06-11 09:29:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 554)
   [druid] 2019-06-11 09:29:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 554 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:29:15,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_277 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:15,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_277_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:29:15,010 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_277_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:15,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 277 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 554 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:29:15,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 554.0 with 2 tasks
   [druid] 2019-06-11 09:29:15,011 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 554.0 (TID 369, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:15,011 [er for task 369] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 554.0 (TID 369)
   [druid] 2019-06-11 09:29:15,024 [er for task 369] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 554.0 (TID 369). 810 bytes result sent to driver
   [druid] 2019-06-11 09:29:15,025 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 554.0 (TID 370, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:15,025 [er for task 370] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 554.0 (TID 370)
   [druid] 2019-06-11 09:29:15,025 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 554.0 (TID 369) in 15 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:29:15,034 [er for task 370] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 554.0 (TID 370). 853 bytes result sent to driver
   [druid] 2019-06-11 09:29:15,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 554.0 (TID 370) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:29:15,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 554.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 554 (parallelize at Transform.scala:16) finished in 0.025 s
   [druid] 2019-06-11 09:29:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:29:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:29:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 555)
   [druid] 2019-06-11 09:29:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:29:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 555 (MapPartitionsRDD[652] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:15,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_278 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:15,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_278_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:15,036 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_278_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:15,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 278 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 555 (MapPartitionsRDD[652] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:29:15,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 555.0 with 1 tasks
   [druid] 2019-06-11 09:29:15,037 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 555.0 (TID 371, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:15,037 [er for task 371] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 555.0 (TID 371)
   [druid] 2019-06-11 09:29:15,038 [er for task 371] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:15,038 [er for task 371] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:15,039 [er for task 371] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:15,039 [er for task 371] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:29:15,040 [er for task 371] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 555.0 (TID 371). 966 bytes result sent to driver
   [druid] 2019-06-11 09:29:15,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 555.0 (TID 371) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:15,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 555.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 555 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:29:15,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 185 finished: print at Transform.scala:36, took 0.031965 s
   [druid] 2019-06-11 09:29:15,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:15,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 184 is 83 bytes
   [druid] 2019-06-11 09:29:15,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 185 is 160 bytes
   [druid] 2019-06-11 09:29:15,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 186 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:15,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 558 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:15,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 556, ShuffleMapStage 557)
   [druid] 2019-06-11 09:29:15,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:29:15,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 558 (MapPartitionsRDD[652] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:15,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_279 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:15,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_279_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:15,045 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_279_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:15,045 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 279 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:15,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 558 (MapPartitionsRDD[652] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:29:15,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 558.0 with 1 tasks
   [druid] 2019-06-11 09:29:15,046 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 558.0 (TID 372, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:15,046 [er for task 372] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 558.0 (TID 372)
   [druid] 2019-06-11 09:29:15,047 [er for task 372] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:15,047 [er for task 372] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:15,047 [er for task 372] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:15,047 [er for task 372] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:15,048 [er for task 372] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 558.0 (TID 372). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:29:15,049 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 558.0 (TID 372) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:15,049 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 558.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:15,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 558 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:29:15,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 186 finished: print at Transform.scala:36, took 0.006538 s
   [druid] 2019-06-11 09:29:15,049 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216555000 ms.0 from job set of time 1560216555000 ms
   [druid] 2019-06-11 09:29:15,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.049 s for time 1560216555000 ms (execution: 0.043 s)
   [druid] 2019-06-11 09:29:15,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 645 from persistence list
   [druid] 2019-06-11 09:29:15,050 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 645
   [druid] 2019-06-11 09:29:15,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 640 from persistence list
   [druid] 2019-06-11 09:29:15,050 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 640
   [druid] 2019-06-11 09:29:15,050 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 639 from persistence list
   [druid] 2019-06-11 09:29:15,050 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 639
   [druid] 2019-06-11 09:29:15,050 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[639] at socketTextStream at Transform.scala:18 of time 1560216555000 ms
   [druid] 2019-06-11 09:29:15,050 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216545000 ms
   [druid] 2019-06-11 09:29:15,050 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216545000 ms
   [druid] 2019-06-11 09:29:20,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216560000 ms
   [druid] 2019-06-11 09:29:20,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216560000 ms.0 from job set of time 1560216560000 ms
   [druid] 2019-06-11 09:29:20,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 654 (map at Transform.scala:20)
   [druid] 2019-06-11 09:29:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:29:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 187 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 561 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 560, ShuffleMapStage 559)
   [druid] 2019-06-11 09:29:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 560)
   [druid] 2019-06-11 09:29:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 560 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:29:20,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_280 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:20,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_280_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:29:20,011 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_280_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:20,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 280 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:20,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 560 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:29:20,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 560.0 with 2 tasks
   [druid] 2019-06-11 09:29:20,012 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 560.0 (TID 373, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:20,012 [er for task 373] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 560.0 (TID 373)
   [druid] 2019-06-11 09:29:20,020 [er for task 373] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 560.0 (TID 373). 810 bytes result sent to driver
   [druid] 2019-06-11 09:29:20,020 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 560.0 (TID 374, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:20,020 [er for task 374] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 560.0 (TID 374)
   [druid] 2019-06-11 09:29:20,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 560.0 (TID 373) in 8 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:29:20,028 [er for task 374] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 560.0 (TID 374). 810 bytes result sent to driver
   [druid] 2019-06-11 09:29:20,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 560.0 (TID 374) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:29:20,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 560.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 560 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:29:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:29:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:29:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 561)
   [druid] 2019-06-11 09:29:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:29:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 561 (MapPartitionsRDD[659] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:20,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_281 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:20,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_281_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:20,030 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_281_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:20,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 281 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 561 (MapPartitionsRDD[659] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:29:20,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 561.0 with 1 tasks
   [druid] 2019-06-11 09:29:20,031 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 561.0 (TID 375, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:20,031 [er for task 375] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 561.0 (TID 375)
   [druid] 2019-06-11 09:29:20,032 [er for task 375] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:20,032 [er for task 375] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:20,032 [er for task 375] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:20,032 [er for task 375] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:20,033 [er for task 375] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 561.0 (TID 375). 966 bytes result sent to driver
   [druid] 2019-06-11 09:29:20,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 561.0 (TID 375) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:20,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 561.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:20,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 561 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:20,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 187 finished: print at Transform.scala:36, took 0.024991 s
   [druid] 2019-06-11 09:29:20,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:20,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 186 is 83 bytes
   [druid] 2019-06-11 09:29:20,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 187 is 160 bytes
   [druid] 2019-06-11 09:29:20,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 188 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:20,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 564 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:20,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 562, ShuffleMapStage 563)
   [druid] 2019-06-11 09:29:20,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:29:20,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 564 (MapPartitionsRDD[659] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:20,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_282 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:20,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_282_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:20,038 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_282_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:20,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 282 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 564 (MapPartitionsRDD[659] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:29:20,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 564.0 with 1 tasks
   [druid] 2019-06-11 09:29:20,040 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 564.0 (TID 376, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:20,040 [er for task 376] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 564.0 (TID 376)
   [druid] 2019-06-11 09:29:20,041 [er for task 376] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:20,041 [er for task 376] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:20,041 [er for task 376] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:20,041 [er for task 376] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:20,041 [er for task 376] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 564.0 (TID 376). 966 bytes result sent to driver
   [druid] 2019-06-11 09:29:20,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 564.0 (TID 376) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:20,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 564.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:20,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 564 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:29:20,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 188 finished: print at Transform.scala:36, took 0.006514 s
   [druid] 2019-06-11 09:29:20,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216560000 ms.0 from job set of time 1560216560000 ms
   [druid] 2019-06-11 09:29:20,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560216560000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:29:20,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 652 from persistence list
   [druid] 2019-06-11 09:29:20,043 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 652
   [druid] 2019-06-11 09:29:20,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 647 from persistence list
   [druid] 2019-06-11 09:29:20,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 646 from persistence list
   [druid] 2019-06-11 09:29:20,043 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 647
   [druid] 2019-06-11 09:29:20,043 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 646
   [druid] 2019-06-11 09:29:20,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[646] at socketTextStream at Transform.scala:18 of time 1560216560000 ms
   [druid] 2019-06-11 09:29:20,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216550000 ms
   [druid] 2019-06-11 09:29:20,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216550000 ms
   [druid] 2019-06-11 09:29:25,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216565000 ms
   [druid] 2019-06-11 09:29:25,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216565000 ms.0 from job set of time 1560216565000 ms
   [druid] 2019-06-11 09:29:25,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 661 (map at Transform.scala:20)
   [druid] 2019-06-11 09:29:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:29:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 189 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 567 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 565, ShuffleMapStage 566)
   [druid] 2019-06-11 09:29:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 566)
   [druid] 2019-06-11 09:29:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 566 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:29:25,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_283 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_283_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:29:25,009 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_283_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 283 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 566 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:29:25,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 566.0 with 2 tasks
   [druid] 2019-06-11 09:29:25,010 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 566.0 (TID 377, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:25,010 [er for task 377] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 566.0 (TID 377)
   [druid] 2019-06-11 09:29:25,019 [er for task 377] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 566.0 (TID 377). 810 bytes result sent to driver
   [druid] 2019-06-11 09:29:25,019 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 566.0 (TID 378, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:25,019 [er for task 378] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 566.0 (TID 378)
   [druid] 2019-06-11 09:29:25,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 566.0 (TID 377) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:29:25,027 [er for task 378] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 566.0 (TID 378). 767 bytes result sent to driver
   [druid] 2019-06-11 09:29:25,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 566.0 (TID 378) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:29:25,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 566.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:25,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 566 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:29:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:29:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:29:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 567)
   [druid] 2019-06-11 09:29:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:29:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 567 (MapPartitionsRDD[666] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:25,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_284 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:25,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_284_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:25,029 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_284_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 284 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 567 (MapPartitionsRDD[666] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:29:25,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 567.0 with 1 tasks
   [druid] 2019-06-11 09:29:25,030 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 567.0 (TID 379, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:25,030 [er for task 379] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 567.0 (TID 379)
   [druid] 2019-06-11 09:29:25,031 [er for task 379] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:25,031 [er for task 379] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:25,031 [er for task 379] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:25,031 [er for task 379] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:25,032 [er for task 379] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 567.0 (TID 379). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:29:25,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 567.0 (TID 379) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:25,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 567.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 567 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:25,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 189 finished: print at Transform.scala:36, took 0.024641 s
   [druid] 2019-06-11 09:29:25,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:25,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 188 is 83 bytes
   [druid] 2019-06-11 09:29:25,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 189 is 160 bytes
   [druid] 2019-06-11 09:29:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 190 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 570 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 568, ShuffleMapStage 569)
   [druid] 2019-06-11 09:29:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:29:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 570 (MapPartitionsRDD[666] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:25,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_285 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:25,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_285_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:25,037 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_285_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 285 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:25,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 570 (MapPartitionsRDD[666] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:29:25,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 570.0 with 1 tasks
   [druid] 2019-06-11 09:29:25,038 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 570.0 (TID 380, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:25,038 [er for task 380] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 570.0 (TID 380)
   [druid] 2019-06-11 09:29:25,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 185
   [druid] 2019-06-11 09:29:25,046 [er for task 380] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:25,047 [er for task 380] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:29:25,049 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_268_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,049 [er for task 380] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:25,050 [er for task 380] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:29:25,051 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_274_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,051 [er for task 380] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 570.0 (TID 380). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:29:25,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 179
   [druid] 2019-06-11 09:29:25,052 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 570.0 (TID 380) in 14 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:25,052 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 570.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:25,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 570 (print at Transform.scala:36) finished in 0.015 s
   [druid] 2019-06-11 09:29:25,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 190 finished: print at Transform.scala:36, took 0.017849 s
   [druid] 2019-06-11 09:29:25,052 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_279_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216565000 ms.0 from job set of time 1560216565000 ms
   [druid] 2019-06-11 09:29:25,053 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.052 s for time 1560216565000 ms (execution: 0.046 s)
   [druid] 2019-06-11 09:29:25,053 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 659 from persistence list
   [druid] 2019-06-11 09:29:25,053 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 659
   [druid] 2019-06-11 09:29:25,053 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 654 from persistence list
   [druid] 2019-06-11 09:29:25,053 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 654
   [druid] 2019-06-11 09:29:25,053 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 653 from persistence list
   [druid] 2019-06-11 09:29:25,053 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 653
   [druid] 2019-06-11 09:29:25,053 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[653] at socketTextStream at Transform.scala:18 of time 1560216565000 ms
   [druid] 2019-06-11 09:29:25,053 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216555000 ms
   [druid] 2019-06-11 09:29:25,053 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216555000 ms
   [druid] 2019-06-11 09:29:25,053 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_276_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,054 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 176
   [druid] 2019-06-11 09:29:25,054 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 184
   [druid] 2019-06-11 09:29:25,054 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 182
   [druid] 2019-06-11 09:29:25,055 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_272_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 177
   [druid] 2019-06-11 09:29:25,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 175
   [druid] 2019-06-11 09:29:25,055 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_273_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,056 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_266_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,057 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_270_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,058 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_282_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,059 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_275_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,059 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 183
   [druid] 2019-06-11 09:29:25,060 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 174
   [druid] 2019-06-11 09:29:25,060 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 180
   [druid] 2019-06-11 09:29:25,060 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_281_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,061 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_284_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,062 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 181
   [druid] 2019-06-11 09:29:25,062 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 178
   [druid] 2019-06-11 09:29:25,062 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_277_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,063 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_278_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,064 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_283_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,065 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_269_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,066 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_280_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,067 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_271_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,068 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_267_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:25,069 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_265_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:30,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216570000 ms
   [druid] 2019-06-11 09:29:30,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216570000 ms.0 from job set of time 1560216570000 ms
   [druid] 2019-06-11 09:29:30,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 668 (map at Transform.scala:20)
   [druid] 2019-06-11 09:29:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:29:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 191 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 573 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 571, ShuffleMapStage 572)
   [druid] 2019-06-11 09:29:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 572)
   [druid] 2019-06-11 09:29:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 572 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:29:30,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_286 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:30,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_286_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:29:30,011 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_286_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:30,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 286 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 572 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:29:30,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 572.0 with 2 tasks
   [druid] 2019-06-11 09:29:30,012 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 572.0 (TID 381, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:30,012 [er for task 381] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 572.0 (TID 381)
   [druid] 2019-06-11 09:29:30,021 [er for task 381] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 572.0 (TID 381). 810 bytes result sent to driver
   [druid] 2019-06-11 09:29:30,021 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 572.0 (TID 382, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:30,022 [er for task 382] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 572.0 (TID 382)
   [druid] 2019-06-11 09:29:30,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 572.0 (TID 381) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:29:30,030 [er for task 382] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 572.0 (TID 382). 767 bytes result sent to driver
   [druid] 2019-06-11 09:29:30,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 572.0 (TID 382) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:29:30,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 572.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 572 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:29:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:29:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:29:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 573)
   [druid] 2019-06-11 09:29:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:29:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 573 (MapPartitionsRDD[673] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:30,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_287 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:30,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_287_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:30,032 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_287_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:30,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 287 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:30,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 573 (MapPartitionsRDD[673] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:29:30,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 573.0 with 1 tasks
   [druid] 2019-06-11 09:29:30,033 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 573.0 (TID 383, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:30,033 [er for task 383] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 573.0 (TID 383)
   [druid] 2019-06-11 09:29:30,034 [er for task 383] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:30,034 [er for task 383] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:30,034 [er for task 383] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:30,034 [er for task 383] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:30,035 [er for task 383] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 573.0 (TID 383). 966 bytes result sent to driver
   [druid] 2019-06-11 09:29:30,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 573.0 (TID 383) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:30,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 573.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 573 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:29:30,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 191 finished: print at Transform.scala:36, took 0.026705 s
   [druid] 2019-06-11 09:29:30,038 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:30,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 190 is 83 bytes
   [druid] 2019-06-11 09:29:30,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 191 is 160 bytes
   [druid] 2019-06-11 09:29:30,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 192 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:30,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 576 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:30,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 574, ShuffleMapStage 575)
   [druid] 2019-06-11 09:29:30,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:29:30,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 576 (MapPartitionsRDD[673] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:30,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_288 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:30,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_288_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:30,042 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_288_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:30,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 288 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:30,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 576 (MapPartitionsRDD[673] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:29:30,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 576.0 with 1 tasks
   [druid] 2019-06-11 09:29:30,042 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 576.0 (TID 384, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:30,042 [er for task 384] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 576.0 (TID 384)
   [druid] 2019-06-11 09:29:30,043 [er for task 384] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:30,043 [er for task 384] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:30,044 [er for task 384] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:30,044 [er for task 384] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:30,044 [er for task 384] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 576.0 (TID 384). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:29:30,045 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 576.0 (TID 384) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:30,045 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 576.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:30,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 576 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:29:30,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 192 finished: print at Transform.scala:36, took 0.006942 s
   [druid] 2019-06-11 09:29:30,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216570000 ms.0 from job set of time 1560216570000 ms
   [druid] 2019-06-11 09:29:30,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560216570000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:29:30,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 666 from persistence list
   [druid] 2019-06-11 09:29:30,046 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 666
   [druid] 2019-06-11 09:29:30,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 661 from persistence list
   [druid] 2019-06-11 09:29:30,046 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 661
   [druid] 2019-06-11 09:29:30,046 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 660 from persistence list
   [druid] 2019-06-11 09:29:30,046 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 660
   [druid] 2019-06-11 09:29:30,046 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[660] at socketTextStream at Transform.scala:18 of time 1560216570000 ms
   [druid] 2019-06-11 09:29:30,046 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216560000 ms
   [druid] 2019-06-11 09:29:30,047 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216560000 ms
   [druid] 2019-06-11 09:29:35,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216575000 ms
   [druid] 2019-06-11 09:29:35,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216575000 ms.0 from job set of time 1560216575000 ms
   [druid] 2019-06-11 09:29:35,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 675 (map at Transform.scala:20)
   [druid] 2019-06-11 09:29:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:29:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 193 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 579 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 578, ShuffleMapStage 577)
   [druid] 2019-06-11 09:29:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 578)
   [druid] 2019-06-11 09:29:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 578 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:29:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_289 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:35,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_289_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:29:35,010 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_289_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:35,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 289 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 578 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:29:35,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 578.0 with 2 tasks
   [druid] 2019-06-11 09:29:35,011 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 578.0 (TID 385, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:35,011 [er for task 385] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 578.0 (TID 385)
   [druid] 2019-06-11 09:29:35,021 [er for task 385] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 578.0 (TID 385). 853 bytes result sent to driver
   [druid] 2019-06-11 09:29:35,021 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 578.0 (TID 386, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:35,021 [er for task 386] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 578.0 (TID 386)
   [druid] 2019-06-11 09:29:35,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 578.0 (TID 385) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:29:35,029 [er for task 386] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 578.0 (TID 386). 767 bytes result sent to driver
   [druid] 2019-06-11 09:29:35,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 578.0 (TID 386) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:29:35,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 578.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 578 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:29:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:29:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:29:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 579)
   [druid] 2019-06-11 09:29:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:29:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 579 (MapPartitionsRDD[680] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:35,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_290 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:35,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_290_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:35,031 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_290_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:35,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 290 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 579 (MapPartitionsRDD[680] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:29:35,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 579.0 with 1 tasks
   [druid] 2019-06-11 09:29:35,032 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 579.0 (TID 387, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:35,032 [er for task 387] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 579.0 (TID 387)
   [druid] 2019-06-11 09:29:35,033 [er for task 387] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:35,033 [er for task 387] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:35,033 [er for task 387] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:35,033 [er for task 387] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:35,033 [er for task 387] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 579.0 (TID 387). 966 bytes result sent to driver
   [druid] 2019-06-11 09:29:35,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 579.0 (TID 387) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:35,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 579.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 579 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:35,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 193 finished: print at Transform.scala:36, took 0.026274 s
   [druid] 2019-06-11 09:29:35,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:35,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 192 is 83 bytes
   [druid] 2019-06-11 09:29:35,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 193 is 160 bytes
   [druid] 2019-06-11 09:29:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 194 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 582 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 580, ShuffleMapStage 581)
   [druid] 2019-06-11 09:29:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:29:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 582 (MapPartitionsRDD[680] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:35,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_291 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:35,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_291_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:35,039 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_291_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:35,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 291 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 582 (MapPartitionsRDD[680] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:29:35,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 582.0 with 1 tasks
   [druid] 2019-06-11 09:29:35,040 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 582.0 (TID 388, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:35,040 [er for task 388] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 582.0 (TID 388)
   [druid] 2019-06-11 09:29:35,041 [er for task 388] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:35,041 [er for task 388] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:35,041 [er for task 388] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:35,041 [er for task 388] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:35,042 [er for task 388] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 582.0 (TID 388). 966 bytes result sent to driver
   [druid] 2019-06-11 09:29:35,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 582.0 (TID 388) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:35,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 582.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:35,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 582 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:35,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 194 finished: print at Transform.scala:36, took 0.005899 s
   [druid] 2019-06-11 09:29:35,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216575000 ms.0 from job set of time 1560216575000 ms
   [druid] 2019-06-11 09:29:35,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560216575000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:29:35,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 673 from persistence list
   [druid] 2019-06-11 09:29:35,043 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 673
   [druid] 2019-06-11 09:29:35,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 668 from persistence list
   [druid] 2019-06-11 09:29:35,043 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 668
   [druid] 2019-06-11 09:29:35,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 667 from persistence list
   [druid] 2019-06-11 09:29:35,043 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 667
   [druid] 2019-06-11 09:29:35,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[667] at socketTextStream at Transform.scala:18 of time 1560216575000 ms
   [druid] 2019-06-11 09:29:35,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216565000 ms
   [druid] 2019-06-11 09:29:35,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216565000 ms
   [druid] 2019-06-11 09:29:40,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216580000 ms
   [druid] 2019-06-11 09:29:40,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216580000 ms.0 from job set of time 1560216580000 ms
   [druid] 2019-06-11 09:29:40,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:29:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 682 (map at Transform.scala:20)
   [druid] 2019-06-11 09:29:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 195 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 585 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 583, ShuffleMapStage 584)
   [druid] 2019-06-11 09:29:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 583)
   [druid] 2019-06-11 09:29:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 583 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:29:40,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_292 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:40,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_292_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:29:40,009 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_292_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:40,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 292 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 583 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:29:40,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 583.0 with 2 tasks
   [druid] 2019-06-11 09:29:40,010 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 583.0 (TID 389, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:40,010 [er for task 389] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 583.0 (TID 389)
   [druid] 2019-06-11 09:29:40,019 [er for task 389] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 583.0 (TID 389). 853 bytes result sent to driver
   [druid] 2019-06-11 09:29:40,020 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 583.0 (TID 390, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:40,020 [er for task 390] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 583.0 (TID 390)
   [druid] 2019-06-11 09:29:40,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 583.0 (TID 389) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:29:40,027 [er for task 390] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 583.0 (TID 390). 767 bytes result sent to driver
   [druid] 2019-06-11 09:29:40,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 583.0 (TID 390) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:29:40,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 583.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 583 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:29:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:29:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:29:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 585)
   [druid] 2019-06-11 09:29:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:29:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 585 (MapPartitionsRDD[687] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:40,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_293 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:40,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_293_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:29:40,029 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_293_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:40,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 293 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 585 (MapPartitionsRDD[687] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:29:40,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 585.0 with 1 tasks
   [druid] 2019-06-11 09:29:40,030 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 585.0 (TID 391, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:40,030 [er for task 391] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 585.0 (TID 391)
   [druid] 2019-06-11 09:29:40,031 [er for task 391] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:40,031 [er for task 391] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:40,031 [er for task 391] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:40,031 [er for task 391] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:40,031 [er for task 391] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 585.0 (TID 391). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:29:40,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 585.0 (TID 391) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:40,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 585.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 585 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:40,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 195 finished: print at Transform.scala:36, took 0.024939 s
   [druid] 2019-06-11 09:29:40,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:40,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 195 is 160 bytes
   [druid] 2019-06-11 09:29:40,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 194 is 83 bytes
   [druid] 2019-06-11 09:29:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 196 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 588 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 586, ShuffleMapStage 587)
   [druid] 2019-06-11 09:29:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:29:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 588 (MapPartitionsRDD[687] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:40,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_294 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:40,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_294_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:40,037 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_294_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:40,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 294 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:40,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 588 (MapPartitionsRDD[687] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:29:40,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 588.0 with 1 tasks
   [druid] 2019-06-11 09:29:40,038 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 588.0 (TID 392, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:40,038 [er for task 392] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 588.0 (TID 392)
   [druid] 2019-06-11 09:29:40,038 [er for task 392] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:40,038 [er for task 392] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:40,039 [er for task 392] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:40,039 [er for task 392] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:40,039 [er for task 392] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 588.0 (TID 392). 966 bytes result sent to driver
   [druid] 2019-06-11 09:29:40,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 588.0 (TID 392) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:40,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 588.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:40,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 588 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:40,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 196 finished: print at Transform.scala:36, took 0.006335 s
   [druid] 2019-06-11 09:29:40,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216580000 ms.0 from job set of time 1560216580000 ms
   [druid] 2019-06-11 09:29:40,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560216580000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:29:40,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 680 from persistence list
   [druid] 2019-06-11 09:29:40,040 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 680
   [druid] 2019-06-11 09:29:40,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 675 from persistence list
   [druid] 2019-06-11 09:29:40,040 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 675
   [druid] 2019-06-11 09:29:40,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 674 from persistence list
   [druid] 2019-06-11 09:29:40,041 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 674
   [druid] 2019-06-11 09:29:40,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[674] at socketTextStream at Transform.scala:18 of time 1560216580000 ms
   [druid] 2019-06-11 09:29:40,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216570000 ms
   [druid] 2019-06-11 09:29:40,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216570000 ms
   [druid] 2019-06-11 09:29:45,009 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216585000 ms
   [druid] 2019-06-11 09:29:45,010 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216585000 ms.0 from job set of time 1560216585000 ms
   [druid] 2019-06-11 09:29:45,011 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:45,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 689 (map at Transform.scala:20)
   [druid] 2019-06-11 09:29:45,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:29:45,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 197 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:45,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 591 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:45,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 589, ShuffleMapStage 590)
   [druid] 2019-06-11 09:29:45,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 590)
   [druid] 2019-06-11 09:29:45,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 590 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:29:45,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_295 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:45,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_295_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:29:45,013 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_295_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:45,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 295 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:45,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 590 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:29:45,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 590.0 with 2 tasks
   [druid] 2019-06-11 09:29:45,014 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 590.0 (TID 393, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:45,014 [er for task 393] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 590.0 (TID 393)
   [druid] 2019-06-11 09:29:45,027 [er for task 393] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 590.0 (TID 393). 810 bytes result sent to driver
   [druid] 2019-06-11 09:29:45,028 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 590.0 (TID 394, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:45,028 [er for task 394] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 590.0 (TID 394)
   [druid] 2019-06-11 09:29:45,028 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 590.0 (TID 393) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:29:45,036 [er for task 394] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 590.0 (TID 394). 810 bytes result sent to driver
   [druid] 2019-06-11 09:29:45,036 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 590.0 (TID 394) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:29:45,036 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 590.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 590 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:29:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:29:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:29:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 591)
   [druid] 2019-06-11 09:29:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:29:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 591 (MapPartitionsRDD[694] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:45,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_296 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:45,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_296_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:45,037 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_296_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:45,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 296 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 591 (MapPartitionsRDD[694] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:29:45,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 591.0 with 1 tasks
   [druid] 2019-06-11 09:29:45,038 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 591.0 (TID 395, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:45,039 [er for task 395] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 591.0 (TID 395)
   [druid] 2019-06-11 09:29:45,040 [er for task 395] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:45,040 [er for task 395] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:45,040 [er for task 395] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:45,040 [er for task 395] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:45,040 [er for task 395] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 591.0 (TID 395). 966 bytes result sent to driver
   [druid] 2019-06-11 09:29:45,041 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 591.0 (TID 395) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:45,041 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 591.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:45,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 591 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:29:45,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 197 finished: print at Transform.scala:36, took 0.030031 s
   [druid] 2019-06-11 09:29:45,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:45,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 196 is 83 bytes
   [druid] 2019-06-11 09:29:45,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 197 is 160 bytes
   [druid] 2019-06-11 09:29:45,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 198 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:45,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 594 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:45,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 592, ShuffleMapStage 593)
   [druid] 2019-06-11 09:29:45,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:29:45,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 594 (MapPartitionsRDD[694] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:45,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_297 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:45,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_297_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:45,045 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_297_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:45,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 297 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:45,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 594 (MapPartitionsRDD[694] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:29:45,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 594.0 with 1 tasks
   [druid] 2019-06-11 09:29:45,046 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 594.0 (TID 396, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:45,047 [er for task 396] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 594.0 (TID 396)
   [druid] 2019-06-11 09:29:45,048 [er for task 396] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:45,048 [er for task 396] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:45,048 [er for task 396] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:45,048 [er for task 396] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:45,049 [er for task 396] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 594.0 (TID 396). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:29:45,049 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 594.0 (TID 396) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:45,049 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 594.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:45,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 594 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:29:45,050 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 198 finished: print at Transform.scala:36, took 0.006344 s
   [druid] 2019-06-11 09:29:45,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216585000 ms.0 from job set of time 1560216585000 ms
   [druid] 2019-06-11 09:29:45,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.050 s for time 1560216585000 ms (execution: 0.040 s)
   [druid] 2019-06-11 09:29:45,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 687 from persistence list
   [druid] 2019-06-11 09:29:45,050 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 687
   [druid] 2019-06-11 09:29:45,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 682 from persistence list
   [druid] 2019-06-11 09:29:45,050 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 682
   [druid] 2019-06-11 09:29:45,050 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 681 from persistence list
   [druid] 2019-06-11 09:29:45,050 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 681
   [druid] 2019-06-11 09:29:45,050 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[681] at socketTextStream at Transform.scala:18 of time 1560216585000 ms
   [druid] 2019-06-11 09:29:45,051 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216575000 ms
   [druid] 2019-06-11 09:29:45,051 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216575000 ms
   [druid] 2019-06-11 09:29:50,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216590000 ms
   [druid] 2019-06-11 09:29:50,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216590000 ms.0 from job set of time 1560216590000 ms
   [druid] 2019-06-11 09:29:50,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 696 (map at Transform.scala:20)
   [druid] 2019-06-11 09:29:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:29:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 199 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 597 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 596, ShuffleMapStage 595)
   [druid] 2019-06-11 09:29:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 596)
   [druid] 2019-06-11 09:29:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 596 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:29:50,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_298 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:50,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_298_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:29:50,011 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_298_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:50,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 298 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:50,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 596 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:29:50,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 596.0 with 2 tasks
   [druid] 2019-06-11 09:29:50,012 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 596.0 (TID 397, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:50,012 [er for task 397] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 596.0 (TID 397)
   [druid] 2019-06-11 09:29:50,021 [er for task 397] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 596.0 (TID 397). 810 bytes result sent to driver
   [druid] 2019-06-11 09:29:50,021 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 596.0 (TID 398, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:50,021 [er for task 398] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 596.0 (TID 398)
   [druid] 2019-06-11 09:29:50,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 596.0 (TID 397) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:29:50,029 [er for task 398] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 596.0 (TID 398). 810 bytes result sent to driver
   [druid] 2019-06-11 09:29:50,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 596.0 (TID 398) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:29:50,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 596.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 596 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:29:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:29:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:29:50,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 597)
   [druid] 2019-06-11 09:29:50,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:29:50,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 597 (MapPartitionsRDD[701] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:50,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_299 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:50,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_299_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:50,031 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_299_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:50,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 299 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:50,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 597 (MapPartitionsRDD[701] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:29:50,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 597.0 with 1 tasks
   [druid] 2019-06-11 09:29:50,032 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 597.0 (TID 399, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:50,032 [er for task 399] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 597.0 (TID 399)
   [druid] 2019-06-11 09:29:50,033 [er for task 399] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:50,033 [er for task 399] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:50,033 [er for task 399] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:50,033 [er for task 399] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:50,033 [er for task 399] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 597.0 (TID 399). 966 bytes result sent to driver
   [druid] 2019-06-11 09:29:50,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 597.0 (TID 399) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:50,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 597.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 597 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:50,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 199 finished: print at Transform.scala:36, took 0.025814 s
   [druid] 2019-06-11 09:29:50,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:50,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 198 is 83 bytes
   [druid] 2019-06-11 09:29:50,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 199 is 160 bytes
   [druid] 2019-06-11 09:29:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 200 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 600 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 598, ShuffleMapStage 599)
   [druid] 2019-06-11 09:29:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:29:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 600 (MapPartitionsRDD[701] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:50,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_300 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:50,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_300_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:50,040 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_300_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:50,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 300 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:50,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 600 (MapPartitionsRDD[701] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:29:50,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 600.0 with 1 tasks
   [druid] 2019-06-11 09:29:50,041 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 600.0 (TID 400, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:50,041 [er for task 400] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 600.0 (TID 400)
   [druid] 2019-06-11 09:29:50,042 [er for task 400] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:50,042 [er for task 400] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:50,042 [er for task 400] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:50,042 [er for task 400] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:50,043 [er for task 400] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 600.0 (TID 400). 966 bytes result sent to driver
   [druid] 2019-06-11 09:29:50,043 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 600.0 (TID 400) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:50,043 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 600.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:50,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 600 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:50,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 200 finished: print at Transform.scala:36, took 0.006949 s
   [druid] 2019-06-11 09:29:50,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216590000 ms.0 from job set of time 1560216590000 ms
   [druid] 2019-06-11 09:29:50,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216590000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:29:50,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 694 from persistence list
   [druid] 2019-06-11 09:29:50,044 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 694
   [druid] 2019-06-11 09:29:50,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 689 from persistence list
   [druid] 2019-06-11 09:29:50,044 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 689
   [druid] 2019-06-11 09:29:50,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 688 from persistence list
   [druid] 2019-06-11 09:29:50,044 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 688
   [druid] 2019-06-11 09:29:50,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[688] at socketTextStream at Transform.scala:18 of time 1560216590000 ms
   [druid] 2019-06-11 09:29:50,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216580000 ms
   [druid] 2019-06-11 09:29:50,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216580000 ms
   [druid] 2019-06-11 09:29:55,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216595000 ms
   [druid] 2019-06-11 09:29:55,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216595000 ms.0 from job set of time 1560216595000 ms
   [druid] 2019-06-11 09:29:55,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 703 (map at Transform.scala:20)
   [druid] 2019-06-11 09:29:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:29:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 201 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 603 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 601, ShuffleMapStage 602)
   [druid] 2019-06-11 09:29:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 602)
   [druid] 2019-06-11 09:29:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 602 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:29:55,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_301 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:55,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_301_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:29:55,011 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_301_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:55,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 301 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:55,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 602 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:29:55,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 602.0 with 2 tasks
   [druid] 2019-06-11 09:29:55,011 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 602.0 (TID 401, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:55,011 [er for task 401] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 602.0 (TID 401)
   [druid] 2019-06-11 09:29:55,020 [er for task 401] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 602.0 (TID 401). 853 bytes result sent to driver
   [druid] 2019-06-11 09:29:55,020 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 602.0 (TID 402, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:29:55,021 [er for task 402] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 602.0 (TID 402)
   [druid] 2019-06-11 09:29:55,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 602.0 (TID 401) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:29:55,029 [er for task 402] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 602.0 (TID 402). 767 bytes result sent to driver
   [druid] 2019-06-11 09:29:55,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 602.0 (TID 402) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:29:55,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 602.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 602 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:29:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:29:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:29:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 603)
   [druid] 2019-06-11 09:29:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:29:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 603 (MapPartitionsRDD[708] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:55,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_302 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:55,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_302_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:55,032 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_302_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:55,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 302 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:55,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 603 (MapPartitionsRDD[708] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:29:55,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 603.0 with 1 tasks
   [druid] 2019-06-11 09:29:55,034 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 603.0 (TID 403, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:55,034 [er for task 403] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 603.0 (TID 403)
   [druid] 2019-06-11 09:29:55,035 [er for task 403] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:55,035 [er for task 403] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:55,035 [er for task 403] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:55,035 [er for task 403] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:55,035 [er for task 403] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 603.0 (TID 403). 966 bytes result sent to driver
   [druid] 2019-06-11 09:29:55,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 603.0 (TID 403) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:55,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 603.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 603 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:55,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 201 finished: print at Transform.scala:36, took 0.028132 s
   [druid] 2019-06-11 09:29:55,038 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:29:55,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 200 is 83 bytes
   [druid] 2019-06-11 09:29:55,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 201 is 160 bytes
   [druid] 2019-06-11 09:29:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 202 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:29:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 606 (print at Transform.scala:36)
   [druid] 2019-06-11 09:29:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 604, ShuffleMapStage 605)
   [druid] 2019-06-11 09:29:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:29:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 606 (MapPartitionsRDD[708] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:29:55,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_303 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:55,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_303_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:29:55,041 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_303_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:29:55,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 303 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:29:55,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 606 (MapPartitionsRDD[708] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:29:55,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 606.0 with 1 tasks
   [druid] 2019-06-11 09:29:55,042 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 606.0 (TID 404, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:29:55,042 [er for task 404] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 606.0 (TID 404)
   [druid] 2019-06-11 09:29:55,043 [er for task 404] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:29:55,043 [er for task 404] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:55,043 [er for task 404] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:29:55,043 [er for task 404] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:29:55,043 [er for task 404] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 606.0 (TID 404). 966 bytes result sent to driver
   [druid] 2019-06-11 09:29:55,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 606.0 (TID 404) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:29:55,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 606.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:29:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 606 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:29:55,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 202 finished: print at Transform.scala:36, took 0.005551 s
   [druid] 2019-06-11 09:29:55,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216595000 ms.0 from job set of time 1560216595000 ms
   [druid] 2019-06-11 09:29:55,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.044 s for time 1560216595000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:29:55,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 701 from persistence list
   [druid] 2019-06-11 09:29:55,044 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 701
   [druid] 2019-06-11 09:29:55,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 696 from persistence list
   [druid] 2019-06-11 09:29:55,045 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 696
   [druid] 2019-06-11 09:29:55,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 695 from persistence list
   [druid] 2019-06-11 09:29:55,045 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 695
   [druid] 2019-06-11 09:29:55,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[695] at socketTextStream at Transform.scala:18 of time 1560216595000 ms
   [druid] 2019-06-11 09:29:55,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216585000 ms
   [druid] 2019-06-11 09:29:55,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216585000 ms
   [druid] 2019-06-11 09:30:00,009 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216600000 ms
   [druid] 2019-06-11 09:30:00,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216600000 ms.0 from job set of time 1560216600000 ms
   [druid] 2019-06-11 09:30:00,012 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 710 (map at Transform.scala:20)
   [druid] 2019-06-11 09:30:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:30:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 203 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 609 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 607, ShuffleMapStage 608)
   [druid] 2019-06-11 09:30:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 608)
   [druid] 2019-06-11 09:30:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 608 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:30:00,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_304 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:00,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_304_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:30:00,019 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_304_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,021 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 304 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:00,023 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_302_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,024 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 608 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:30:00,024 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 608.0 with 2 tasks
   [druid] 2019-06-11 09:30:00,025 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 608.0 (TID 405, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:00,025 [er for task 405] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 608.0 (TID 405)
   [druid] 2019-06-11 09:30:00,026 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_287_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,027 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_295_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,027 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 198
   [druid] 2019-06-11 09:30:00,027 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 196
   [druid] 2019-06-11 09:30:00,028 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 189
   [druid] 2019-06-11 09:30:00,029 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_296_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,029 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 199
   [druid] 2019-06-11 09:30:00,029 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 193
   [druid] 2019-06-11 09:30:00,030 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_301_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,033 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_290_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,034 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_299_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,035 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_289_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,035 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 197
   [druid] 2019-06-11 09:30:00,035 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 186
   [druid] 2019-06-11 09:30:00,035 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 190
   [druid] 2019-06-11 09:30:00,036 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_288_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,036 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 192
   [druid] 2019-06-11 09:30:00,036 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 188
   [druid] 2019-06-11 09:30:00,037 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_294_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,038 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_300_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,038 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 195
   [druid] 2019-06-11 09:30:00,039 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_293_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,040 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_298_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,041 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_285_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,041 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 194
   [druid] 2019-06-11 09:30:00,041 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_291_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,042 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_292_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,043 [er for task 405] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 608.0 (TID 405). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:00,043 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 608.0 (TID 406, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:00,043 [er for task 406] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 608.0 (TID 406)
   [druid] 2019-06-11 09:30:00,043 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 608.0 (TID 405) in 18 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:30:00,043 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_297_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 191
   [druid] 2019-06-11 09:30:00,044 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_286_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 187
   [druid] 2019-06-11 09:30:00,045 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_303_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,056 [er for task 406] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 608.0 (TID 406). 853 bytes result sent to driver
   [druid] 2019-06-11 09:30:00,056 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 608.0 (TID 406) in 13 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:30:00,056 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 608.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 608 (parallelize at Transform.scala:16) finished in 0.031 s
   [druid] 2019-06-11 09:30:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:30:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:30:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 609)
   [druid] 2019-06-11 09:30:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:30:00,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 609 (MapPartitionsRDD[715] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:00,057 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_305 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:00,058 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_305_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:00,059 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_305_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,059 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 305 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:00,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 609 (MapPartitionsRDD[715] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:30:00,059 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 609.0 with 1 tasks
   [druid] 2019-06-11 09:30:00,060 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 609.0 (TID 407, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:00,060 [er for task 407] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 609.0 (TID 407)
   [druid] 2019-06-11 09:30:00,061 [er for task 407] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:00,061 [er for task 407] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:00,061 [er for task 407] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:00,061 [er for task 407] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:00,062 [er for task 407] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 609.0 (TID 407). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:00,062 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 609.0 (TID 407) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:00,062 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 609.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:00,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 609 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:30:00,062 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 203 finished: print at Transform.scala:36, took 0.050554 s
   [druid] 2019-06-11 09:30:00,064 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:00,064 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 202 is 83 bytes
   [druid] 2019-06-11 09:30:00,065 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 203 is 160 bytes
   [druid] 2019-06-11 09:30:00,065 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 204 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:00,065 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 612 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:00,065 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 610, ShuffleMapStage 611)
   [druid] 2019-06-11 09:30:00,065 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:30:00,065 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 612 (MapPartitionsRDD[715] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:00,065 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_306 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:00,067 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_306_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:00,067 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_306_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:00,067 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 306 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:00,067 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 612 (MapPartitionsRDD[715] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:30:00,067 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 612.0 with 1 tasks
   [druid] 2019-06-11 09:30:00,067 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 612.0 (TID 408, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:00,068 [er for task 408] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 612.0 (TID 408)
   [druid] 2019-06-11 09:30:00,069 [er for task 408] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:00,069 [er for task 408] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:00,069 [er for task 408] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:00,069 [er for task 408] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:00,070 [er for task 408] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 612.0 (TID 408). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:00,070 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 612.0 (TID 408) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:00,070 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 612.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:00,070 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 612 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:30:00,070 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 204 finished: print at Transform.scala:36, took 0.006119 s
   [druid] 2019-06-11 09:30:00,071 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216600000 ms.0 from job set of time 1560216600000 ms
   [druid] 2019-06-11 09:30:00,071 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.071 s for time 1560216600000 ms (execution: 0.062 s)
   [druid] 2019-06-11 09:30:00,071 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 708 from persistence list
   [druid] 2019-06-11 09:30:00,071 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 708
   [druid] 2019-06-11 09:30:00,071 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 703 from persistence list
   [druid] 2019-06-11 09:30:00,071 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 703
   [druid] 2019-06-11 09:30:00,071 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 702 from persistence list
   [druid] 2019-06-11 09:30:00,071 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 702
   [druid] 2019-06-11 09:30:00,071 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[702] at socketTextStream at Transform.scala:18 of time 1560216600000 ms
   [druid] 2019-06-11 09:30:00,071 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216590000 ms
   [druid] 2019-06-11 09:30:00,071 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216590000 ms
   [druid] 2019-06-11 09:30:05,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216605000 ms
   [druid] 2019-06-11 09:30:05,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216605000 ms.0 from job set of time 1560216605000 ms
   [druid] 2019-06-11 09:30:05,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 717 (map at Transform.scala:20)
   [druid] 2019-06-11 09:30:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:30:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 205 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 615 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 614, ShuffleMapStage 613)
   [druid] 2019-06-11 09:30:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 614)
   [druid] 2019-06-11 09:30:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 614 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:30:05,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_307 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:05,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_307_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:30:05,012 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_307_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:05,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 307 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:05,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 614 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:30:05,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 614.0 with 2 tasks
   [druid] 2019-06-11 09:30:05,013 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 614.0 (TID 409, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:05,013 [er for task 409] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 614.0 (TID 409)
   [druid] 2019-06-11 09:30:05,026 [er for task 409] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 614.0 (TID 409). 767 bytes result sent to driver
   [druid] 2019-06-11 09:30:05,026 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 614.0 (TID 410, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:05,026 [er for task 410] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 614.0 (TID 410)
   [druid] 2019-06-11 09:30:05,027 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 614.0 (TID 409) in 13 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:30:05,034 [er for task 410] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 614.0 (TID 410). 853 bytes result sent to driver
   [druid] 2019-06-11 09:30:05,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 614.0 (TID 410) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:30:05,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 614.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 614 (parallelize at Transform.scala:16) finished in 0.022 s
   [druid] 2019-06-11 09:30:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:30:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:30:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 615)
   [druid] 2019-06-11 09:30:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:30:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 615 (MapPartitionsRDD[722] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:05,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_308 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:05,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_308_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:05,036 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_308_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:05,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 308 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 615 (MapPartitionsRDD[722] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:30:05,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 615.0 with 1 tasks
   [druid] 2019-06-11 09:30:05,037 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 615.0 (TID 411, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:05,037 [er for task 411] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 615.0 (TID 411)
   [druid] 2019-06-11 09:30:05,038 [er for task 411] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:05,038 [er for task 411] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:05,039 [er for task 411] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:05,039 [er for task 411] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:05,040 [er for task 411] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 615.0 (TID 411). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:30:05,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 615.0 (TID 411) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:05,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 615.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:05,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 615 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:30:05,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 205 finished: print at Transform.scala:36, took 0.031596 s
   [druid] 2019-06-11 09:30:05,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:05,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 204 is 83 bytes
   [druid] 2019-06-11 09:30:05,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 205 is 160 bytes
   [druid] 2019-06-11 09:30:05,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 206 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:05,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 618 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:05,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 616, ShuffleMapStage 617)
   [druid] 2019-06-11 09:30:05,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:30:05,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 618 (MapPartitionsRDD[722] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:05,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_309 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:05,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_309_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:05,046 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_309_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:05,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 309 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:05,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 618 (MapPartitionsRDD[722] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:30:05,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 618.0 with 1 tasks
   [druid] 2019-06-11 09:30:05,046 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 618.0 (TID 412, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:05,047 [er for task 412] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 618.0 (TID 412)
   [druid] 2019-06-11 09:30:05,048 [er for task 412] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:05,048 [er for task 412] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:05,048 [er for task 412] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:05,048 [er for task 412] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:05,049 [er for task 412] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 618.0 (TID 412). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:05,049 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 618.0 (TID 412) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:05,049 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 618.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:05,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 618 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:30:05,050 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 206 finished: print at Transform.scala:36, took 0.006874 s
   [druid] 2019-06-11 09:30:05,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216605000 ms.0 from job set of time 1560216605000 ms
   [druid] 2019-06-11 09:30:05,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.050 s for time 1560216605000 ms (execution: 0.043 s)
   [druid] 2019-06-11 09:30:05,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 715 from persistence list
   [druid] 2019-06-11 09:30:05,050 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 715
   [druid] 2019-06-11 09:30:05,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 710 from persistence list
   [druid] 2019-06-11 09:30:05,050 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 710
   [druid] 2019-06-11 09:30:05,051 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 709 from persistence list
   [druid] 2019-06-11 09:30:05,051 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 709
   [druid] 2019-06-11 09:30:05,051 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[709] at socketTextStream at Transform.scala:18 of time 1560216605000 ms
   [druid] 2019-06-11 09:30:05,051 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216595000 ms
   [druid] 2019-06-11 09:30:05,051 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216595000 ms
   [druid] 2019-06-11 09:30:10,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216610000 ms
   [druid] 2019-06-11 09:30:10,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216610000 ms.0 from job set of time 1560216610000 ms
   [druid] 2019-06-11 09:30:10,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 724 (map at Transform.scala:20)
   [druid] 2019-06-11 09:30:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:30:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 207 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 621 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 619, ShuffleMapStage 620)
   [druid] 2019-06-11 09:30:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 620)
   [druid] 2019-06-11 09:30:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 620 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:30:10,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_310 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:10,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_310_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:30:10,011 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_310_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:10,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 310 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 620 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:30:10,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 620.0 with 2 tasks
   [druid] 2019-06-11 09:30:10,012 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 620.0 (TID 413, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:10,012 [er for task 413] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 620.0 (TID 413)
   [druid] 2019-06-11 09:30:10,021 [er for task 413] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 620.0 (TID 413). 853 bytes result sent to driver
   [druid] 2019-06-11 09:30:10,021 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 620.0 (TID 414, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:10,021 [er for task 414] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 620.0 (TID 414)
   [druid] 2019-06-11 09:30:10,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 620.0 (TID 413) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:30:10,030 [er for task 414] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 620.0 (TID 414). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:10,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 620.0 (TID 414) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:30:10,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 620.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 620 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:30:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:30:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:30:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 621)
   [druid] 2019-06-11 09:30:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:30:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 621 (MapPartitionsRDD[729] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:10,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_311 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:10,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_311_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:10,032 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_311_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:10,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 311 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 621 (MapPartitionsRDD[729] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:30:10,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 621.0 with 1 tasks
   [druid] 2019-06-11 09:30:10,033 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 621.0 (TID 415, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:10,033 [er for task 415] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 621.0 (TID 415)
   [druid] 2019-06-11 09:30:10,034 [er for task 415] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:10,034 [er for task 415] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:10,034 [er for task 415] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:10,034 [er for task 415] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:10,035 [er for task 415] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 621.0 (TID 415). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:10,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 621.0 (TID 415) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:10,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 621.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:10,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 621 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:30:10,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 207 finished: print at Transform.scala:36, took 0.026472 s
   [druid] 2019-06-11 09:30:10,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:10,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 206 is 83 bytes
   [druid] 2019-06-11 09:30:10,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 207 is 160 bytes
   [druid] 2019-06-11 09:30:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 208 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 624 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 622, ShuffleMapStage 623)
   [druid] 2019-06-11 09:30:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:30:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 624 (MapPartitionsRDD[729] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:10,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_312 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:10,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_312_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:10,042 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_312_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:10,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 312 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 624 (MapPartitionsRDD[729] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:30:10,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 624.0 with 1 tasks
   [druid] 2019-06-11 09:30:10,043 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 624.0 (TID 416, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:10,043 [er for task 416] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 624.0 (TID 416)
   [druid] 2019-06-11 09:30:10,043 [er for task 416] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:10,043 [er for task 416] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:10,044 [er for task 416] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:10,044 [er for task 416] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:10,044 [er for task 416] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 624.0 (TID 416). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:10,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 624.0 (TID 416) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:10,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 624.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:10,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 624 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:30:10,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 208 finished: print at Transform.scala:36, took 0.005704 s
   [druid] 2019-06-11 09:30:10,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216610000 ms.0 from job set of time 1560216610000 ms
   [druid] 2019-06-11 09:30:10,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560216610000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:30:10,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 722 from persistence list
   [druid] 2019-06-11 09:30:10,045 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 722
   [druid] 2019-06-11 09:30:10,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 717 from persistence list
   [druid] 2019-06-11 09:30:10,046 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 717
   [druid] 2019-06-11 09:30:10,046 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 716 from persistence list
   [druid] 2019-06-11 09:30:10,046 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 716
   [druid] 2019-06-11 09:30:10,046 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[716] at socketTextStream at Transform.scala:18 of time 1560216610000 ms
   [druid] 2019-06-11 09:30:10,046 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216600000 ms
   [druid] 2019-06-11 09:30:10,046 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216600000 ms
   [druid] 2019-06-11 09:30:15,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216615000 ms
   [druid] 2019-06-11 09:30:15,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216615000 ms.0 from job set of time 1560216615000 ms
   [druid] 2019-06-11 09:30:15,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:30:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 731 (map at Transform.scala:20)
   [druid] 2019-06-11 09:30:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 209 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 627 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 625, ShuffleMapStage 626)
   [druid] 2019-06-11 09:30:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 625)
   [druid] 2019-06-11 09:30:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 625 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:30:15,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_313 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:15,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_313_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:30:15,012 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_313_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:15,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 313 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 625 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:30:15,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 625.0 with 2 tasks
   [druid] 2019-06-11 09:30:15,013 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 625.0 (TID 417, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:15,013 [er for task 417] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 625.0 (TID 417)
   [druid] 2019-06-11 09:30:15,022 [er for task 417] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 625.0 (TID 417). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:15,023 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 625.0 (TID 418, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:15,023 [er for task 418] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 625.0 (TID 418)
   [druid] 2019-06-11 09:30:15,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 625.0 (TID 417) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:30:15,037 [er for task 418] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 625.0 (TID 418). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:15,037 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 625.0 (TID 418) in 14 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:30:15,037 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 625.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 625 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:30:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:30:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:30:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 627)
   [druid] 2019-06-11 09:30:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:30:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 627 (MapPartitionsRDD[736] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:15,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_314 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:15,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_314_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:15,039 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_314_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:15,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 314 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 627 (MapPartitionsRDD[736] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:30:15,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 627.0 with 1 tasks
   [druid] 2019-06-11 09:30:15,040 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 627.0 (TID 419, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:15,040 [er for task 419] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 627.0 (TID 419)
   [druid] 2019-06-11 09:30:15,041 [er for task 419] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:15,041 [er for task 419] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:15,041 [er for task 419] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:15,041 [er for task 419] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:15,041 [er for task 419] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 627.0 (TID 419). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:15,042 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 627.0 (TID 419) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:15,042 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 627.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 627 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:30:15,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 209 finished: print at Transform.scala:36, took 0.032319 s
   [druid] 2019-06-11 09:30:15,045 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:15,045 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 209 is 160 bytes
   [druid] 2019-06-11 09:30:15,045 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 208 is 83 bytes
   [druid] 2019-06-11 09:30:15,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 210 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:15,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 630 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:15,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 628, ShuffleMapStage 629)
   [druid] 2019-06-11 09:30:15,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:30:15,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 630 (MapPartitionsRDD[736] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:15,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_315 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:15,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_315_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:15,048 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_315_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:15,048 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 315 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:15,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 630 (MapPartitionsRDD[736] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:30:15,048 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 630.0 with 1 tasks
   [druid] 2019-06-11 09:30:15,049 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 630.0 (TID 420, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:15,049 [er for task 420] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 630.0 (TID 420)
   [druid] 2019-06-11 09:30:15,049 [er for task 420] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:15,049 [er for task 420] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:15,050 [er for task 420] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:15,050 [er for task 420] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:15,050 [er for task 420] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 630.0 (TID 420). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:15,051 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 630.0 (TID 420) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:15,051 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 630.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:15,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 630 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:30:15,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 210 finished: print at Transform.scala:36, took 0.006292 s
   [druid] 2019-06-11 09:30:15,051 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216615000 ms.0 from job set of time 1560216615000 ms
   [druid] 2019-06-11 09:30:15,051 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.051 s for time 1560216615000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:30:15,051 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 729 from persistence list
   [druid] 2019-06-11 09:30:15,051 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 729
   [druid] 2019-06-11 09:30:15,051 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 724 from persistence list
   [druid] 2019-06-11 09:30:15,052 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 724
   [druid] 2019-06-11 09:30:15,052 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 723 from persistence list
   [druid] 2019-06-11 09:30:15,052 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 723
   [druid] 2019-06-11 09:30:15,052 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[723] at socketTextStream at Transform.scala:18 of time 1560216615000 ms
   [druid] 2019-06-11 09:30:15,052 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216605000 ms
   [druid] 2019-06-11 09:30:15,052 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216605000 ms
   [druid] 2019-06-11 09:30:20,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216620000 ms
   [druid] 2019-06-11 09:30:20,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216620000 ms.0 from job set of time 1560216620000 ms
   [druid] 2019-06-11 09:30:20,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 738 (map at Transform.scala:20)
   [druid] 2019-06-11 09:30:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:30:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 211 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 633 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 631, ShuffleMapStage 632)
   [druid] 2019-06-11 09:30:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 632)
   [druid] 2019-06-11 09:30:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 632 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:30:20,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_316 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:20,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_316_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:30:20,011 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_316_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:20,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 316 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 632 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:30:20,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 632.0 with 2 tasks
   [druid] 2019-06-11 09:30:20,012 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 632.0 (TID 421, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:20,012 [er for task 421] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 632.0 (TID 421)
   [druid] 2019-06-11 09:30:20,021 [er for task 421] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 632.0 (TID 421). 853 bytes result sent to driver
   [druid] 2019-06-11 09:30:20,022 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 632.0 (TID 422, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:20,022 [er for task 422] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 632.0 (TID 422)
   [druid] 2019-06-11 09:30:20,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 632.0 (TID 421) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:30:20,030 [er for task 422] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 632.0 (TID 422). 767 bytes result sent to driver
   [druid] 2019-06-11 09:30:20,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 632.0 (TID 422) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:30:20,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 632.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 632 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:30:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:30:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:30:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 633)
   [druid] 2019-06-11 09:30:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:30:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 633 (MapPartitionsRDD[743] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:20,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_317 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:20,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_317_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:20,032 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_317_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:20,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 317 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 633 (MapPartitionsRDD[743] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:30:20,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 633.0 with 1 tasks
   [druid] 2019-06-11 09:30:20,032 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 633.0 (TID 423, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:20,032 [er for task 423] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 633.0 (TID 423)
   [druid] 2019-06-11 09:30:20,033 [er for task 423] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:20,033 [er for task 423] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:20,033 [er for task 423] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:20,033 [er for task 423] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:20,034 [er for task 423] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 633.0 (TID 423). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:30:20,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 633.0 (TID 423) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:20,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 633.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:20,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 633 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:30:20,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 211 finished: print at Transform.scala:36, took 0.025967 s
   [druid] 2019-06-11 09:30:20,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:20,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 210 is 83 bytes
   [druid] 2019-06-11 09:30:20,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 211 is 160 bytes
   [druid] 2019-06-11 09:30:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 212 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 636 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 634, ShuffleMapStage 635)
   [druid] 2019-06-11 09:30:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:30:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 636 (MapPartitionsRDD[743] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:20,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_318 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:20,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_318_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:20,039 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_318_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:20,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 318 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 636 (MapPartitionsRDD[743] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:30:20,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 636.0 with 1 tasks
   [druid] 2019-06-11 09:30:20,040 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 636.0 (TID 424, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:20,040 [er for task 424] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 636.0 (TID 424)
   [druid] 2019-06-11 09:30:20,041 [er for task 424] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:20,041 [er for task 424] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:20,041 [er for task 424] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:20,041 [er for task 424] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:20,043 [er for task 424] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 636.0 (TID 424). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:30:20,043 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 636.0 (TID 424) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:20,043 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 636.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:20,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 636 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:30:20,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 212 finished: print at Transform.scala:36, took 0.006508 s
   [druid] 2019-06-11 09:30:20,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216620000 ms.0 from job set of time 1560216620000 ms
   [druid] 2019-06-11 09:30:20,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216620000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:30:20,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 736 from persistence list
   [druid] 2019-06-11 09:30:20,043 [-thread-pool-20] INFO  che.spark.storage.BlockManager {1} - Removing RDD 736
   [druid] 2019-06-11 09:30:20,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 731 from persistence list
   [druid] 2019-06-11 09:30:20,044 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 731
   [druid] 2019-06-11 09:30:20,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 730 from persistence list
   [druid] 2019-06-11 09:30:20,044 [-thread-pool-20] INFO  che.spark.storage.BlockManager {1} - Removing RDD 730
   [druid] 2019-06-11 09:30:20,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[730] at socketTextStream at Transform.scala:18 of time 1560216620000 ms
   [druid] 2019-06-11 09:30:20,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216610000 ms
   [druid] 2019-06-11 09:30:20,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216610000 ms
   [druid] 2019-06-11 09:30:25,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216625000 ms
   [druid] 2019-06-11 09:30:25,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216625000 ms.0 from job set of time 1560216625000 ms
   [druid] 2019-06-11 09:30:25,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:30:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 745 (map at Transform.scala:20)
   [druid] 2019-06-11 09:30:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 213 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 639 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 637, ShuffleMapStage 638)
   [druid] 2019-06-11 09:30:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 637)
   [druid] 2019-06-11 09:30:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 637 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:30:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_319 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_319_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:30:25,010 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_319_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:25,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 319 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 637 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:30:25,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 637.0 with 2 tasks
   [druid] 2019-06-11 09:30:25,011 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 637.0 (TID 425, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:25,011 [er for task 425] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 637.0 (TID 425)
   [druid] 2019-06-11 09:30:25,019 [er for task 425] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 637.0 (TID 425). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:25,020 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 637.0 (TID 426, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:25,020 [er for task 426] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 637.0 (TID 426)
   [druid] 2019-06-11 09:30:25,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 637.0 (TID 425) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:30:25,027 [er for task 426] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 637.0 (TID 426). 767 bytes result sent to driver
   [druid] 2019-06-11 09:30:25,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 637.0 (TID 426) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:30:25,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 637.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 637 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:30:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:30:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:30:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 639)
   [druid] 2019-06-11 09:30:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:30:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 639 (MapPartitionsRDD[750] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:25,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_320 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:25,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_320_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:25,030 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_320_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:25,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 320 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 639 (MapPartitionsRDD[750] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:30:25,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 639.0 with 1 tasks
   [druid] 2019-06-11 09:30:25,031 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 639.0 (TID 427, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:25,031 [er for task 427] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 639.0 (TID 427)
   [druid] 2019-06-11 09:30:25,031 [er for task 427] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:25,031 [er for task 427] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:25,031 [er for task 427] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:25,032 [er for task 427] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:30:25,032 [er for task 427] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 639.0 (TID 427). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:25,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 639.0 (TID 427) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:25,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 639.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:25,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 639 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:30:25,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 213 finished: print at Transform.scala:36, took 0.024693 s
   [druid] 2019-06-11 09:30:25,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:25,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 213 is 160 bytes
   [druid] 2019-06-11 09:30:25,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 212 is 83 bytes
   [druid] 2019-06-11 09:30:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 214 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 642 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 640, ShuffleMapStage 641)
   [druid] 2019-06-11 09:30:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:30:25,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 642 (MapPartitionsRDD[750] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:25,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_321 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:25,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_321_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:25,037 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_321_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:25,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 321 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 642 (MapPartitionsRDD[750] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:30:25,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 642.0 with 1 tasks
   [druid] 2019-06-11 09:30:25,038 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 642.0 (TID 428, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:25,038 [er for task 428] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 642.0 (TID 428)
   [druid] 2019-06-11 09:30:25,039 [er for task 428] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:25,039 [er for task 428] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:25,039 [er for task 428] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:25,039 [er for task 428] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:25,040 [er for task 428] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 642.0 (TID 428). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:30:25,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 642.0 (TID 428) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:25,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 642.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 642 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:30:25,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 214 finished: print at Transform.scala:36, took 0.005711 s
   [druid] 2019-06-11 09:30:25,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216625000 ms.0 from job set of time 1560216625000 ms
   [druid] 2019-06-11 09:30:25,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560216625000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:30:25,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 743 from persistence list
   [druid] 2019-06-11 09:30:25,041 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 743
   [druid] 2019-06-11 09:30:25,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 738 from persistence list
   [druid] 2019-06-11 09:30:25,041 [-thread-pool-20] INFO  che.spark.storage.BlockManager {1} - Removing RDD 738
   [druid] 2019-06-11 09:30:25,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 737 from persistence list
   [druid] 2019-06-11 09:30:25,041 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 737
   [druid] 2019-06-11 09:30:25,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[737] at socketTextStream at Transform.scala:18 of time 1560216625000 ms
   [druid] 2019-06-11 09:30:25,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216615000 ms
   [druid] 2019-06-11 09:30:25,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216615000 ms
   [druid] 2019-06-11 09:30:30,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216630000 ms
   [druid] 2019-06-11 09:30:30,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216630000 ms.0 from job set of time 1560216630000 ms
   [druid] 2019-06-11 09:30:30,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:30:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 752 (map at Transform.scala:20)
   [druid] 2019-06-11 09:30:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 215 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 645 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 643, ShuffleMapStage 644)
   [druid] 2019-06-11 09:30:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 643)
   [druid] 2019-06-11 09:30:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 643 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:30:30,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_322 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:30,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_322_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:30:30,009 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_322_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 322 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 643 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:30:30,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 643.0 with 2 tasks
   [druid] 2019-06-11 09:30:30,010 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 643.0 (TID 429, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:30,010 [er for task 429] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 643.0 (TID 429)
   [druid] 2019-06-11 09:30:30,023 [er for task 429] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 643.0 (TID 429). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:30,024 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 643.0 (TID 430, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:30,024 [er for task 430] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 643.0 (TID 430)
   [druid] 2019-06-11 09:30:30,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 643.0 (TID 429) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:30:30,032 [er for task 430] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 643.0 (TID 430). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:30,032 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 643.0 (TID 430) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:30:30,032 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 643.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 643 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:30:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:30:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:30:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 645)
   [druid] 2019-06-11 09:30:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:30:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 645 (MapPartitionsRDD[757] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:30,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_323 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:30,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_323_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:30,040 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_323_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,042 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_313_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 323 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:30,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 645 (MapPartitionsRDD[757] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:30:30,045 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 645.0 with 1 tasks
   [druid] 2019-06-11 09:30:30,045 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 645.0 (TID 431, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:30,045 [er for task 431] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 645.0 (TID 431)
   [druid] 2019-06-11 09:30:30,045 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_308_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,045 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 204
   [druid] 2019-06-11 09:30:30,046 [er for task 431] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:30,046 [er for task 431] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:30,046 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_312_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,046 [er for task 431] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:30,046 [er for task 431] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:30,046 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 206
   [druid] 2019-06-11 09:30:30,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 205
   [druid] 2019-06-11 09:30:30,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 200
   [druid] 2019-06-11 09:30:30,048 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_318_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,048 [er for task 431] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 645.0 (TID 431). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:30:30,048 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 645.0 (TID 431) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:30,049 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 645.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:30,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 645 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:30:30,049 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_307_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 215 finished: print at Transform.scala:36, took 0.041894 s
   [druid] 2019-06-11 09:30:30,050 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_315_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,050 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_316_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,051 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_317_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 203
   [druid] 2019-06-11 09:30:30,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 202
   [druid] 2019-06-11 09:30:30,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 209
   [druid] 2019-06-11 09:30:30,054 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_309_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 208
   [druid] 2019-06-11 09:30:30,056 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 207
   [druid] 2019-06-11 09:30:30,056 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_306_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,057 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:30,057 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_320_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,057 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 215 is 160 bytes
   [druid] 2019-06-11 09:30:30,058 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 214 is 83 bytes
   [druid] 2019-06-11 09:30:30,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 216 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:30,058 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_314_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 648 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:30,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 647, ShuffleMapStage 646)
   [druid] 2019-06-11 09:30:30,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:30:30,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 648 (MapPartitionsRDD[757] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:30,059 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_324 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:30,059 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_321_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,059 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_310_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,060 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_324_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:30,060 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 210
   [druid] 2019-06-11 09:30:30,061 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_324_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,061 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 211
   [druid] 2019-06-11 09:30:30,061 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 324 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:30,061 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 648 (MapPartitionsRDD[757] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:30:30,061 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 648.0 with 1 tasks
   [druid] 2019-06-11 09:30:30,062 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_311_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,062 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 648.0 (TID 432, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:30,062 [er for task 432] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 648.0 (TID 432)
   [druid] 2019-06-11 09:30:30,062 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_304_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,063 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_305_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,063 [er for task 432] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:30,063 [er for task 432] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:30,063 [er for task 432] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:30,063 [er for task 432] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:30,063 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 201
   [druid] 2019-06-11 09:30:30,064 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_319_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:30,064 [er for task 432] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 648.0 (TID 432). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:30:30,064 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 648.0 (TID 432) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:30,065 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 648.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:30,065 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 648 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:30:30,065 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 216 finished: print at Transform.scala:36, took 0.007636 s
   [druid] 2019-06-11 09:30:30,065 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216630000 ms.0 from job set of time 1560216630000 ms
   [druid] 2019-06-11 09:30:30,065 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.065 s for time 1560216630000 ms (execution: 0.059 s)
   [druid] 2019-06-11 09:30:30,065 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 750 from persistence list
   [druid] 2019-06-11 09:30:30,065 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 750
   [druid] 2019-06-11 09:30:30,065 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 745 from persistence list
   [druid] 2019-06-11 09:30:30,065 [-thread-pool-20] INFO  che.spark.storage.BlockManager {1} - Removing RDD 745
   [druid] 2019-06-11 09:30:30,066 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 744 from persistence list
   [druid] 2019-06-11 09:30:30,066 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 744
   [druid] 2019-06-11 09:30:30,066 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[744] at socketTextStream at Transform.scala:18 of time 1560216630000 ms
   [druid] 2019-06-11 09:30:30,066 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216620000 ms
   [druid] 2019-06-11 09:30:30,066 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216620000 ms
   [druid] 2019-06-11 09:30:35,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216635000 ms
   [druid] 2019-06-11 09:30:35,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216635000 ms.0 from job set of time 1560216635000 ms
   [druid] 2019-06-11 09:30:35,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 759 (map at Transform.scala:20)
   [druid] 2019-06-11 09:30:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:30:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 217 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 651 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 649, ShuffleMapStage 650)
   [druid] 2019-06-11 09:30:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 650)
   [druid] 2019-06-11 09:30:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 650 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:30:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_325 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:35,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_325_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:30:35,010 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_325_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:35,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 325 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 650 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:30:35,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 650.0 with 2 tasks
   [druid] 2019-06-11 09:30:35,011 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 650.0 (TID 433, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:35,011 [er for task 433] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 650.0 (TID 433)
   [druid] 2019-06-11 09:30:35,026 [er for task 433] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 650.0 (TID 433). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:35,026 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 650.0 (TID 434, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:35,026 [er for task 434] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 650.0 (TID 434)
   [druid] 2019-06-11 09:30:35,026 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 650.0 (TID 433) in 15 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:30:35,034 [er for task 434] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 650.0 (TID 434). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:35,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 650.0 (TID 434) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:30:35,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 650.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 650 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:30:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:30:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:30:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 651)
   [druid] 2019-06-11 09:30:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:30:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 651 (MapPartitionsRDD[764] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:35,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_326 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:35,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_326_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:35,036 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_326_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:35,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 326 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 651 (MapPartitionsRDD[764] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:30:35,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 651.0 with 1 tasks
   [druid] 2019-06-11 09:30:35,036 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 651.0 (TID 435, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:35,036 [er for task 435] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 651.0 (TID 435)
   [druid] 2019-06-11 09:30:35,038 [er for task 435] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:35,038 [er for task 435] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:30:35,038 [er for task 435] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:35,038 [er for task 435] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:35,039 [er for task 435] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 651.0 (TID 435). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:30:35,039 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 651.0 (TID 435) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:35,039 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 651.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 651 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:30:35,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 217 finished: print at Transform.scala:36, took 0.030809 s
   [druid] 2019-06-11 09:30:35,041 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:35,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 216 is 83 bytes
   [druid] 2019-06-11 09:30:35,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 217 is 160 bytes
   [druid] 2019-06-11 09:30:35,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 218 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:35,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 654 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:35,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 652, ShuffleMapStage 653)
   [druid] 2019-06-11 09:30:35,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:30:35,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 654 (MapPartitionsRDD[764] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:35,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_327 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:35,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_327_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:35,044 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_327_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:35,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 327 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:35,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 654 (MapPartitionsRDD[764] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:30:35,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 654.0 with 1 tasks
   [druid] 2019-06-11 09:30:35,044 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 654.0 (TID 436, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:35,044 [er for task 436] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 654.0 (TID 436)
   [druid] 2019-06-11 09:30:35,045 [er for task 436] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:35,045 [er for task 436] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:35,045 [er for task 436] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:35,045 [er for task 436] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:35,046 [er for task 436] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 654.0 (TID 436). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:35,046 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 654.0 (TID 436) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:35,046 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 654.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 654 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:30:35,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 218 finished: print at Transform.scala:36, took 0.004587 s
   [druid] 2019-06-11 09:30:35,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216635000 ms.0 from job set of time 1560216635000 ms
   [druid] 2019-06-11 09:30:35,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.046 s for time 1560216635000 ms (execution: 0.040 s)
   [druid] 2019-06-11 09:30:35,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 757 from persistence list
   [druid] 2019-06-11 09:30:35,047 [-thread-pool-20] INFO  che.spark.storage.BlockManager {1} - Removing RDD 757
   [druid] 2019-06-11 09:30:35,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 752 from persistence list
   [druid] 2019-06-11 09:30:35,047 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 752
   [druid] 2019-06-11 09:30:35,047 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 751 from persistence list
   [druid] 2019-06-11 09:30:35,047 [-thread-pool-20] INFO  che.spark.storage.BlockManager {1} - Removing RDD 751
   [druid] 2019-06-11 09:30:35,047 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[751] at socketTextStream at Transform.scala:18 of time 1560216635000 ms
   [druid] 2019-06-11 09:30:35,047 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216625000 ms
   [druid] 2019-06-11 09:30:35,047 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216625000 ms
   [druid] 2019-06-11 09:30:40,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216640000 ms
   [druid] 2019-06-11 09:30:40,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216640000 ms.0 from job set of time 1560216640000 ms
   [druid] 2019-06-11 09:30:40,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 766 (map at Transform.scala:20)
   [druid] 2019-06-11 09:30:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:30:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 219 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 657 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 655, ShuffleMapStage 656)
   [druid] 2019-06-11 09:30:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 656)
   [druid] 2019-06-11 09:30:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 656 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:30:40,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_328 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:40,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_328_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:30:40,010 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_328_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:40,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 328 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 656 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:30:40,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 656.0 with 2 tasks
   [druid] 2019-06-11 09:30:40,010 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 656.0 (TID 437, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:40,011 [er for task 437] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 656.0 (TID 437)
   [druid] 2019-06-11 09:30:40,020 [er for task 437] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 656.0 (TID 437). 853 bytes result sent to driver
   [druid] 2019-06-11 09:30:40,020 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 656.0 (TID 438, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:40,020 [er for task 438] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 656.0 (TID 438)
   [druid] 2019-06-11 09:30:40,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 656.0 (TID 437) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:30:40,028 [er for task 438] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 656.0 (TID 438). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:40,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 656.0 (TID 438) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:30:40,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 656.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 656 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:30:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:30:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:30:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 657)
   [druid] 2019-06-11 09:30:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:30:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 657 (MapPartitionsRDD[771] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:40,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_329 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:40,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_329_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:40,030 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_329_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:40,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 329 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 657 (MapPartitionsRDD[771] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:30:40,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 657.0 with 1 tasks
   [druid] 2019-06-11 09:30:40,031 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 657.0 (TID 439, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:40,031 [er for task 439] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 657.0 (TID 439)
   [druid] 2019-06-11 09:30:40,032 [er for task 439] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:40,032 [er for task 439] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:40,032 [er for task 439] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:40,032 [er for task 439] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:40,032 [er for task 439] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 657.0 (TID 439). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:40,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 657.0 (TID 439) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:40,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 657.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 657 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:30:40,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 219 finished: print at Transform.scala:36, took 0.024863 s
   [druid] 2019-06-11 09:30:40,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:40,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 218 is 83 bytes
   [druid] 2019-06-11 09:30:40,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 219 is 160 bytes
   [druid] 2019-06-11 09:30:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 220 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 660 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 658, ShuffleMapStage 659)
   [druid] 2019-06-11 09:30:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:30:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 660 (MapPartitionsRDD[771] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:40,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_330 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:40,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_330_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:30:40,038 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_330_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:40,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 330 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:40,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 660 (MapPartitionsRDD[771] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:30:40,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 660.0 with 1 tasks
   [druid] 2019-06-11 09:30:40,039 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 660.0 (TID 440, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:40,039 [er for task 440] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 660.0 (TID 440)
   [druid] 2019-06-11 09:30:40,040 [er for task 440] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:40,040 [er for task 440] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:40,040 [er for task 440] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:40,040 [er for task 440] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:40,041 [er for task 440] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 660.0 (TID 440). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:40,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 660.0 (TID 440) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:40,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 660.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:40,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 660 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:30:40,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 220 finished: print at Transform.scala:36, took 0.006372 s
   [druid] 2019-06-11 09:30:40,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216640000 ms.0 from job set of time 1560216640000 ms
   [druid] 2019-06-11 09:30:40,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560216640000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:30:40,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 764 from persistence list
   [druid] 2019-06-11 09:30:40,042 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 764
   [druid] 2019-06-11 09:30:40,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 759 from persistence list
   [druid] 2019-06-11 09:30:40,042 [-thread-pool-20] INFO  che.spark.storage.BlockManager {1} - Removing RDD 759
   [druid] 2019-06-11 09:30:40,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 758 from persistence list
   [druid] 2019-06-11 09:30:40,043 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 758
   [druid] 2019-06-11 09:30:40,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[758] at socketTextStream at Transform.scala:18 of time 1560216640000 ms
   [druid] 2019-06-11 09:30:40,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216630000 ms
   [druid] 2019-06-11 09:30:40,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216630000 ms
   [druid] 2019-06-11 09:30:45,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216645000 ms
   [druid] 2019-06-11 09:30:45,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216645000 ms.0 from job set of time 1560216645000 ms
   [druid] 2019-06-11 09:30:45,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:30:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 773 (map at Transform.scala:20)
   [druid] 2019-06-11 09:30:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 221 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 663 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 661, ShuffleMapStage 662)
   [druid] 2019-06-11 09:30:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 661)
   [druid] 2019-06-11 09:30:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 661 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:30:45,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_331 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:45,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_331_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:30:45,009 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_331_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:45,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 331 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 661 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:30:45,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 661.0 with 2 tasks
   [druid] 2019-06-11 09:30:45,010 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 661.0 (TID 441, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:45,010 [er for task 441] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 661.0 (TID 441)
   [druid] 2019-06-11 09:30:45,019 [er for task 441] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 661.0 (TID 441). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:45,019 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 661.0 (TID 442, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:45,019 [er for task 442] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 661.0 (TID 442)
   [druid] 2019-06-11 09:30:45,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 661.0 (TID 441) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:30:45,027 [er for task 442] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 661.0 (TID 442). 767 bytes result sent to driver
   [druid] 2019-06-11 09:30:45,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 661.0 (TID 442) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:30:45,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 661.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:45,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 661 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:30:45,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:30:45,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:30:45,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 663)
   [druid] 2019-06-11 09:30:45,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:30:45,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 663 (MapPartitionsRDD[778] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:45,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_332 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:45,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_332_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:45,029 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_332_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:45,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 332 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 663 (MapPartitionsRDD[778] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:30:45,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 663.0 with 1 tasks
   [druid] 2019-06-11 09:30:45,030 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 663.0 (TID 443, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:45,030 [er for task 443] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 663.0 (TID 443)
   [druid] 2019-06-11 09:30:45,030 [er for task 443] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:45,030 [er for task 443] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:45,030 [er for task 443] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:45,030 [er for task 443] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:45,031 [er for task 443] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 663.0 (TID 443). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:30:45,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 663.0 (TID 443) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:45,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 663.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:45,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 663 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:30:45,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 221 finished: print at Transform.scala:36, took 0.024365 s
   [druid] 2019-06-11 09:30:45,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:45,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 221 is 160 bytes
   [druid] 2019-06-11 09:30:45,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 220 is 83 bytes
   [druid] 2019-06-11 09:30:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 222 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 666 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 665, ShuffleMapStage 664)
   [druid] 2019-06-11 09:30:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:30:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 666 (MapPartitionsRDD[778] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:45,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_333 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:45,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_333_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:45,036 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_333_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:45,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 333 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 666 (MapPartitionsRDD[778] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:30:45,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 666.0 with 1 tasks
   [druid] 2019-06-11 09:30:45,036 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 666.0 (TID 444, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:45,037 [er for task 444] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 666.0 (TID 444)
   [druid] 2019-06-11 09:30:45,038 [er for task 444] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:45,038 [er for task 444] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:45,038 [er for task 444] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:45,038 [er for task 444] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:45,039 [er for task 444] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 666.0 (TID 444). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:30:45,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 666.0 (TID 444) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:45,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 666.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:45,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 666 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:30:45,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 222 finished: print at Transform.scala:36, took 0.005556 s
   [druid] 2019-06-11 09:30:45,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216645000 ms.0 from job set of time 1560216645000 ms
   [druid] 2019-06-11 09:30:45,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.039 s for time 1560216645000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:30:45,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 771 from persistence list
   [druid] 2019-06-11 09:30:45,040 [-thread-pool-20] INFO  che.spark.storage.BlockManager {1} - Removing RDD 771
   [druid] 2019-06-11 09:30:45,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 766 from persistence list
   [druid] 2019-06-11 09:30:45,040 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 766
   [druid] 2019-06-11 09:30:45,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 765 from persistence list
   [druid] 2019-06-11 09:30:45,040 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 765
   [druid] 2019-06-11 09:30:45,040 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[765] at socketTextStream at Transform.scala:18 of time 1560216645000 ms
   [druid] 2019-06-11 09:30:45,040 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216635000 ms
   [druid] 2019-06-11 09:30:45,040 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216635000 ms
   [druid] 2019-06-11 09:30:50,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216650000 ms
   [druid] 2019-06-11 09:30:50,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216650000 ms.0 from job set of time 1560216650000 ms
   [druid] 2019-06-11 09:30:50,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 780 (map at Transform.scala:20)
   [druid] 2019-06-11 09:30:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:30:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 223 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 669 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 667, ShuffleMapStage 668)
   [druid] 2019-06-11 09:30:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 668)
   [druid] 2019-06-11 09:30:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 668 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:30:50,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_334 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:50,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_334_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:30:50,011 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_334_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:50,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 334 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:50,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 668 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:30:50,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 668.0 with 2 tasks
   [druid] 2019-06-11 09:30:50,011 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 668.0 (TID 445, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:50,011 [er for task 445] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 668.0 (TID 445)
   [druid] 2019-06-11 09:30:50,020 [er for task 445] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 668.0 (TID 445). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:50,021 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 668.0 (TID 446, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:50,021 [er for task 446] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 668.0 (TID 446)
   [druid] 2019-06-11 09:30:50,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 668.0 (TID 445) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:30:50,028 [er for task 446] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 668.0 (TID 446). 767 bytes result sent to driver
   [druid] 2019-06-11 09:30:50,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 668.0 (TID 446) in 7 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:30:50,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 668.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 668 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:30:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:30:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:30:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 669)
   [druid] 2019-06-11 09:30:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:30:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 669 (MapPartitionsRDD[785] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:50,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_335 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:50,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_335_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:50,030 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_335_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:50,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 335 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:50,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 669 (MapPartitionsRDD[785] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:30:50,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 669.0 with 1 tasks
   [druid] 2019-06-11 09:30:50,030 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 669.0 (TID 447, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:50,031 [er for task 447] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 669.0 (TID 447)
   [druid] 2019-06-11 09:30:50,031 [er for task 447] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:50,031 [er for task 447] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:50,031 [er for task 447] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:50,031 [er for task 447] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:50,032 [er for task 447] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 669.0 (TID 447). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:50,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 669.0 (TID 447) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:50,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 669.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:50,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 669 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:30:50,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 223 finished: print at Transform.scala:36, took 0.024235 s
   [druid] 2019-06-11 09:30:50,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:50,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 222 is 83 bytes
   [druid] 2019-06-11 09:30:50,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 223 is 160 bytes
   [druid] 2019-06-11 09:30:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 224 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 672 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 670, ShuffleMapStage 671)
   [druid] 2019-06-11 09:30:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:30:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 672 (MapPartitionsRDD[785] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:50,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_336 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:50,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_336_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:50,038 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_336_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:50,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 336 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:50,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 672 (MapPartitionsRDD[785] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:30:50,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 672.0 with 1 tasks
   [druid] 2019-06-11 09:30:50,039 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 672.0 (TID 448, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:50,039 [er for task 448] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 672.0 (TID 448)
   [druid] 2019-06-11 09:30:50,040 [er for task 448] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:50,040 [er for task 448] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:50,040 [er for task 448] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:50,040 [er for task 448] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:50,040 [er for task 448] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 672.0 (TID 448). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:50,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 672.0 (TID 448) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:50,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 672.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:50,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 672 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:30:50,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 224 finished: print at Transform.scala:36, took 0.005163 s
   [druid] 2019-06-11 09:30:50,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216650000 ms.0 from job set of time 1560216650000 ms
   [druid] 2019-06-11 09:30:50,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560216650000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:30:50,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 778 from persistence list
   [druid] 2019-06-11 09:30:50,041 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 778
   [druid] 2019-06-11 09:30:50,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 773 from persistence list
   [druid] 2019-06-11 09:30:50,042 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 773
   [druid] 2019-06-11 09:30:50,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 772 from persistence list
   [druid] 2019-06-11 09:30:50,042 [-thread-pool-20] INFO  che.spark.storage.BlockManager {1} - Removing RDD 772
   [druid] 2019-06-11 09:30:50,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[772] at socketTextStream at Transform.scala:18 of time 1560216650000 ms
   [druid] 2019-06-11 09:30:50,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216640000 ms
   [druid] 2019-06-11 09:30:50,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216640000 ms
   [druid] 2019-06-11 09:30:55,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216655000 ms
   [druid] 2019-06-11 09:30:55,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216655000 ms.0 from job set of time 1560216655000 ms
   [druid] 2019-06-11 09:30:55,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 787 (map at Transform.scala:20)
   [druid] 2019-06-11 09:30:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:30:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 225 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 675 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 673, ShuffleMapStage 674)
   [druid] 2019-06-11 09:30:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 674)
   [druid] 2019-06-11 09:30:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 674 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:30:55,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_337 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:55,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_337_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:30:55,010 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_337_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:55,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 337 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:55,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 674 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:30:55,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 674.0 with 2 tasks
   [druid] 2019-06-11 09:30:55,012 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 674.0 (TID 449, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:55,012 [er for task 449] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 674.0 (TID 449)
   [druid] 2019-06-11 09:30:55,021 [er for task 449] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 674.0 (TID 449). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:55,021 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 674.0 (TID 450, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:30:55,021 [er for task 450] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 674.0 (TID 450)
   [druid] 2019-06-11 09:30:55,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 674.0 (TID 449) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:30:55,029 [er for task 450] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 674.0 (TID 450). 810 bytes result sent to driver
   [druid] 2019-06-11 09:30:55,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 674.0 (TID 450) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:30:55,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 674.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 674 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:30:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:30:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:30:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 675)
   [druid] 2019-06-11 09:30:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:30:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 675 (MapPartitionsRDD[792] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:55,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_338 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:55,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_338_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:55,031 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_338_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:55,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 338 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:55,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 675 (MapPartitionsRDD[792] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:30:55,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 675.0 with 1 tasks
   [druid] 2019-06-11 09:30:55,032 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 675.0 (TID 451, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:55,032 [er for task 451] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 675.0 (TID 451)
   [druid] 2019-06-11 09:30:55,033 [er for task 451] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:55,033 [er for task 451] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:55,034 [er for task 451] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:55,034 [er for task 451] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:30:55,034 [er for task 451] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 675.0 (TID 451). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:55,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 675.0 (TID 451) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:55,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 675.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 675 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:30:55,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 225 finished: print at Transform.scala:36, took 0.026232 s
   [druid] 2019-06-11 09:30:55,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:30:55,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 224 is 83 bytes
   [druid] 2019-06-11 09:30:55,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 225 is 160 bytes
   [druid] 2019-06-11 09:30:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 226 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:30:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 678 (print at Transform.scala:36)
   [druid] 2019-06-11 09:30:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 676, ShuffleMapStage 677)
   [druid] 2019-06-11 09:30:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:30:55,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 678 (MapPartitionsRDD[792] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:30:55,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_339 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:55,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_339_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:30:55,039 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_339_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:30:55,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 339 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:30:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 678 (MapPartitionsRDD[792] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:30:55,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 678.0 with 1 tasks
   [druid] 2019-06-11 09:30:55,040 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 678.0 (TID 452, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:30:55,040 [er for task 452] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 678.0 (TID 452)
   [druid] 2019-06-11 09:30:55,040 [er for task 452] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:30:55,040 [er for task 452] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:30:55,041 [er for task 452] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:30:55,041 [er for task 452] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:30:55,041 [er for task 452] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 678.0 (TID 452). 966 bytes result sent to driver
   [druid] 2019-06-11 09:30:55,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 678.0 (TID 452) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:30:55,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 678.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:30:55,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 678 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:30:55,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 226 finished: print at Transform.scala:36, took 0.004931 s
   [druid] 2019-06-11 09:30:55,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216655000 ms.0 from job set of time 1560216655000 ms
   [druid] 2019-06-11 09:30:55,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560216655000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:30:55,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 785 from persistence list
   [druid] 2019-06-11 09:30:55,042 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 785
   [druid] 2019-06-11 09:30:55,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 780 from persistence list
   [druid] 2019-06-11 09:30:55,042 [-thread-pool-20] INFO  che.spark.storage.BlockManager {1} - Removing RDD 780
   [druid] 2019-06-11 09:30:55,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 779 from persistence list
   [druid] 2019-06-11 09:30:55,043 [-thread-pool-16] INFO  che.spark.storage.BlockManager {1} - Removing RDD 779
   [druid] 2019-06-11 09:30:55,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[779] at socketTextStream at Transform.scala:18 of time 1560216655000 ms
   [druid] 2019-06-11 09:30:55,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216645000 ms
   [druid] 2019-06-11 09:30:55,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216645000 ms
   [druid] 2019-06-11 09:31:00,009 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216660000 ms
   [druid] 2019-06-11 09:31:00,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216660000 ms.0 from job set of time 1560216660000 ms
   [druid] 2019-06-11 09:31:00,011 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:31:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 794 (map at Transform.scala:20)
   [druid] 2019-06-11 09:31:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 227 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 681 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 679, ShuffleMapStage 680)
   [druid] 2019-06-11 09:31:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 679)
   [druid] 2019-06-11 09:31:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 679 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:31:00,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_340 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:00,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_340_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:31:00,013 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_340_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 340 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:00,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 679 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:31:00,014 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 679.0 with 2 tasks
   [druid] 2019-06-11 09:31:00,014 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 679.0 (TID 453, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:00,014 [er for task 453] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 679.0 (TID 453)
   [druid] 2019-06-11 09:31:00,023 [er for task 453] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 679.0 (TID 453). 853 bytes result sent to driver
   [druid] 2019-06-11 09:31:00,024 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 679.0 (TID 454, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:00,024 [er for task 454] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 679.0 (TID 454)
   [druid] 2019-06-11 09:31:00,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 679.0 (TID 453) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:31:00,038 [er for task 454] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 679.0 (TID 454). 767 bytes result sent to driver
   [druid] 2019-06-11 09:31:00,039 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 679.0 (TID 454) in 15 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:31:00,039 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 679.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 679 (parallelize at Transform.scala:16) finished in 0.025 s
   [druid] 2019-06-11 09:31:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:31:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:31:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 681)
   [druid] 2019-06-11 09:31:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:31:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 681 (MapPartitionsRDD[799] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:00,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_341 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:00,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_341_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:00,046 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 212
   [druid] 2019-06-11 09:31:00,047 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_341_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,049 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 341 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:00,050 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_333_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 681 (MapPartitionsRDD[799] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:31:00,050 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 681.0 with 1 tasks
   [druid] 2019-06-11 09:31:00,050 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 218
   [druid] 2019-06-11 09:31:00,051 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 681.0 (TID 455, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:00,051 [er for task 455] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 681.0 (TID 455)
   [druid] 2019-06-11 09:31:00,051 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_326_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 216
   [druid] 2019-06-11 09:31:00,051 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_327_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,051 [er for task 455] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:00,051 [er for task 455] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:00,052 [er for task 455] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:00,052 [er for task 455] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:31:00,052 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_336_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,052 [er for task 455] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 681.0 (TID 455). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:00,052 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 681.0 (TID 455) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:00,052 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_324_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,053 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 681.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:00,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 681 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:31:00,053 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 227 finished: print at Transform.scala:36, took 0.041054 s
   [druid] 2019-06-11 09:31:00,053 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_322_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,054 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_337_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,054 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_331_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 223
   [druid] 2019-06-11 09:31:00,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 217
   [druid] 2019-06-11 09:31:00,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 219
   [druid] 2019-06-11 09:31:00,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 214
   [druid] 2019-06-11 09:31:00,056 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 222
   [druid] 2019-06-11 09:31:00,056 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_335_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,057 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 215
   [druid] 2019-06-11 09:31:00,057 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 221
   [druid] 2019-06-11 09:31:00,058 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_329_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,058 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:00,059 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 227 is 160 bytes
   [druid] 2019-06-11 09:31:00,059 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 226 is 83 bytes
   [druid] 2019-06-11 09:31:00,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 228 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:00,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 684 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:00,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 682, ShuffleMapStage 683)
   [druid] 2019-06-11 09:31:00,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:31:00,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 684 (MapPartitionsRDD[799] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:00,060 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_342 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:00,061 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_342_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:00,062 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_342_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,062 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 342 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:00,062 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_334_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 684 (MapPartitionsRDD[799] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:31:00,062 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 684.0 with 1 tasks
   [druid] 2019-06-11 09:31:00,063 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 684.0 (TID 456, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:00,063 [er for task 456] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 684.0 (TID 456)
   [druid] 2019-06-11 09:31:00,063 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_338_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,064 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 213
   [druid] 2019-06-11 09:31:00,065 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_328_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,065 [er for task 456] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:00,065 [er for task 456] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:00,066 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_339_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,066 [er for task 456] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:00,066 [er for task 456] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:00,066 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 220
   [druid] 2019-06-11 09:31:00,066 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_323_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,067 [er for task 456] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 684.0 (TID 456). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:31:00,067 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_330_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,067 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 684.0 (TID 456) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:00,067 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 684.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:00,067 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 684 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:31:00,067 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 228 finished: print at Transform.scala:36, took 0.008685 s
   [druid] 2019-06-11 09:31:00,067 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_325_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:00,067 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216660000 ms.0 from job set of time 1560216660000 ms
   [druid] 2019-06-11 09:31:00,068 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.067 s for time 1560216660000 ms (execution: 0.058 s)
   [druid] 2019-06-11 09:31:00,068 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 792 from persistence list
   [druid] 2019-06-11 09:31:00,068 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 792
   [druid] 2019-06-11 09:31:00,068 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 787 from persistence list
   [druid] 2019-06-11 09:31:00,068 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 787
   [druid] 2019-06-11 09:31:00,068 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 786 from persistence list
   [druid] 2019-06-11 09:31:00,068 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 786
   [druid] 2019-06-11 09:31:00,068 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[786] at socketTextStream at Transform.scala:18 of time 1560216660000 ms
   [druid] 2019-06-11 09:31:00,068 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216650000 ms
   [druid] 2019-06-11 09:31:00,068 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216650000 ms
   [druid] 2019-06-11 09:31:00,068 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_332_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:05,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216665000 ms
   [druid] 2019-06-11 09:31:05,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216665000 ms.0 from job set of time 1560216665000 ms
   [druid] 2019-06-11 09:31:05,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 801 (map at Transform.scala:20)
   [druid] 2019-06-11 09:31:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:31:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 229 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 687 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 685, ShuffleMapStage 686)
   [druid] 2019-06-11 09:31:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 686)
   [druid] 2019-06-11 09:31:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 686 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:31:05,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_343 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:05,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_343_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:31:05,010 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_343_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:05,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 343 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 686 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:31:05,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 686.0 with 2 tasks
   [druid] 2019-06-11 09:31:05,011 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 686.0 (TID 457, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:05,011 [er for task 457] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 686.0 (TID 457)
   [druid] 2019-06-11 09:31:05,020 [er for task 457] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 686.0 (TID 457). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:05,020 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 686.0 (TID 458, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:05,020 [er for task 458] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 686.0 (TID 458)
   [druid] 2019-06-11 09:31:05,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 686.0 (TID 457) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:31:05,028 [er for task 458] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 686.0 (TID 458). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:05,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 686.0 (TID 458) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:31:05,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 686.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 686 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:31:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:31:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:31:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 687)
   [druid] 2019-06-11 09:31:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:31:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 687 (MapPartitionsRDD[806] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:05,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_344 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:05,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_344_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:05,030 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_344_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:05,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 344 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:05,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 687 (MapPartitionsRDD[806] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:31:05,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 687.0 with 1 tasks
   [druid] 2019-06-11 09:31:05,030 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 687.0 (TID 459, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:05,031 [er for task 459] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 687.0 (TID 459)
   [druid] 2019-06-11 09:31:05,031 [er for task 459] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:05,031 [er for task 459] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:05,031 [er for task 459] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:05,031 [er for task 459] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:05,032 [er for task 459] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 687.0 (TID 459). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:05,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 687.0 (TID 459) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:05,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 687.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:05,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 687 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:05,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 229 finished: print at Transform.scala:36, took 0.024379 s
   [druid] 2019-06-11 09:31:05,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:05,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 228 is 83 bytes
   [druid] 2019-06-11 09:31:05,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 229 is 160 bytes
   [druid] 2019-06-11 09:31:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 230 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 690 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 688, ShuffleMapStage 689)
   [druid] 2019-06-11 09:31:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:31:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 690 (MapPartitionsRDD[806] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:05,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_345 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:05,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_345_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:05,037 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_345_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:05,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 345 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:05,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 690 (MapPartitionsRDD[806] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:31:05,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 690.0 with 1 tasks
   [druid] 2019-06-11 09:31:05,038 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 690.0 (TID 460, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:05,038 [er for task 460] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 690.0 (TID 460)
   [druid] 2019-06-11 09:31:05,038 [er for task 460] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:05,038 [er for task 460] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:05,039 [er for task 460] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:05,039 [er for task 460] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:31:05,039 [er for task 460] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 690.0 (TID 460). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:05,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 690.0 (TID 460) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:05,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 690.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:05,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 690 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:31:05,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 230 finished: print at Transform.scala:36, took 0.005659 s
   [druid] 2019-06-11 09:31:05,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216665000 ms.0 from job set of time 1560216665000 ms
   [druid] 2019-06-11 09:31:05,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560216665000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:31:05,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 799 from persistence list
   [druid] 2019-06-11 09:31:05,040 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 799
   [druid] 2019-06-11 09:31:05,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 794 from persistence list
   [druid] 2019-06-11 09:31:05,041 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 794
   [druid] 2019-06-11 09:31:05,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 793 from persistence list
   [druid] 2019-06-11 09:31:05,041 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 793
   [druid] 2019-06-11 09:31:05,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[793] at socketTextStream at Transform.scala:18 of time 1560216665000 ms
   [druid] 2019-06-11 09:31:05,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216655000 ms
   [druid] 2019-06-11 09:31:05,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216655000 ms
   [druid] 2019-06-11 09:31:10,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216670000 ms
   [druid] 2019-06-11 09:31:10,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216670000 ms.0 from job set of time 1560216670000 ms
   [druid] 2019-06-11 09:31:10,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:31:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 808 (map at Transform.scala:20)
   [druid] 2019-06-11 09:31:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 231 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 693 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 691, ShuffleMapStage 692)
   [druid] 2019-06-11 09:31:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 691)
   [druid] 2019-06-11 09:31:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 691 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:31:10,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_346 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:10,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_346_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:31:10,010 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_346_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:10,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 346 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 691 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:31:10,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 691.0 with 2 tasks
   [druid] 2019-06-11 09:31:10,011 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 691.0 (TID 461, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:10,011 [er for task 461] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 691.0 (TID 461)
   [druid] 2019-06-11 09:31:10,020 [er for task 461] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 691.0 (TID 461). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:10,021 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 691.0 (TID 462, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:10,021 [er for task 462] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 691.0 (TID 462)
   [druid] 2019-06-11 09:31:10,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 691.0 (TID 461) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:31:10,029 [er for task 462] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 691.0 (TID 462). 767 bytes result sent to driver
   [druid] 2019-06-11 09:31:10,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 691.0 (TID 462) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:31:10,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 691.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 691 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:31:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:31:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:31:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 693)
   [druid] 2019-06-11 09:31:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:31:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 693 (MapPartitionsRDD[813] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:10,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_347 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:10,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_347_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:10,032 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_347_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:10,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 347 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 693 (MapPartitionsRDD[813] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:31:10,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 693.0 with 1 tasks
   [druid] 2019-06-11 09:31:10,032 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 693.0 (TID 463, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:10,032 [er for task 463] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 693.0 (TID 463)
   [druid] 2019-06-11 09:31:10,033 [er for task 463] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:10,033 [er for task 463] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:10,033 [er for task 463] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:10,033 [er for task 463] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:10,034 [er for task 463] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 693.0 (TID 463). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:10,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 693.0 (TID 463) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:10,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 693.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 693 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:10,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 231 finished: print at Transform.scala:36, took 0.026104 s
   [druid] 2019-06-11 09:31:10,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:10,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 231 is 160 bytes
   [druid] 2019-06-11 09:31:10,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 230 is 83 bytes
   [druid] 2019-06-11 09:31:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 232 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 696 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 694, ShuffleMapStage 695)
   [druid] 2019-06-11 09:31:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:31:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 696 (MapPartitionsRDD[813] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:10,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_348 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:10,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_348_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:10,039 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_348_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:10,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 348 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 696 (MapPartitionsRDD[813] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:31:10,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 696.0 with 1 tasks
   [druid] 2019-06-11 09:31:10,040 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 696.0 (TID 464, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:10,040 [er for task 464] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 696.0 (TID 464)
   [druid] 2019-06-11 09:31:10,041 [er for task 464] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:10,041 [er for task 464] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:10,041 [er for task 464] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:10,041 [er for task 464] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:10,041 [er for task 464] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 696.0 (TID 464). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:10,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 696.0 (TID 464) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:10,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 696.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 696 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:10,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 232 finished: print at Transform.scala:36, took 0.004792 s
   [druid] 2019-06-11 09:31:10,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216670000 ms.0 from job set of time 1560216670000 ms
   [druid] 2019-06-11 09:31:10,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560216670000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:31:10,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 806 from persistence list
   [druid] 2019-06-11 09:31:10,043 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 806
   [druid] 2019-06-11 09:31:10,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 801 from persistence list
   [druid] 2019-06-11 09:31:10,043 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 801
   [druid] 2019-06-11 09:31:10,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 800 from persistence list
   [druid] 2019-06-11 09:31:10,043 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 800
   [druid] 2019-06-11 09:31:10,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[800] at socketTextStream at Transform.scala:18 of time 1560216670000 ms
   [druid] 2019-06-11 09:31:10,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216660000 ms
   [druid] 2019-06-11 09:31:10,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216660000 ms
   [druid] 2019-06-11 09:31:15,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216675000 ms
   [druid] 2019-06-11 09:31:15,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216675000 ms.0 from job set of time 1560216675000 ms
   [druid] 2019-06-11 09:31:15,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:31:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 815 (map at Transform.scala:20)
   [druid] 2019-06-11 09:31:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 233 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 699 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 698, ShuffleMapStage 697)
   [druid] 2019-06-11 09:31:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 697)
   [druid] 2019-06-11 09:31:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 697 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:31:15,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_349 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:15,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_349_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:31:15,010 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_349_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:15,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 349 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:15,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 697 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:31:15,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 697.0 with 2 tasks
   [druid] 2019-06-11 09:31:15,011 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 697.0 (TID 465, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:15,011 [er for task 465] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 697.0 (TID 465)
   [druid] 2019-06-11 09:31:15,020 [er for task 465] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 697.0 (TID 465). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:15,020 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 697.0 (TID 466, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:15,021 [er for task 466] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 697.0 (TID 466)
   [druid] 2019-06-11 09:31:15,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 697.0 (TID 465) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:31:15,034 [er for task 466] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 697.0 (TID 466). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:15,035 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 697.0 (TID 466) in 15 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:31:15,035 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 697.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 697 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:31:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:31:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:31:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 699)
   [druid] 2019-06-11 09:31:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:31:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 699 (MapPartitionsRDD[820] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:15,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_350 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:15,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_350_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:15,037 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_350_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:15,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 350 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 699 (MapPartitionsRDD[820] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:31:15,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 699.0 with 1 tasks
   [druid] 2019-06-11 09:31:15,037 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 699.0 (TID 467, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:15,038 [er for task 467] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 699.0 (TID 467)
   [druid] 2019-06-11 09:31:15,039 [er for task 467] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:15,039 [er for task 467] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:15,039 [er for task 467] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:15,039 [er for task 467] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:15,040 [er for task 467] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 699.0 (TID 467). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:31:15,041 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 699.0 (TID 467) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:15,041 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 699.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 699 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:31:15,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 233 finished: print at Transform.scala:36, took 0.032759 s
   [druid] 2019-06-11 09:31:15,044 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:15,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 233 is 160 bytes
   [druid] 2019-06-11 09:31:15,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 232 is 83 bytes
   [druid] 2019-06-11 09:31:15,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 234 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:15,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 702 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:15,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 700, ShuffleMapStage 701)
   [druid] 2019-06-11 09:31:15,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:31:15,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 702 (MapPartitionsRDD[820] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:15,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_351 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:15,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_351_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:15,046 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_351_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:15,047 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 351 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:15,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 702 (MapPartitionsRDD[820] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:31:15,047 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 702.0 with 1 tasks
   [druid] 2019-06-11 09:31:15,047 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 702.0 (TID 468, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:15,047 [er for task 468] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 702.0 (TID 468)
   [druid] 2019-06-11 09:31:15,048 [er for task 468] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:15,048 [er for task 468] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:15,049 [er for task 468] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:15,049 [er for task 468] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:15,050 [er for task 468] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 702.0 (TID 468). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:15,050 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 702.0 (TID 468) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:15,050 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 702.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:15,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 702 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:31:15,050 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 234 finished: print at Transform.scala:36, took 0.006388 s
   [druid] 2019-06-11 09:31:15,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216675000 ms.0 from job set of time 1560216675000 ms
   [druid] 2019-06-11 09:31:15,051 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.050 s for time 1560216675000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:31:15,051 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 813 from persistence list
   [druid] 2019-06-11 09:31:15,051 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 813
   [druid] 2019-06-11 09:31:15,051 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 808 from persistence list
   [druid] 2019-06-11 09:31:15,051 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 808
   [druid] 2019-06-11 09:31:15,051 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 807 from persistence list
   [druid] 2019-06-11 09:31:15,051 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 807
   [druid] 2019-06-11 09:31:15,051 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[807] at socketTextStream at Transform.scala:18 of time 1560216675000 ms
   [druid] 2019-06-11 09:31:15,051 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216665000 ms
   [druid] 2019-06-11 09:31:15,051 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216665000 ms
   [druid] 2019-06-11 09:31:20,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216680000 ms
   [druid] 2019-06-11 09:31:20,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216680000 ms.0 from job set of time 1560216680000 ms
   [druid] 2019-06-11 09:31:20,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:31:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 822 (map at Transform.scala:20)
   [druid] 2019-06-11 09:31:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 235 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 705 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 703, ShuffleMapStage 704)
   [druid] 2019-06-11 09:31:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 703)
   [druid] 2019-06-11 09:31:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 703 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:31:20,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_352 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:20,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_352_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:31:20,011 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_352_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:20,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 352 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:20,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 703 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:31:20,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 703.0 with 2 tasks
   [druid] 2019-06-11 09:31:20,012 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 703.0 (TID 469, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:20,012 [er for task 469] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 703.0 (TID 469)
   [druid] 2019-06-11 09:31:20,021 [er for task 469] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 703.0 (TID 469). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:20,022 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 703.0 (TID 470, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:20,022 [er for task 470] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 703.0 (TID 470)
   [druid] 2019-06-11 09:31:20,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 703.0 (TID 469) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:31:20,031 [er for task 470] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 703.0 (TID 470). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:20,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 703.0 (TID 470) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:31:20,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 703.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 703 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:31:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:31:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:31:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 705)
   [druid] 2019-06-11 09:31:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:31:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 705 (MapPartitionsRDD[827] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:20,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_353 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:20,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_353_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:20,034 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_353_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:20,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 353 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 705 (MapPartitionsRDD[827] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:31:20,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 705.0 with 1 tasks
   [druid] 2019-06-11 09:31:20,035 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 705.0 (TID 471, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:20,035 [er for task 471] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 705.0 (TID 471)
   [druid] 2019-06-11 09:31:20,036 [er for task 471] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:20,036 [er for task 471] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:20,036 [er for task 471] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:20,036 [er for task 471] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:20,037 [er for task 471] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 705.0 (TID 471). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:20,037 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 705.0 (TID 471) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:20,037 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 705.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 705 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:20,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 235 finished: print at Transform.scala:36, took 0.029011 s
   [druid] 2019-06-11 09:31:20,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:20,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 235 is 160 bytes
   [druid] 2019-06-11 09:31:20,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 234 is 83 bytes
   [druid] 2019-06-11 09:31:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 236 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 708 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 706, ShuffleMapStage 707)
   [druid] 2019-06-11 09:31:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:31:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 708 (MapPartitionsRDD[827] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:20,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_354 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:20,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_354_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:20,042 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_354_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:20,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 354 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:20,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 708 (MapPartitionsRDD[827] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:31:20,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 708.0 with 1 tasks
   [druid] 2019-06-11 09:31:20,042 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 708.0 (TID 472, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:20,043 [er for task 472] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 708.0 (TID 472)
   [druid] 2019-06-11 09:31:20,043 [er for task 472] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:20,043 [er for task 472] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:20,043 [er for task 472] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:20,043 [er for task 472] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:20,044 [er for task 472] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 708.0 (TID 472). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:31:20,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 708.0 (TID 472) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:20,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 708.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:20,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 708 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:20,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 236 finished: print at Transform.scala:36, took 0.005067 s
   [druid] 2019-06-11 09:31:20,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216680000 ms.0 from job set of time 1560216680000 ms
   [druid] 2019-06-11 09:31:20,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560216680000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:31:20,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 820 from persistence list
   [druid] 2019-06-11 09:31:20,045 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 820
   [druid] 2019-06-11 09:31:20,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 815 from persistence list
   [druid] 2019-06-11 09:31:20,045 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 815
   [druid] 2019-06-11 09:31:20,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 814 from persistence list
   [druid] 2019-06-11 09:31:20,045 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 814
   [druid] 2019-06-11 09:31:20,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[814] at socketTextStream at Transform.scala:18 of time 1560216680000 ms
   [druid] 2019-06-11 09:31:20,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216670000 ms
   [druid] 2019-06-11 09:31:20,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216670000 ms
   [druid] 2019-06-11 09:31:25,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216685000 ms
   [druid] 2019-06-11 09:31:25,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216685000 ms.0 from job set of time 1560216685000 ms
   [druid] 2019-06-11 09:31:25,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:31:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 829 (map at Transform.scala:20)
   [druid] 2019-06-11 09:31:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 237 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 711 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 709, ShuffleMapStage 710)
   [druid] 2019-06-11 09:31:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 709)
   [druid] 2019-06-11 09:31:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 709 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:31:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_355 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:25,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_355_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:31:25,010 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_355_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:25,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 355 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 709 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:31:25,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 709.0 with 2 tasks
   [druid] 2019-06-11 09:31:25,011 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 709.0 (TID 473, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:25,011 [er for task 473] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 709.0 (TID 473)
   [druid] 2019-06-11 09:31:25,024 [er for task 473] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 709.0 (TID 473). 767 bytes result sent to driver
   [druid] 2019-06-11 09:31:25,024 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 709.0 (TID 474, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:25,024 [er for task 474] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 709.0 (TID 474)
   [druid] 2019-06-11 09:31:25,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 709.0 (TID 473) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:31:25,033 [er for task 474] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 709.0 (TID 474). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:25,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 709.0 (TID 474) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:31:25,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 709.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 709 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:31:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:31:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:31:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 711)
   [druid] 2019-06-11 09:31:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:31:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 711 (MapPartitionsRDD[834] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:25,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_356 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:25,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_356_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:25,037 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_356_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:25,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 356 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:25,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 711 (MapPartitionsRDD[834] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:31:25,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 711.0 with 1 tasks
   [druid] 2019-06-11 09:31:25,038 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 711.0 (TID 475, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:25,038 [er for task 475] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 711.0 (TID 475)
   [druid] 2019-06-11 09:31:25,039 [er for task 475] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:25,039 [er for task 475] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:25,039 [er for task 475] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:25,039 [er for task 475] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:25,040 [er for task 475] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 711.0 (TID 475). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:31:25,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 711.0 (TID 475) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:25,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 711.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 711 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:31:25,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 237 finished: print at Transform.scala:36, took 0.031879 s
   [druid] 2019-06-11 09:31:25,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:25,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 237 is 160 bytes
   [druid] 2019-06-11 09:31:25,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 236 is 83 bytes
   [druid] 2019-06-11 09:31:25,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 238 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 714 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 712, ShuffleMapStage 713)
   [druid] 2019-06-11 09:31:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:31:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 714 (MapPartitionsRDD[834] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:25,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_357 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:25,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_357_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:25,045 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_357_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:25,045 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 357 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:25,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 714 (MapPartitionsRDD[834] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:31:25,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 714.0 with 1 tasks
   [druid] 2019-06-11 09:31:25,046 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 714.0 (TID 476, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:25,046 [er for task 476] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 714.0 (TID 476)
   [druid] 2019-06-11 09:31:25,048 [er for task 476] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:25,048 [er for task 476] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:25,048 [er for task 476] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:25,048 [er for task 476] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:25,049 [er for task 476] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 714.0 (TID 476). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:31:25,049 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 714.0 (TID 476) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:25,049 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 714.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:25,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 714 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:31:25,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 238 finished: print at Transform.scala:36, took 0.006534 s
   [druid] 2019-06-11 09:31:25,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216685000 ms.0 from job set of time 1560216685000 ms
   [druid] 2019-06-11 09:31:25,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.050 s for time 1560216685000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:31:25,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 827 from persistence list
   [druid] 2019-06-11 09:31:25,050 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 827
   [druid] 2019-06-11 09:31:25,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 822 from persistence list
   [druid] 2019-06-11 09:31:25,050 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 822
   [druid] 2019-06-11 09:31:25,050 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 821 from persistence list
   [druid] 2019-06-11 09:31:25,050 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 821
   [druid] 2019-06-11 09:31:25,050 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[821] at socketTextStream at Transform.scala:18 of time 1560216685000 ms
   [druid] 2019-06-11 09:31:25,050 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216675000 ms
   [druid] 2019-06-11 09:31:25,050 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216675000 ms
   [druid] 2019-06-11 09:31:30,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216690000 ms
   [druid] 2019-06-11 09:31:30,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216690000 ms.0 from job set of time 1560216690000 ms
   [druid] 2019-06-11 09:31:30,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:31:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 836 (map at Transform.scala:20)
   [druid] 2019-06-11 09:31:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 239 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 717 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 716, ShuffleMapStage 715)
   [druid] 2019-06-11 09:31:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 715)
   [druid] 2019-06-11 09:31:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 715 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:31:30,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_358 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:30,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_358_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:31:30,010 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_358_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 358 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 715 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:31:30,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 715.0 with 2 tasks
   [druid] 2019-06-11 09:31:30,011 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 715.0 (TID 477, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:30,011 [er for task 477] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 715.0 (TID 477)
   [druid] 2019-06-11 09:31:30,025 [er for task 477] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 715.0 (TID 477). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:30,025 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 715.0 (TID 478, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:30,025 [er for task 478] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 715.0 (TID 478)
   [druid] 2019-06-11 09:31:30,025 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 715.0 (TID 477) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:31:30,033 [er for task 478] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 715.0 (TID 478). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:30,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 715.0 (TID 478) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:31:30,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 715.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 715 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:31:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:31:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:31:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 717)
   [druid] 2019-06-11 09:31:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:31:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 717 (MapPartitionsRDD[841] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:30,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_359 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:30,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_359_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:30,042 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_359_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,043 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_346_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 359 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:30,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 717 (MapPartitionsRDD[841] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:31:30,047 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 717.0 with 1 tasks
   [druid] 2019-06-11 09:31:30,047 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_351_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,047 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 717.0 (TID 479, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:30,048 [er for task 479] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 717.0 (TID 479)
   [druid] 2019-06-11 09:31:30,049 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_344_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,049 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 226
   [druid] 2019-06-11 09:31:30,049 [er for task 479] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:30,050 [er for task 479] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:31:30,050 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 228
   [druid] 2019-06-11 09:31:30,050 [er for task 479] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:30,050 [er for task 479] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:30,050 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 232
   [druid] 2019-06-11 09:31:30,050 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 234
   [druid] 2019-06-11 09:31:30,051 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_354_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,051 [er for task 479] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 717.0 (TID 479). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:31:30,051 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 717.0 (TID 479) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:30,051 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 717.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:30,051 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_355_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 717 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:31:30,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 239 finished: print at Transform.scala:36, took 0.043751 s
   [druid] 2019-06-11 09:31:30,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 224
   [druid] 2019-06-11 09:31:30,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 225
   [druid] 2019-06-11 09:31:30,052 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_348_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,053 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_349_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,054 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_352_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,054 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 229
   [druid] 2019-06-11 09:31:30,054 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 227
   [druid] 2019-06-11 09:31:30,055 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_340_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,056 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_353_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,056 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_343_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,057 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 231
   [druid] 2019-06-11 09:31:30,057 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_356_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,058 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_357_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,059 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_350_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,059 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:30,060 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 239 is 160 bytes
   [druid] 2019-06-11 09:31:30,060 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_342_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,060 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 238 is 83 bytes
   [druid] 2019-06-11 09:31:30,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 240 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:30,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 720 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:30,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 718, ShuffleMapStage 719)
   [druid] 2019-06-11 09:31:30,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:31:30,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 720 (MapPartitionsRDD[841] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:30,060 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_347_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,060 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 233
   [druid] 2019-06-11 09:31:30,061 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 230
   [druid] 2019-06-11 09:31:30,061 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_360 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:30,061 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 235
   [druid] 2019-06-11 09:31:30,062 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_360_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:30,062 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_360_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,062 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_341_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,062 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 360 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:30,063 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 720 (MapPartitionsRDD[841] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:31:30,063 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 720.0 with 1 tasks
   [druid] 2019-06-11 09:31:30,063 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 720.0 (TID 480, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:30,063 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_345_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:30,063 [er for task 480] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 720.0 (TID 480)
   [druid] 2019-06-11 09:31:30,064 [er for task 480] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:30,064 [er for task 480] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:30,065 [er for task 480] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:30,065 [er for task 480] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:30,066 [er for task 480] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 720.0 (TID 480). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:30,066 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 720.0 (TID 480) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:30,066 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 720.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:30,066 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 720 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:31:30,066 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 240 finished: print at Transform.scala:36, took 0.006979 s
   [druid] 2019-06-11 09:31:30,066 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216690000 ms.0 from job set of time 1560216690000 ms
   [druid] 2019-06-11 09:31:30,066 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.066 s for time 1560216690000 ms (execution: 0.060 s)
   [druid] 2019-06-11 09:31:30,067 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 834 from persistence list
   [druid] 2019-06-11 09:31:30,067 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 834
   [druid] 2019-06-11 09:31:30,067 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 829 from persistence list
   [druid] 2019-06-11 09:31:30,067 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 829
   [druid] 2019-06-11 09:31:30,067 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 828 from persistence list
   [druid] 2019-06-11 09:31:30,067 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 828
   [druid] 2019-06-11 09:31:30,067 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[828] at socketTextStream at Transform.scala:18 of time 1560216690000 ms
   [druid] 2019-06-11 09:31:30,067 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216680000 ms
   [druid] 2019-06-11 09:31:30,067 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216680000 ms
   [druid] 2019-06-11 09:31:35,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216695000 ms
   [druid] 2019-06-11 09:31:35,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216695000 ms.0 from job set of time 1560216695000 ms
   [druid] 2019-06-11 09:31:35,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:31:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 843 (map at Transform.scala:20)
   [druid] 2019-06-11 09:31:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 241 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 723 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 721, ShuffleMapStage 722)
   [druid] 2019-06-11 09:31:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 721)
   [druid] 2019-06-11 09:31:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 721 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:31:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_361 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_361_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:31:35,010 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_361_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:35,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 361 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:35,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 721 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:31:35,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 721.0 with 2 tasks
   [druid] 2019-06-11 09:31:35,010 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 721.0 (TID 481, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:35,010 [er for task 481] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 721.0 (TID 481)
   [druid] 2019-06-11 09:31:35,022 [er for task 481] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 721.0 (TID 481). 853 bytes result sent to driver
   [druid] 2019-06-11 09:31:35,022 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 721.0 (TID 482, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:35,022 [er for task 482] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 721.0 (TID 482)
   [druid] 2019-06-11 09:31:35,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 721.0 (TID 481) in 12 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:31:35,030 [er for task 482] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 721.0 (TID 482). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:35,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 721.0 (TID 482) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:31:35,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 721.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 721 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:31:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:31:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:31:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 723)
   [druid] 2019-06-11 09:31:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:31:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 723 (MapPartitionsRDD[848] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:35,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_362 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:35,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_362_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:35,032 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_362_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:35,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 362 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 723 (MapPartitionsRDD[848] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:31:35,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 723.0 with 1 tasks
   [druid] 2019-06-11 09:31:35,032 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 723.0 (TID 483, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:35,033 [er for task 483] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 723.0 (TID 483)
   [druid] 2019-06-11 09:31:35,033 [er for task 483] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:35,033 [er for task 483] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:35,033 [er for task 483] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:35,033 [er for task 483] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:35,034 [er for task 483] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 723.0 (TID 483). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:35,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 723.0 (TID 483) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:35,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 723.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 723 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:35,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 241 finished: print at Transform.scala:36, took 0.026666 s
   [druid] 2019-06-11 09:31:35,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:35,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 241 is 160 bytes
   [druid] 2019-06-11 09:31:35,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 240 is 83 bytes
   [druid] 2019-06-11 09:31:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 242 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 726 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 724, ShuffleMapStage 725)
   [druid] 2019-06-11 09:31:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:31:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 726 (MapPartitionsRDD[848] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:35,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_363 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:35,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_363_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:35,039 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_363_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:35,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 363 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 726 (MapPartitionsRDD[848] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:31:35,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 726.0 with 1 tasks
   [druid] 2019-06-11 09:31:35,040 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 726.0 (TID 484, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:35,040 [er for task 484] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 726.0 (TID 484)
   [druid] 2019-06-11 09:31:35,041 [er for task 484] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:35,041 [er for task 484] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:35,041 [er for task 484] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:35,041 [er for task 484] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:35,041 [er for task 484] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 726.0 (TID 484). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:35,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 726.0 (TID 484) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:35,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 726.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:35,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 726 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:35,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 242 finished: print at Transform.scala:36, took 0.005325 s
   [druid] 2019-06-11 09:31:35,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216695000 ms.0 from job set of time 1560216695000 ms
   [druid] 2019-06-11 09:31:35,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560216695000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:31:35,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 841 from persistence list
   [druid] 2019-06-11 09:31:35,043 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 841
   [druid] 2019-06-11 09:31:35,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 836 from persistence list
   [druid] 2019-06-11 09:31:35,043 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 836
   [druid] 2019-06-11 09:31:35,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 835 from persistence list
   [druid] 2019-06-11 09:31:35,043 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 835
   [druid] 2019-06-11 09:31:35,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[835] at socketTextStream at Transform.scala:18 of time 1560216695000 ms
   [druid] 2019-06-11 09:31:35,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216685000 ms
   [druid] 2019-06-11 09:31:35,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216685000 ms
   [druid] 2019-06-11 09:31:40,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216700000 ms
   [druid] 2019-06-11 09:31:40,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216700000 ms.0 from job set of time 1560216700000 ms
   [druid] 2019-06-11 09:31:40,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 850 (map at Transform.scala:20)
   [druid] 2019-06-11 09:31:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:31:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 243 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 729 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 727, ShuffleMapStage 728)
   [druid] 2019-06-11 09:31:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 728)
   [druid] 2019-06-11 09:31:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 728 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:31:40,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_364 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:40,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_364_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:31:40,009 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_364_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:40,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 364 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 728 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:31:40,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 728.0 with 2 tasks
   [druid] 2019-06-11 09:31:40,009 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 728.0 (TID 485, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:40,009 [er for task 485] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 728.0 (TID 485)
   [druid] 2019-06-11 09:31:40,019 [er for task 485] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 728.0 (TID 485). 853 bytes result sent to driver
   [druid] 2019-06-11 09:31:40,020 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 728.0 (TID 486, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:40,020 [er for task 486] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 728.0 (TID 486)
   [druid] 2019-06-11 09:31:40,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 728.0 (TID 485) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:31:40,027 [er for task 486] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 728.0 (TID 486). 767 bytes result sent to driver
   [druid] 2019-06-11 09:31:40,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 728.0 (TID 486) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:31:40,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 728.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 728 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:31:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:31:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:31:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 729)
   [druid] 2019-06-11 09:31:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:31:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 729 (MapPartitionsRDD[855] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:40,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_365 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:40,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_365_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:40,029 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_365_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:40,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 365 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 729 (MapPartitionsRDD[855] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:31:40,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 729.0 with 1 tasks
   [druid] 2019-06-11 09:31:40,030 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 729.0 (TID 487, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:40,030 [er for task 487] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 729.0 (TID 487)
   [druid] 2019-06-11 09:31:40,031 [er for task 487] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:40,031 [er for task 487] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:40,031 [er for task 487] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:40,031 [er for task 487] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:40,031 [er for task 487] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 729.0 (TID 487). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:31:40,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 729.0 (TID 487) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:40,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 729.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 729 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:40,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 243 finished: print at Transform.scala:36, took 0.025071 s
   [druid] 2019-06-11 09:31:40,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:40,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 242 is 83 bytes
   [druid] 2019-06-11 09:31:40,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 243 is 160 bytes
   [druid] 2019-06-11 09:31:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 244 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 732 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 730, ShuffleMapStage 731)
   [druid] 2019-06-11 09:31:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:31:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 732 (MapPartitionsRDD[855] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:40,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_366 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:40,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_366_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:31:40,037 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_366_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:40,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 366 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:40,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 732 (MapPartitionsRDD[855] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:31:40,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 732.0 with 1 tasks
   [druid] 2019-06-11 09:31:40,038 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 732.0 (TID 488, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:40,038 [er for task 488] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 732.0 (TID 488)
   [druid] 2019-06-11 09:31:40,038 [er for task 488] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:40,038 [er for task 488] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:40,039 [er for task 488] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:40,039 [er for task 488] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:40,039 [er for task 488] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 732.0 (TID 488). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:40,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 732.0 (TID 488) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:40,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 732.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:40,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 732 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:31:40,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 244 finished: print at Transform.scala:36, took 0.005400 s
   [druid] 2019-06-11 09:31:40,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216700000 ms.0 from job set of time 1560216700000 ms
   [druid] 2019-06-11 09:31:40,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560216700000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:31:40,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 848 from persistence list
   [druid] 2019-06-11 09:31:40,040 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 848
   [druid] 2019-06-11 09:31:40,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 843 from persistence list
   [druid] 2019-06-11 09:31:40,040 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 843
   [druid] 2019-06-11 09:31:40,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 842 from persistence list
   [druid] 2019-06-11 09:31:40,041 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 842
   [druid] 2019-06-11 09:31:40,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[842] at socketTextStream at Transform.scala:18 of time 1560216700000 ms
   [druid] 2019-06-11 09:31:40,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216690000 ms
   [druid] 2019-06-11 09:31:40,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216690000 ms
   [druid] 2019-06-11 09:31:45,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216705000 ms
   [druid] 2019-06-11 09:31:45,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216705000 ms.0 from job set of time 1560216705000 ms
   [druid] 2019-06-11 09:31:45,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:31:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 857 (map at Transform.scala:20)
   [druid] 2019-06-11 09:31:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 245 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 735 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 734, ShuffleMapStage 733)
   [druid] 2019-06-11 09:31:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 733)
   [druid] 2019-06-11 09:31:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 733 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:31:45,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_367 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:45,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_367_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:31:45,010 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_367_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:45,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 367 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:45,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 733 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:31:45,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 733.0 with 2 tasks
   [druid] 2019-06-11 09:31:45,011 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 733.0 (TID 489, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:45,011 [er for task 489] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 733.0 (TID 489)
   [druid] 2019-06-11 09:31:45,020 [er for task 489] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 733.0 (TID 489). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:45,021 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 733.0 (TID 490, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:45,021 [er for task 490] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 733.0 (TID 490)
   [druid] 2019-06-11 09:31:45,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 733.0 (TID 489) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:31:45,029 [er for task 490] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 733.0 (TID 490). 767 bytes result sent to driver
   [druid] 2019-06-11 09:31:45,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 733.0 (TID 490) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:31:45,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 733.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 733 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:31:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:31:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:31:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 735)
   [druid] 2019-06-11 09:31:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:31:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 735 (MapPartitionsRDD[862] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:45,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_368 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:45,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_368_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:45,031 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_368_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:45,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 368 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:45,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 735 (MapPartitionsRDD[862] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:31:45,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 735.0 with 1 tasks
   [druid] 2019-06-11 09:31:45,032 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 735.0 (TID 491, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:45,032 [er for task 491] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 735.0 (TID 491)
   [druid] 2019-06-11 09:31:45,033 [er for task 491] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:45,033 [er for task 491] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:45,033 [er for task 491] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:45,033 [er for task 491] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:45,034 [er for task 491] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 735.0 (TID 491). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:45,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 735.0 (TID 491) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:45,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 735.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 735 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:45,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 245 finished: print at Transform.scala:36, took 0.025696 s
   [druid] 2019-06-11 09:31:45,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:45,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 245 is 160 bytes
   [druid] 2019-06-11 09:31:45,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 244 is 83 bytes
   [druid] 2019-06-11 09:31:45,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 246 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:45,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 738 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:45,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 736, ShuffleMapStage 737)
   [druid] 2019-06-11 09:31:45,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:31:45,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 738 (MapPartitionsRDD[862] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:45,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_369 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:45,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_369_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:45,039 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_369_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:45,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 369 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:45,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 738 (MapPartitionsRDD[862] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:31:45,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 738.0 with 1 tasks
   [druid] 2019-06-11 09:31:45,039 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 738.0 (TID 492, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:45,039 [er for task 492] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 738.0 (TID 492)
   [druid] 2019-06-11 09:31:45,040 [er for task 492] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:45,040 [er for task 492] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:45,040 [er for task 492] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:45,040 [er for task 492] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:45,041 [er for task 492] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 738.0 (TID 492). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:31:45,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 738.0 (TID 492) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:45,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 738.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:45,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 738 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:45,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 246 finished: print at Transform.scala:36, took 0.004555 s
   [druid] 2019-06-11 09:31:45,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216705000 ms.0 from job set of time 1560216705000 ms
   [druid] 2019-06-11 09:31:45,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560216705000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:31:45,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 855 from persistence list
   [druid] 2019-06-11 09:31:45,042 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 855
   [druid] 2019-06-11 09:31:45,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 850 from persistence list
   [druid] 2019-06-11 09:31:45,042 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 850
   [druid] 2019-06-11 09:31:45,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 849 from persistence list
   [druid] 2019-06-11 09:31:45,042 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 849
   [druid] 2019-06-11 09:31:45,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[849] at socketTextStream at Transform.scala:18 of time 1560216705000 ms
   [druid] 2019-06-11 09:31:45,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216695000 ms
   [druid] 2019-06-11 09:31:45,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216695000 ms
   [druid] 2019-06-11 09:31:50,010 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216710000 ms
   [druid] 2019-06-11 09:31:50,010 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216710000 ms.0 from job set of time 1560216710000 ms
   [druid] 2019-06-11 09:31:50,012 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:50,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 864 (map at Transform.scala:20)
   [druid] 2019-06-11 09:31:50,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:31:50,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 247 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:50,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 741 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:50,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 739, ShuffleMapStage 740)
   [druid] 2019-06-11 09:31:50,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 740)
   [druid] 2019-06-11 09:31:50,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 740 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:31:50,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_370 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:50,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_370_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:31:50,014 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_370_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:50,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 370 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:50,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 740 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:31:50,014 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 740.0 with 2 tasks
   [druid] 2019-06-11 09:31:50,014 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 740.0 (TID 493, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:50,015 [er for task 493] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 740.0 (TID 493)
   [druid] 2019-06-11 09:31:50,024 [er for task 493] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 740.0 (TID 493). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:50,024 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 740.0 (TID 494, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:50,024 [er for task 494] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 740.0 (TID 494)
   [druid] 2019-06-11 09:31:50,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 740.0 (TID 493) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:31:50,039 [er for task 494] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 740.0 (TID 494). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:50,040 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 740.0 (TID 494) in 16 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:31:50,040 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 740.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:50,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 740 (parallelize at Transform.scala:16) finished in 0.026 s
   [druid] 2019-06-11 09:31:50,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:31:50,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:31:50,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 741)
   [druid] 2019-06-11 09:31:50,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:31:50,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 741 (MapPartitionsRDD[869] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:50,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_371 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:50,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_371_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:50,041 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_371_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:50,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 371 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:50,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 741 (MapPartitionsRDD[869] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:31:50,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 741.0 with 1 tasks
   [druid] 2019-06-11 09:31:50,043 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 741.0 (TID 495, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:50,043 [er for task 495] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 741.0 (TID 495)
   [druid] 2019-06-11 09:31:50,044 [er for task 495] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:50,044 [er for task 495] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:50,044 [er for task 495] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:50,044 [er for task 495] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:50,045 [er for task 495] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 741.0 (TID 495). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:50,045 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 741.0 (TID 495) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:50,045 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 741.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:50,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 741 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:50,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 247 finished: print at Transform.scala:36, took 0.032700 s
   [druid] 2019-06-11 09:31:50,047 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:50,047 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 246 is 83 bytes
   [druid] 2019-06-11 09:31:50,047 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 247 is 160 bytes
   [druid] 2019-06-11 09:31:50,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 248 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:50,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 744 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:50,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 742, ShuffleMapStage 743)
   [druid] 2019-06-11 09:31:50,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:31:50,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 744 (MapPartitionsRDD[869] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:50,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_372 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:50,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_372_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:50,050 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_372_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:50,050 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 372 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:50,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 744 (MapPartitionsRDD[869] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:31:50,050 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 744.0 with 1 tasks
   [druid] 2019-06-11 09:31:50,051 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 744.0 (TID 496, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:50,051 [er for task 496] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 744.0 (TID 496)
   [druid] 2019-06-11 09:31:50,051 [er for task 496] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:50,051 [er for task 496] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:50,051 [er for task 496] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:50,051 [er for task 496] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:50,052 [er for task 496] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 744.0 (TID 496). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:50,052 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 744.0 (TID 496) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:50,052 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 744.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:50,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 744 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:50,053 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 248 finished: print at Transform.scala:36, took 0.005629 s
   [druid] 2019-06-11 09:31:50,053 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216710000 ms.0 from job set of time 1560216710000 ms
   [druid] 2019-06-11 09:31:50,053 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.053 s for time 1560216710000 ms (execution: 0.043 s)
   [druid] 2019-06-11 09:31:50,053 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 862 from persistence list
   [druid] 2019-06-11 09:31:50,053 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 862
   [druid] 2019-06-11 09:31:50,053 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 857 from persistence list
   [druid] 2019-06-11 09:31:50,053 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 857
   [druid] 2019-06-11 09:31:50,054 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 856 from persistence list
   [druid] 2019-06-11 09:31:50,054 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 856
   [druid] 2019-06-11 09:31:50,054 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[856] at socketTextStream at Transform.scala:18 of time 1560216710000 ms
   [druid] 2019-06-11 09:31:50,054 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216700000 ms
   [druid] 2019-06-11 09:31:50,054 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216700000 ms
   [druid] 2019-06-11 09:31:55,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216715000 ms
   [druid] 2019-06-11 09:31:55,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216715000 ms.0 from job set of time 1560216715000 ms
   [druid] 2019-06-11 09:31:55,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 871 (map at Transform.scala:20)
   [druid] 2019-06-11 09:31:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:31:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 249 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 747 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 745, ShuffleMapStage 746)
   [druid] 2019-06-11 09:31:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 746)
   [druid] 2019-06-11 09:31:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 746 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:31:55,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_373 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:55,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_373_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:31:55,010 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_373_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:55,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 373 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 746 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:31:55,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 746.0 with 2 tasks
   [druid] 2019-06-11 09:31:55,010 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 746.0 (TID 497, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:55,010 [er for task 497] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 746.0 (TID 497)
   [druid] 2019-06-11 09:31:55,019 [er for task 497] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 746.0 (TID 497). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:55,019 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 746.0 (TID 498, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:31:55,019 [er for task 498] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 746.0 (TID 498)
   [druid] 2019-06-11 09:31:55,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 746.0 (TID 497) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:31:55,027 [er for task 498] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 746.0 (TID 498). 810 bytes result sent to driver
   [druid] 2019-06-11 09:31:55,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 746.0 (TID 498) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:31:55,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 746.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 746 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:31:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:31:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:31:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 747)
   [druid] 2019-06-11 09:31:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:31:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 747 (MapPartitionsRDD[876] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:55,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_374 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:55,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_374_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:55,029 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_374_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:55,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 374 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:55,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 747 (MapPartitionsRDD[876] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:31:55,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 747.0 with 1 tasks
   [druid] 2019-06-11 09:31:55,030 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 747.0 (TID 499, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:55,030 [er for task 499] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 747.0 (TID 499)
   [druid] 2019-06-11 09:31:55,031 [er for task 499] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:55,031 [er for task 499] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:55,031 [er for task 499] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:55,031 [er for task 499] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:55,031 [er for task 499] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 747.0 (TID 499). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:31:55,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 747.0 (TID 499) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:55,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 747.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:55,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 747 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:55,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 249 finished: print at Transform.scala:36, took 0.024067 s
   [druid] 2019-06-11 09:31:55,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:31:55,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 248 is 83 bytes
   [druid] 2019-06-11 09:31:55,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 249 is 160 bytes
   [druid] 2019-06-11 09:31:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 250 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:31:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 750 (print at Transform.scala:36)
   [druid] 2019-06-11 09:31:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 748, ShuffleMapStage 749)
   [druid] 2019-06-11 09:31:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:31:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 750 (MapPartitionsRDD[876] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:31:55,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_375 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:55,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_375_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:31:55,037 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_375_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:31:55,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 375 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:31:55,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 750 (MapPartitionsRDD[876] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:31:55,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 750.0 with 1 tasks
   [druid] 2019-06-11 09:31:55,038 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 750.0 (TID 500, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:31:55,038 [er for task 500] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 750.0 (TID 500)
   [druid] 2019-06-11 09:31:55,039 [er for task 500] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:31:55,039 [er for task 500] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:55,039 [er for task 500] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:31:55,039 [er for task 500] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:31:55,039 [er for task 500] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 750.0 (TID 500). 966 bytes result sent to driver
   [druid] 2019-06-11 09:31:55,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 750.0 (TID 500) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:31:55,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 750.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:31:55,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 750 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:31:55,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 250 finished: print at Transform.scala:36, took 0.005957 s
   [druid] 2019-06-11 09:31:55,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216715000 ms.0 from job set of time 1560216715000 ms
   [druid] 2019-06-11 09:31:55,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560216715000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:31:55,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 869 from persistence list
   [druid] 2019-06-11 09:31:55,040 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 869
   [druid] 2019-06-11 09:31:55,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 864 from persistence list
   [druid] 2019-06-11 09:31:55,041 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 864
   [druid] 2019-06-11 09:31:55,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 863 from persistence list
   [druid] 2019-06-11 09:31:55,041 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 863
   [druid] 2019-06-11 09:31:55,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[863] at socketTextStream at Transform.scala:18 of time 1560216715000 ms
   [druid] 2019-06-11 09:31:55,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216705000 ms
   [druid] 2019-06-11 09:31:55,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216705000 ms
   [druid] 2019-06-11 09:32:00,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216720000 ms
   [druid] 2019-06-11 09:32:00,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216720000 ms.0 from job set of time 1560216720000 ms
   [druid] 2019-06-11 09:32:00,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:32:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 878 (map at Transform.scala:20)
   [druid] 2019-06-11 09:32:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 251 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 753 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 751, ShuffleMapStage 752)
   [druid] 2019-06-11 09:32:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 751)
   [druid] 2019-06-11 09:32:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 751 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:32:00,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_376 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:00,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_376_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:32:00,012 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_376_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 376 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 751 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:32:00,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 751.0 with 2 tasks
   [druid] 2019-06-11 09:32:00,013 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 751.0 (TID 501, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:00,013 [er for task 501] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 751.0 (TID 501)
   [druid] 2019-06-11 09:32:00,022 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_360_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 241
   [druid] 2019-06-11 09:32:00,025 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_370_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,025 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_372_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,026 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_368_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,027 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 243
   [druid] 2019-06-11 09:32:00,027 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 247
   [druid] 2019-06-11 09:32:00,028 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_364_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,029 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_369_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,030 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_375_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,030 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 245
   [druid] 2019-06-11 09:32:00,031 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_365_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,032 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_358_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,032 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 237
   [druid] 2019-06-11 09:32:00,034 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_371_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,034 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_366_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,035 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_374_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,035 [er for task 501] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 751.0 (TID 501). 853 bytes result sent to driver
   [druid] 2019-06-11 09:32:00,036 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_359_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,036 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 751.0 (TID 502, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:00,036 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 751.0 (TID 501) in 23 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:32:00,036 [er for task 502] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 751.0 (TID 502)
   [druid] 2019-06-11 09:32:00,036 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_361_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,037 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_363_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,038 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 239
   [druid] 2019-06-11 09:32:00,038 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 240
   [druid] 2019-06-11 09:32:00,038 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 236
   [druid] 2019-06-11 09:32:00,038 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 244
   [druid] 2019-06-11 09:32:00,038 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_367_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,040 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_362_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,040 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 242
   [druid] 2019-06-11 09:32:00,040 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 246
   [druid] 2019-06-11 09:32:00,040 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 238
   [druid] 2019-06-11 09:32:00,041 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_373_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,046 [er for task 502] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 751.0 (TID 502). 810 bytes result sent to driver
   [druid] 2019-06-11 09:32:00,047 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 751.0 (TID 502) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:32:00,047 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 751.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:00,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 751 (parallelize at Transform.scala:16) finished in 0.034 s
   [druid] 2019-06-11 09:32:00,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:32:00,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:32:00,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 753)
   [druid] 2019-06-11 09:32:00,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:32:00,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 753 (MapPartitionsRDD[883] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:00,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_377 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:00,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_377_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:00,049 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_377_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,049 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 377 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:00,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 753 (MapPartitionsRDD[883] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:32:00,050 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 753.0 with 1 tasks
   [druid] 2019-06-11 09:32:00,050 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 753.0 (TID 503, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:00,050 [er for task 503] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 753.0 (TID 503)
   [druid] 2019-06-11 09:32:00,051 [er for task 503] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:00,051 [er for task 503] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:00,051 [er for task 503] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:00,051 [er for task 503] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:00,052 [er for task 503] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 753.0 (TID 503). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:00,052 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 753.0 (TID 503) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:00,052 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 753.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:00,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 753 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:00,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 251 finished: print at Transform.scala:36, took 0.042100 s
   [druid] 2019-06-11 09:32:00,054 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:00,054 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 251 is 160 bytes
   [druid] 2019-06-11 09:32:00,055 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 250 is 83 bytes
   [druid] 2019-06-11 09:32:00,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 252 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:00,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 756 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:00,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 754, ShuffleMapStage 755)
   [druid] 2019-06-11 09:32:00,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:32:00,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 756 (MapPartitionsRDD[883] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:00,055 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_378 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:00,056 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_378_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:00,056 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_378_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:00,057 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 378 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:00,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 756 (MapPartitionsRDD[883] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:32:00,057 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 756.0 with 1 tasks
   [druid] 2019-06-11 09:32:00,057 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 756.0 (TID 504, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:00,057 [er for task 504] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 756.0 (TID 504)
   [druid] 2019-06-11 09:32:00,058 [er for task 504] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:00,058 [er for task 504] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:00,058 [er for task 504] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:00,058 [er for task 504] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:00,058 [er for task 504] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 756.0 (TID 504). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:00,059 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 756.0 (TID 504) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:00,059 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 756.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:00,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 756 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:00,059 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 252 finished: print at Transform.scala:36, took 0.004723 s
   [druid] 2019-06-11 09:32:00,059 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216720000 ms.0 from job set of time 1560216720000 ms
   [druid] 2019-06-11 09:32:00,059 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.059 s for time 1560216720000 ms (execution: 0.051 s)
   [druid] 2019-06-11 09:32:00,059 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 876 from persistence list
   [druid] 2019-06-11 09:32:00,060 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 876
   [druid] 2019-06-11 09:32:00,060 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 871 from persistence list
   [druid] 2019-06-11 09:32:00,060 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 871
   [druid] 2019-06-11 09:32:00,060 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 870 from persistence list
   [druid] 2019-06-11 09:32:00,060 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 870
   [druid] 2019-06-11 09:32:00,060 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[870] at socketTextStream at Transform.scala:18 of time 1560216720000 ms
   [druid] 2019-06-11 09:32:00,060 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216710000 ms
   [druid] 2019-06-11 09:32:00,060 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216710000 ms
   [druid] 2019-06-11 09:32:05,009 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216725000 ms
   [druid] 2019-06-11 09:32:05,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216725000 ms.0 from job set of time 1560216725000 ms
   [druid] 2019-06-11 09:32:05,011 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:32:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 885 (map at Transform.scala:20)
   [druid] 2019-06-11 09:32:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 253 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 759 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 757, ShuffleMapStage 758)
   [druid] 2019-06-11 09:32:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 757)
   [druid] 2019-06-11 09:32:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 757 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:32:05,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_379 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:05,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_379_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:32:05,013 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_379_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:05,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 379 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:05,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 757 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:32:05,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 757.0 with 2 tasks
   [druid] 2019-06-11 09:32:05,013 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 757.0 (TID 505, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:05,014 [er for task 505] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 757.0 (TID 505)
   [druid] 2019-06-11 09:32:05,026 [er for task 505] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 757.0 (TID 505). 853 bytes result sent to driver
   [druid] 2019-06-11 09:32:05,026 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 757.0 (TID 506, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:05,027 [er for task 506] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 757.0 (TID 506)
   [druid] 2019-06-11 09:32:05,027 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 757.0 (TID 505) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:32:05,035 [er for task 506] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 757.0 (TID 506). 810 bytes result sent to driver
   [druid] 2019-06-11 09:32:05,035 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 757.0 (TID 506) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:32:05,035 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 757.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 757 (parallelize at Transform.scala:16) finished in 0.022 s
   [druid] 2019-06-11 09:32:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:32:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:32:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 759)
   [druid] 2019-06-11 09:32:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:32:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 759 (MapPartitionsRDD[890] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:05,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_380 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:05,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_380_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:05,037 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_380_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:05,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 380 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:05,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 759 (MapPartitionsRDD[890] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:32:05,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 759.0 with 1 tasks
   [druid] 2019-06-11 09:32:05,037 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 759.0 (TID 507, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:05,037 [er for task 507] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 759.0 (TID 507)
   [druid] 2019-06-11 09:32:05,038 [er for task 507] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:05,038 [er for task 507] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:05,038 [er for task 507] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:05,038 [er for task 507] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:05,039 [er for task 507] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 759.0 (TID 507). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:05,039 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 759.0 (TID 507) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:05,039 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 759.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 759 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:05,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 253 finished: print at Transform.scala:36, took 0.028926 s
   [druid] 2019-06-11 09:32:05,042 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:05,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 253 is 160 bytes
   [druid] 2019-06-11 09:32:05,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 252 is 83 bytes
   [druid] 2019-06-11 09:32:05,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 254 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:05,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 762 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:05,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 760, ShuffleMapStage 761)
   [druid] 2019-06-11 09:32:05,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:32:05,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 762 (MapPartitionsRDD[890] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:05,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_381 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:05,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_381_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:05,044 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_381_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:05,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 381 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:05,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 762 (MapPartitionsRDD[890] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:32:05,045 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 762.0 with 1 tasks
   [druid] 2019-06-11 09:32:05,045 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 762.0 (TID 508, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:05,045 [er for task 508] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 762.0 (TID 508)
   [druid] 2019-06-11 09:32:05,046 [er for task 508] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:05,046 [er for task 508] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:05,046 [er for task 508] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:05,046 [er for task 508] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:05,047 [er for task 508] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 762.0 (TID 508). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:32:05,047 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 762.0 (TID 508) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:05,047 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 762.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:05,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 762 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:05,047 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 254 finished: print at Transform.scala:36, took 0.005123 s
   [druid] 2019-06-11 09:32:05,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216725000 ms.0 from job set of time 1560216725000 ms
   [druid] 2019-06-11 09:32:05,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.047 s for time 1560216725000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:32:05,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 883 from persistence list
   [druid] 2019-06-11 09:32:05,048 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 883
   [druid] 2019-06-11 09:32:05,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 878 from persistence list
   [druid] 2019-06-11 09:32:05,048 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 878
   [druid] 2019-06-11 09:32:05,048 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 877 from persistence list
   [druid] 2019-06-11 09:32:05,048 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 877
   [druid] 2019-06-11 09:32:05,048 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[877] at socketTextStream at Transform.scala:18 of time 1560216725000 ms
   [druid] 2019-06-11 09:32:05,048 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216715000 ms
   [druid] 2019-06-11 09:32:05,048 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216715000 ms
   [druid] 2019-06-11 09:32:10,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216730000 ms
   [druid] 2019-06-11 09:32:10,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216730000 ms.0 from job set of time 1560216730000 ms
   [druid] 2019-06-11 09:32:10,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 892 (map at Transform.scala:20)
   [druid] 2019-06-11 09:32:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:32:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 255 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 765 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 763, ShuffleMapStage 764)
   [druid] 2019-06-11 09:32:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 764)
   [druid] 2019-06-11 09:32:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 764 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:32:10,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_382 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:10,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_382_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:32:10,012 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_382_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:10,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 382 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 764 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:32:10,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 764.0 with 2 tasks
   [druid] 2019-06-11 09:32:10,013 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 764.0 (TID 509, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:10,013 [er for task 509] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 764.0 (TID 509)
   [druid] 2019-06-11 09:32:10,022 [er for task 509] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 764.0 (TID 509). 810 bytes result sent to driver
   [druid] 2019-06-11 09:32:10,022 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 764.0 (TID 510, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:10,022 [er for task 510] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 764.0 (TID 510)
   [druid] 2019-06-11 09:32:10,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 764.0 (TID 509) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:32:10,031 [er for task 510] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 764.0 (TID 510). 810 bytes result sent to driver
   [druid] 2019-06-11 09:32:10,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 764.0 (TID 510) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:32:10,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 764.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 764 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:32:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:32:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:32:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 765)
   [druid] 2019-06-11 09:32:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:32:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 765 (MapPartitionsRDD[897] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:10,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_383 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:10,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_383_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:10,033 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_383_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:10,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 383 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 765 (MapPartitionsRDD[897] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:32:10,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 765.0 with 1 tasks
   [druid] 2019-06-11 09:32:10,034 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 765.0 (TID 511, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:10,035 [er for task 511] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 765.0 (TID 511)
   [druid] 2019-06-11 09:32:10,036 [er for task 511] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:10,036 [er for task 511] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:10,036 [er for task 511] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:10,037 [er for task 511] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:32:10,037 [er for task 511] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 765.0 (TID 511). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:10,038 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 765.0 (TID 511) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:10,038 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 765.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 765 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:32:10,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 255 finished: print at Transform.scala:36, took 0.028858 s
   [druid] 2019-06-11 09:32:10,040 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:10,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 254 is 83 bytes
   [druid] 2019-06-11 09:32:10,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 255 is 160 bytes
   [druid] 2019-06-11 09:32:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 256 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 768 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 767, ShuffleMapStage 766)
   [druid] 2019-06-11 09:32:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:32:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 768 (MapPartitionsRDD[897] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:10,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_384 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:10,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_384_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:10,043 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_384_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:10,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 384 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 768 (MapPartitionsRDD[897] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:32:10,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 768.0 with 1 tasks
   [druid] 2019-06-11 09:32:10,044 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 768.0 (TID 512, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:10,044 [er for task 512] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 768.0 (TID 512)
   [druid] 2019-06-11 09:32:10,045 [er for task 512] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:10,045 [er for task 512] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:10,045 [er for task 512] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:10,045 [er for task 512] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:10,046 [er for task 512] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 768.0 (TID 512). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:32:10,047 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 768.0 (TID 512) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:10,047 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 768.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:10,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 768 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:32:10,047 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 256 finished: print at Transform.scala:36, took 0.006750 s
   [druid] 2019-06-11 09:32:10,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216730000 ms.0 from job set of time 1560216730000 ms
   [druid] 2019-06-11 09:32:10,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.048 s for time 1560216730000 ms (execution: 0.041 s)
   [druid] 2019-06-11 09:32:10,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 890 from persistence list
   [druid] 2019-06-11 09:32:10,048 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 890
   [druid] 2019-06-11 09:32:10,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 885 from persistence list
   [druid] 2019-06-11 09:32:10,048 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 885
   [druid] 2019-06-11 09:32:10,048 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 884 from persistence list
   [druid] 2019-06-11 09:32:10,048 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 884
   [druid] 2019-06-11 09:32:10,048 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[884] at socketTextStream at Transform.scala:18 of time 1560216730000 ms
   [druid] 2019-06-11 09:32:10,049 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216720000 ms
   [druid] 2019-06-11 09:32:10,049 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216720000 ms
   [druid] 2019-06-11 09:32:15,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216735000 ms
   [druid] 2019-06-11 09:32:15,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216735000 ms.0 from job set of time 1560216735000 ms
   [druid] 2019-06-11 09:32:15,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:32:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 899 (map at Transform.scala:20)
   [druid] 2019-06-11 09:32:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 257 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 771 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 769, ShuffleMapStage 770)
   [druid] 2019-06-11 09:32:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 769)
   [druid] 2019-06-11 09:32:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 769 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:32:15,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_385 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:15,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_385_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:32:15,010 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_385_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:15,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 385 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:15,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 769 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:32:15,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 769.0 with 2 tasks
   [druid] 2019-06-11 09:32:15,011 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 769.0 (TID 513, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:15,011 [er for task 513] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 769.0 (TID 513)
   [druid] 2019-06-11 09:32:15,021 [er for task 513] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 769.0 (TID 513). 853 bytes result sent to driver
   [druid] 2019-06-11 09:32:15,021 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 769.0 (TID 514, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:15,021 [er for task 514] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 769.0 (TID 514)
   [druid] 2019-06-11 09:32:15,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 769.0 (TID 513) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:32:15,029 [er for task 514] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 769.0 (TID 514). 767 bytes result sent to driver
   [druid] 2019-06-11 09:32:15,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 769.0 (TID 514) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:32:15,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 769.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:15,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 769 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:32:15,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:32:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:32:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 771)
   [druid] 2019-06-11 09:32:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:32:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 771 (MapPartitionsRDD[904] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:15,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_386 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:15,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_386_piece0 stored as bytes in memory (estimated size 2041.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:32:15,031 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_386_piece0 in memory on 192.168.91.1:63518 (size: 2041.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:15,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 386 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:15,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 771 (MapPartitionsRDD[904] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:32:15,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 771.0 with 1 tasks
   [druid] 2019-06-11 09:32:15,032 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 771.0 (TID 515, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:15,032 [er for task 515] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 771.0 (TID 515)
   [druid] 2019-06-11 09:32:15,033 [er for task 515] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:15,033 [er for task 515] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:15,033 [er for task 515] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:15,033 [er for task 515] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:15,033 [er for task 515] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 771.0 (TID 515). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:15,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 771.0 (TID 515) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:15,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 771.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 771 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:15,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 257 finished: print at Transform.scala:36, took 0.025746 s
   [druid] 2019-06-11 09:32:15,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:15,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 257 is 160 bytes
   [druid] 2019-06-11 09:32:15,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 256 is 83 bytes
   [druid] 2019-06-11 09:32:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 258 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 774 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 772, ShuffleMapStage 773)
   [druid] 2019-06-11 09:32:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:32:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 774 (MapPartitionsRDD[904] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:15,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_387 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:15,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_387_piece0 stored as bytes in memory (estimated size 2041.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:32:15,039 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_387_piece0 in memory on 192.168.91.1:63518 (size: 2041.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:15,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 387 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 774 (MapPartitionsRDD[904] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:32:15,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 774.0 with 1 tasks
   [druid] 2019-06-11 09:32:15,040 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 774.0 (TID 516, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:15,040 [er for task 516] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 774.0 (TID 516)
   [druid] 2019-06-11 09:32:15,041 [er for task 516] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:15,041 [er for task 516] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:15,041 [er for task 516] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:15,041 [er for task 516] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:15,042 [er for task 516] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 774.0 (TID 516). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:32:15,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 774.0 (TID 516) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:15,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 774.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 774 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:15,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 258 finished: print at Transform.scala:36, took 0.005956 s
   [druid] 2019-06-11 09:32:15,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216735000 ms.0 from job set of time 1560216735000 ms
   [druid] 2019-06-11 09:32:15,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560216735000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:32:15,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 897 from persistence list
   [druid] 2019-06-11 09:32:15,043 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 897
   [druid] 2019-06-11 09:32:15,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 892 from persistence list
   [druid] 2019-06-11 09:32:15,043 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 892
   [druid] 2019-06-11 09:32:15,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 891 from persistence list
   [druid] 2019-06-11 09:32:15,043 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 891
   [druid] 2019-06-11 09:32:15,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[891] at socketTextStream at Transform.scala:18 of time 1560216735000 ms
   [druid] 2019-06-11 09:32:15,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216725000 ms
   [druid] 2019-06-11 09:32:15,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216725000 ms
   [druid] 2019-06-11 09:32:20,010 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216740000 ms
   [druid] 2019-06-11 09:32:20,010 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216740000 ms.0 from job set of time 1560216740000 ms
   [druid] 2019-06-11 09:32:20,012 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:32:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 906 (map at Transform.scala:20)
   [druid] 2019-06-11 09:32:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 259 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 777 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 775, ShuffleMapStage 776)
   [druid] 2019-06-11 09:32:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 775)
   [druid] 2019-06-11 09:32:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 775 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:32:20,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_388 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:20,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_388_piece0 stored as bytes in memory (estimated size 1309.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:32:20,014 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_388_piece0 in memory on 192.168.91.1:63518 (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:20,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 388 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:20,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 775 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:32:20,014 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 775.0 with 2 tasks
   [druid] 2019-06-11 09:32:20,014 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 775.0 (TID 517, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:20,014 [er for task 517] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 775.0 (TID 517)
   [druid] 2019-06-11 09:32:20,023 [er for task 517] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 775.0 (TID 517). 853 bytes result sent to driver
   [druid] 2019-06-11 09:32:20,024 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 775.0 (TID 518, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:20,024 [er for task 518] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 775.0 (TID 518)
   [druid] 2019-06-11 09:32:20,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 775.0 (TID 517) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:32:20,031 [er for task 518] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 775.0 (TID 518). 810 bytes result sent to driver
   [druid] 2019-06-11 09:32:20,032 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 775.0 (TID 518) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:32:20,032 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 775.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 775 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:32:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:32:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:32:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 777)
   [druid] 2019-06-11 09:32:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:32:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 777 (MapPartitionsRDD[911] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:20,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_389 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:20,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_389_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:20,033 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_389_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:20,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 389 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:20,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 777 (MapPartitionsRDD[911] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:32:20,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 777.0 with 1 tasks
   [druid] 2019-06-11 09:32:20,034 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 777.0 (TID 519, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:20,034 [er for task 519] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 777.0 (TID 519)
   [druid] 2019-06-11 09:32:20,034 [er for task 519] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:20,035 [er for task 519] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:32:20,035 [er for task 519] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:20,035 [er for task 519] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:20,035 [er for task 519] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 777.0 (TID 519). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:20,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 777.0 (TID 519) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:20,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 777.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:20,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 777 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:20,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 259 finished: print at Transform.scala:36, took 0.024381 s
   [druid] 2019-06-11 09:32:20,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:20,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 259 is 160 bytes
   [druid] 2019-06-11 09:32:20,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 258 is 83 bytes
   [druid] 2019-06-11 09:32:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 260 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 780 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 778, ShuffleMapStage 779)
   [druid] 2019-06-11 09:32:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:32:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 780 (MapPartitionsRDD[911] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:20,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_390 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:20,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_390_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:20,041 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_390_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:20,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 390 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:20,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 780 (MapPartitionsRDD[911] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:32:20,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 780.0 with 1 tasks
   [druid] 2019-06-11 09:32:20,042 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 780.0 (TID 520, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:20,042 [er for task 520] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 780.0 (TID 520)
   [druid] 2019-06-11 09:32:20,042 [er for task 520] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:20,042 [er for task 520] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:20,043 [er for task 520] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:20,043 [er for task 520] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:32:20,043 [er for task 520] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 780.0 (TID 520). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:20,043 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 780.0 (TID 520) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:20,043 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 780.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:20,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 780 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:20,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 260 finished: print at Transform.scala:36, took 0.004654 s
   [druid] 2019-06-11 09:32:20,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216740000 ms.0 from job set of time 1560216740000 ms
   [druid] 2019-06-11 09:32:20,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.044 s for time 1560216740000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:32:20,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 904 from persistence list
   [druid] 2019-06-11 09:32:20,044 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 904
   [druid] 2019-06-11 09:32:20,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 899 from persistence list
   [druid] 2019-06-11 09:32:20,044 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 899
   [druid] 2019-06-11 09:32:20,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 898 from persistence list
   [druid] 2019-06-11 09:32:20,045 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 898
   [druid] 2019-06-11 09:32:20,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[898] at socketTextStream at Transform.scala:18 of time 1560216740000 ms
   [druid] 2019-06-11 09:32:20,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216730000 ms
   [druid] 2019-06-11 09:32:20,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216730000 ms
   [druid] 2019-06-11 09:32:25,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216745000 ms
   [druid] 2019-06-11 09:32:25,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216745000 ms.0 from job set of time 1560216745000 ms
   [druid] 2019-06-11 09:32:25,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:32:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 913 (map at Transform.scala:20)
   [druid] 2019-06-11 09:32:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 261 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 783 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 781, ShuffleMapStage 782)
   [druid] 2019-06-11 09:32:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 781)
   [druid] 2019-06-11 09:32:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 781 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:32:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_391 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:25,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_391_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:32:25,010 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_391_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:25,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 391 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 781 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:32:25,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 781.0 with 2 tasks
   [druid] 2019-06-11 09:32:25,010 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 781.0 (TID 521, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:25,011 [er for task 521] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 781.0 (TID 521)
   [druid] 2019-06-11 09:32:25,019 [er for task 521] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 781.0 (TID 521). 810 bytes result sent to driver
   [druid] 2019-06-11 09:32:25,019 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 781.0 (TID 522, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:25,020 [er for task 522] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 781.0 (TID 522)
   [druid] 2019-06-11 09:32:25,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 781.0 (TID 521) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:32:25,028 [er for task 522] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 781.0 (TID 522). 767 bytes result sent to driver
   [druid] 2019-06-11 09:32:25,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 781.0 (TID 522) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:32:25,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 781.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 781 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:32:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:32:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:32:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 783)
   [druid] 2019-06-11 09:32:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:32:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 783 (MapPartitionsRDD[918] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:25,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_392 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:25,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_392_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:25,030 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_392_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:25,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 392 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 783 (MapPartitionsRDD[918] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:32:25,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 783.0 with 1 tasks
   [druid] 2019-06-11 09:32:25,030 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 783.0 (TID 523, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:25,030 [er for task 523] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 783.0 (TID 523)
   [druid] 2019-06-11 09:32:25,031 [er for task 523] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:25,031 [er for task 523] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:25,031 [er for task 523] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:25,031 [er for task 523] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:25,032 [er for task 523] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 783.0 (TID 523). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:32:25,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 783.0 (TID 523) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:25,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 783.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 783 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:25,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 261 finished: print at Transform.scala:36, took 0.024481 s
   [druid] 2019-06-11 09:32:25,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:25,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 261 is 160 bytes
   [druid] 2019-06-11 09:32:25,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 260 is 83 bytes
   [druid] 2019-06-11 09:32:25,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 262 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:25,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 786 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:25,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 785, ShuffleMapStage 784)
   [druid] 2019-06-11 09:32:25,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:32:25,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 786 (MapPartitionsRDD[918] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:25,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_393 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:25,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_393_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:25,037 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_393_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:25,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 393 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 786 (MapPartitionsRDD[918] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:32:25,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 786.0 with 1 tasks
   [druid] 2019-06-11 09:32:25,038 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 786.0 (TID 524, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:25,038 [er for task 524] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 786.0 (TID 524)
   [druid] 2019-06-11 09:32:25,039 [er for task 524] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:25,039 [er for task 524] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:25,039 [er for task 524] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:25,039 [er for task 524] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:25,039 [er for task 524] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 786.0 (TID 524). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:25,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 786.0 (TID 524) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:25,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 786.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 786 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:25,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 262 finished: print at Transform.scala:36, took 0.004786 s
   [druid] 2019-06-11 09:32:25,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216745000 ms.0 from job set of time 1560216745000 ms
   [druid] 2019-06-11 09:32:25,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560216745000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:32:25,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 911 from persistence list
   [druid] 2019-06-11 09:32:25,040 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 911
   [druid] 2019-06-11 09:32:25,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 906 from persistence list
   [druid] 2019-06-11 09:32:25,041 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 906
   [druid] 2019-06-11 09:32:25,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 905 from persistence list
   [druid] 2019-06-11 09:32:25,041 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 905
   [druid] 2019-06-11 09:32:25,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[905] at socketTextStream at Transform.scala:18 of time 1560216745000 ms
   [druid] 2019-06-11 09:32:25,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216735000 ms
   [druid] 2019-06-11 09:32:25,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216735000 ms
   [druid] 2019-06-11 09:32:30,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216750000 ms
   [druid] 2019-06-11 09:32:30,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216750000 ms.0 from job set of time 1560216750000 ms
   [druid] 2019-06-11 09:32:30,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 920 (map at Transform.scala:20)
   [druid] 2019-06-11 09:32:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:32:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 263 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 789 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 787, ShuffleMapStage 788)
   [druid] 2019-06-11 09:32:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 788)
   [druid] 2019-06-11 09:32:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 788 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:32:30,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_394 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:30,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_394_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:32:30,013 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 251
   [druid] 2019-06-11 09:32:30,014 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_394_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,017 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 394 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:30,018 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_384_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,020 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 788 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:32:30,020 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 252
   [druid] 2019-06-11 09:32:30,020 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 788.0 with 2 tasks
   [druid] 2019-06-11 09:32:30,021 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 788.0 (TID 525, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:30,021 [er for task 525] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 788.0 (TID 525)
   [druid] 2019-06-11 09:32:30,021 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_382_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,022 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_386_piece0 on 192.168.91.1:63518 in memory (size: 2041.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,023 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_385_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,023 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 257
   [druid] 2019-06-11 09:32:30,023 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 259
   [druid] 2019-06-11 09:32:30,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 253
   [druid] 2019-06-11 09:32:30,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 258
   [druid] 2019-06-11 09:32:30,024 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_379_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,025 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_383_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,025 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 250
   [druid] 2019-06-11 09:32:30,026 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_380_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,026 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_390_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,027 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_387_piece0 on 192.168.91.1:63518 in memory (size: 2041.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,028 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_391_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,029 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_376_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,029 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 249
   [druid] 2019-06-11 09:32:30,029 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_378_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,030 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 248
   [druid] 2019-06-11 09:32:30,030 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_388_piece0 on 192.168.91.1:63518 in memory (size: 1309.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,031 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_381_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,031 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_392_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,032 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 254
   [druid] 2019-06-11 09:32:30,032 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_389_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,032 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 256
   [druid] 2019-06-11 09:32:30,033 [er for task 525] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 788.0 (TID 525). 853 bytes result sent to driver
   [druid] 2019-06-11 09:32:30,033 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_377_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,033 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 788.0 (TID 526, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:30,033 [er for task 526] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 788.0 (TID 526)
   [druid] 2019-06-11 09:32:30,033 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 788.0 (TID 525) in 12 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:32:30,033 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_393_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,034 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 255
   [druid] 2019-06-11 09:32:30,042 [er for task 526] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 788.0 (TID 526). 767 bytes result sent to driver
   [druid] 2019-06-11 09:32:30,042 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 788.0 (TID 526) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:32:30,042 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 788.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 788 (parallelize at Transform.scala:16) finished in 0.021 s
   [druid] 2019-06-11 09:32:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:32:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:32:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 789)
   [druid] 2019-06-11 09:32:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:32:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 789 (MapPartitionsRDD[925] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:30,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_395 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:30,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_395_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:30,044 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_395_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,045 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 395 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:30,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 789 (MapPartitionsRDD[925] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:32:30,045 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 789.0 with 1 tasks
   [druid] 2019-06-11 09:32:30,045 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 789.0 (TID 527, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:30,045 [er for task 527] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 789.0 (TID 527)
   [druid] 2019-06-11 09:32:30,046 [er for task 527] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:30,046 [er for task 527] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:30,046 [er for task 527] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:30,046 [er for task 527] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:30,047 [er for task 527] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 789.0 (TID 527). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:32:30,047 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 789.0 (TID 527) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:30,047 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 789.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 789 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:32:30,048 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 263 finished: print at Transform.scala:36, took 0.040682 s
   [druid] 2019-06-11 09:32:30,052 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:30,052 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 262 is 83 bytes
   [druid] 2019-06-11 09:32:30,052 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 263 is 160 bytes
   [druid] 2019-06-11 09:32:30,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 264 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:30,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 792 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:30,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 790, ShuffleMapStage 791)
   [druid] 2019-06-11 09:32:30,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:32:30,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 792 (MapPartitionsRDD[925] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:30,053 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_396 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:30,054 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_396_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:30,054 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_396_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:30,054 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 396 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:30,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 792 (MapPartitionsRDD[925] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:32:30,054 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 792.0 with 1 tasks
   [druid] 2019-06-11 09:32:30,055 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 792.0 (TID 528, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:30,055 [er for task 528] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 792.0 (TID 528)
   [druid] 2019-06-11 09:32:30,056 [er for task 528] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:30,056 [er for task 528] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:30,056 [er for task 528] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:30,056 [er for task 528] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:30,057 [er for task 528] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 792.0 (TID 528). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:30,057 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 792.0 (TID 528) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:30,057 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 792.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:30,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 792 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:32:30,057 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 264 finished: print at Transform.scala:36, took 0.005416 s
   [druid] 2019-06-11 09:32:30,057 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216750000 ms.0 from job set of time 1560216750000 ms
   [druid] 2019-06-11 09:32:30,057 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.057 s for time 1560216750000 ms (execution: 0.051 s)
   [druid] 2019-06-11 09:32:30,057 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 918 from persistence list
   [druid] 2019-06-11 09:32:30,058 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 918
   [druid] 2019-06-11 09:32:30,058 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 913 from persistence list
   [druid] 2019-06-11 09:32:30,058 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 913
   [druid] 2019-06-11 09:32:30,058 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 912 from persistence list
   [druid] 2019-06-11 09:32:30,058 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 912
   [druid] 2019-06-11 09:32:30,058 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[912] at socketTextStream at Transform.scala:18 of time 1560216750000 ms
   [druid] 2019-06-11 09:32:30,058 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216740000 ms
   [druid] 2019-06-11 09:32:30,058 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216740000 ms
   [druid] 2019-06-11 09:32:35,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216755000 ms
   [druid] 2019-06-11 09:32:35,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216755000 ms.0 from job set of time 1560216755000 ms
   [druid] 2019-06-11 09:32:35,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 927 (map at Transform.scala:20)
   [druid] 2019-06-11 09:32:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:32:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 265 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 795 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 793, ShuffleMapStage 794)
   [druid] 2019-06-11 09:32:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 794)
   [druid] 2019-06-11 09:32:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 794 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:32:35,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_397 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_397_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:32:35,009 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_397_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:35,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 397 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 794 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:32:35,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 794.0 with 2 tasks
   [druid] 2019-06-11 09:32:35,010 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 794.0 (TID 529, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:35,010 [er for task 529] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 794.0 (TID 529)
   [druid] 2019-06-11 09:32:35,019 [er for task 529] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 794.0 (TID 529). 810 bytes result sent to driver
   [druid] 2019-06-11 09:32:35,019 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 794.0 (TID 530, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:35,019 [er for task 530] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 794.0 (TID 530)
   [druid] 2019-06-11 09:32:35,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 794.0 (TID 529) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:32:35,029 [er for task 530] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 794.0 (TID 530). 810 bytes result sent to driver
   [druid] 2019-06-11 09:32:35,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 794.0 (TID 530) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:32:35,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 794.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 794 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:32:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:32:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:32:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 795)
   [druid] 2019-06-11 09:32:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:32:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 795 (MapPartitionsRDD[932] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:35,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_398 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:35,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_398_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:35,032 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_398_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:35,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 398 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 795 (MapPartitionsRDD[932] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:32:35,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 795.0 with 1 tasks
   [druid] 2019-06-11 09:32:35,032 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 795.0 (TID 531, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:35,033 [er for task 531] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 795.0 (TID 531)
   [druid] 2019-06-11 09:32:35,034 [er for task 531] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:35,034 [er for task 531] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:35,034 [er for task 531] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:35,034 [er for task 531] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:35,035 [er for task 531] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 795.0 (TID 531). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:32:35,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 795.0 (TID 531) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:35,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 795.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 795 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:32:35,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 265 finished: print at Transform.scala:36, took 0.028078 s
   [druid] 2019-06-11 09:32:35,038 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:35,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 264 is 83 bytes
   [druid] 2019-06-11 09:32:35,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 265 is 160 bytes
   [druid] 2019-06-11 09:32:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 266 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 798 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 796, ShuffleMapStage 797)
   [druid] 2019-06-11 09:32:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:32:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 798 (MapPartitionsRDD[932] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:35,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_399 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:35,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_399_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:35,040 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_399_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:35,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 399 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 798 (MapPartitionsRDD[932] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:32:35,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 798.0 with 1 tasks
   [druid] 2019-06-11 09:32:35,041 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 798.0 (TID 532, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:35,041 [er for task 532] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 798.0 (TID 532)
   [druid] 2019-06-11 09:32:35,042 [er for task 532] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:35,042 [er for task 532] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:35,042 [er for task 532] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:35,042 [er for task 532] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:35,042 [er for task 532] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 798.0 (TID 532). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:35,043 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 798.0 (TID 532) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:35,043 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 798.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:35,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 798 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:35,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 266 finished: print at Transform.scala:36, took 0.005161 s
   [druid] 2019-06-11 09:32:35,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216755000 ms.0 from job set of time 1560216755000 ms
   [druid] 2019-06-11 09:32:35,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216755000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:32:35,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 925 from persistence list
   [druid] 2019-06-11 09:32:35,043 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 925
   [druid] 2019-06-11 09:32:35,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 920 from persistence list
   [druid] 2019-06-11 09:32:35,044 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 920
   [druid] 2019-06-11 09:32:35,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 919 from persistence list
   [druid] 2019-06-11 09:32:35,044 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 919
   [druid] 2019-06-11 09:32:35,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[919] at socketTextStream at Transform.scala:18 of time 1560216755000 ms
   [druid] 2019-06-11 09:32:35,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216745000 ms
   [druid] 2019-06-11 09:32:35,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216745000 ms
   [druid] 2019-06-11 09:32:40,004 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216760000 ms
   [druid] 2019-06-11 09:32:40,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216760000 ms.0 from job set of time 1560216760000 ms
   [druid] 2019-06-11 09:32:40,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:32:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 934 (map at Transform.scala:20)
   [druid] 2019-06-11 09:32:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 267 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 801 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 799, ShuffleMapStage 800)
   [druid] 2019-06-11 09:32:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 799)
   [druid] 2019-06-11 09:32:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 799 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:32:40,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_400 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:40,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_400_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:32:40,009 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_400_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:40,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 400 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 799 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:32:40,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 799.0 with 2 tasks
   [druid] 2019-06-11 09:32:40,010 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 799.0 (TID 533, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:40,010 [er for task 533] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 799.0 (TID 533)
   [druid] 2019-06-11 09:32:40,020 [er for task 533] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 799.0 (TID 533). 810 bytes result sent to driver
   [druid] 2019-06-11 09:32:40,020 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 799.0 (TID 534, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:40,020 [er for task 534] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 799.0 (TID 534)
   [druid] 2019-06-11 09:32:40,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 799.0 (TID 533) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:32:40,033 [er for task 534] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 799.0 (TID 534). 810 bytes result sent to driver
   [druid] 2019-06-11 09:32:40,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 799.0 (TID 534) in 13 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:32:40,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 799.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 799 (parallelize at Transform.scala:16) finished in 0.025 s
   [druid] 2019-06-11 09:32:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:32:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:32:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 801)
   [druid] 2019-06-11 09:32:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:32:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 801 (MapPartitionsRDD[939] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:40,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_401 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:40,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_401_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:40,036 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_401_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:40,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 401 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 801 (MapPartitionsRDD[939] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:32:40,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 801.0 with 1 tasks
   [druid] 2019-06-11 09:32:40,037 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 801.0 (TID 535, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:40,037 [er for task 535] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 801.0 (TID 535)
   [druid] 2019-06-11 09:32:40,038 [er for task 535] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:40,038 [er for task 535] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:40,038 [er for task 535] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:40,038 [er for task 535] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:40,039 [er for task 535] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 801.0 (TID 535). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:32:40,039 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 801.0 (TID 535) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:40,039 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 801.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:40,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 801 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:40,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 267 finished: print at Transform.scala:36, took 0.032093 s
   [druid] 2019-06-11 09:32:40,042 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:40,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 267 is 160 bytes
   [druid] 2019-06-11 09:32:40,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 266 is 83 bytes
   [druid] 2019-06-11 09:32:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 268 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 804 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 802, ShuffleMapStage 803)
   [druid] 2019-06-11 09:32:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:32:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 804 (MapPartitionsRDD[939] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:40,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_402 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:40,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_402_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:32:40,044 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_402_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:40,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 402 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:40,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 804 (MapPartitionsRDD[939] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:32:40,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 804.0 with 1 tasks
   [druid] 2019-06-11 09:32:40,044 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 804.0 (TID 536, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:40,045 [er for task 536] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 804.0 (TID 536)
   [druid] 2019-06-11 09:32:40,045 [er for task 536] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:40,045 [er for task 536] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:40,045 [er for task 536] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:40,045 [er for task 536] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:40,046 [er for task 536] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 804.0 (TID 536). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:40,046 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 804.0 (TID 536) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:40,046 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 804.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:40,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 804 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:40,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 268 finished: print at Transform.scala:36, took 0.004692 s
   [druid] 2019-06-11 09:32:40,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216760000 ms.0 from job set of time 1560216760000 ms
   [druid] 2019-06-11 09:32:40,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.047 s for time 1560216760000 ms (execution: 0.043 s)
   [druid] 2019-06-11 09:32:40,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 932 from persistence list
   [druid] 2019-06-11 09:32:40,047 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 932
   [druid] 2019-06-11 09:32:40,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 927 from persistence list
   [druid] 2019-06-11 09:32:40,047 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 927
   [druid] 2019-06-11 09:32:40,047 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 926 from persistence list
   [druid] 2019-06-11 09:32:40,047 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 926
   [druid] 2019-06-11 09:32:40,047 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[926] at socketTextStream at Transform.scala:18 of time 1560216760000 ms
   [druid] 2019-06-11 09:32:40,048 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216750000 ms
   [druid] 2019-06-11 09:32:40,048 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216750000 ms
   [druid] 2019-06-11 09:32:45,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216765000 ms
   [druid] 2019-06-11 09:32:45,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216765000 ms.0 from job set of time 1560216765000 ms
   [druid] 2019-06-11 09:32:45,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 941 (map at Transform.scala:20)
   [druid] 2019-06-11 09:32:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:32:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 269 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 807 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 805, ShuffleMapStage 806)
   [druid] 2019-06-11 09:32:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 806)
   [druid] 2019-06-11 09:32:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 806 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:32:45,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_403 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:45,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_403_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:32:45,010 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_403_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:45,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 403 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:45,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 806 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:32:45,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 806.0 with 2 tasks
   [druid] 2019-06-11 09:32:45,011 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 806.0 (TID 537, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:45,011 [er for task 537] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 806.0 (TID 537)
   [druid] 2019-06-11 09:32:45,021 [er for task 537] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 806.0 (TID 537). 853 bytes result sent to driver
   [druid] 2019-06-11 09:32:45,021 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 806.0 (TID 538, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:45,021 [er for task 538] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 806.0 (TID 538)
   [druid] 2019-06-11 09:32:45,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 806.0 (TID 537) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:32:45,029 [er for task 538] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 806.0 (TID 538). 767 bytes result sent to driver
   [druid] 2019-06-11 09:32:45,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 806.0 (TID 538) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:32:45,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 806.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 806 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:32:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:32:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:32:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 807)
   [druid] 2019-06-11 09:32:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:32:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 807 (MapPartitionsRDD[946] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:45,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_404 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:45,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_404_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:45,031 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_404_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:45,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 404 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:45,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 807 (MapPartitionsRDD[946] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:32:45,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 807.0 with 1 tasks
   [druid] 2019-06-11 09:32:45,032 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 807.0 (TID 539, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:45,032 [er for task 539] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 807.0 (TID 539)
   [druid] 2019-06-11 09:32:45,032 [er for task 539] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:45,032 [er for task 539] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:45,032 [er for task 539] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:45,032 [er for task 539] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:45,033 [er for task 539] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 807.0 (TID 539). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:45,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 807.0 (TID 539) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:45,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 807.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 807 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:45,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 269 finished: print at Transform.scala:36, took 0.025164 s
   [druid] 2019-06-11 09:32:45,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:45,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 268 is 83 bytes
   [druid] 2019-06-11 09:32:45,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 269 is 160 bytes
   [druid] 2019-06-11 09:32:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 270 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 810 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 808, ShuffleMapStage 809)
   [druid] 2019-06-11 09:32:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:32:45,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 810 (MapPartitionsRDD[946] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:45,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_405 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:45,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_405_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:45,039 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_405_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:45,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 405 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:45,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 810 (MapPartitionsRDD[946] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:32:45,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 810.0 with 1 tasks
   [druid] 2019-06-11 09:32:45,040 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 810.0 (TID 540, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:45,040 [er for task 540] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 810.0 (TID 540)
   [druid] 2019-06-11 09:32:45,041 [er for task 540] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:45,041 [er for task 540] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:45,041 [er for task 540] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:45,041 [er for task 540] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:45,041 [er for task 540] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 810.0 (TID 540). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:45,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 810.0 (TID 540) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:45,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 810.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:45,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 810 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:45,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 270 finished: print at Transform.scala:36, took 0.006302 s
   [druid] 2019-06-11 09:32:45,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216765000 ms.0 from job set of time 1560216765000 ms
   [druid] 2019-06-11 09:32:45,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560216765000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:32:45,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 939 from persistence list
   [druid] 2019-06-11 09:32:45,043 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 939
   [druid] 2019-06-11 09:32:45,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 934 from persistence list
   [druid] 2019-06-11 09:32:45,043 [-thread-pool-22] INFO  che.spark.storage.BlockManager {1} - Removing RDD 934
   [druid] 2019-06-11 09:32:45,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 933 from persistence list
   [druid] 2019-06-11 09:32:45,043 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 933
   [druid] 2019-06-11 09:32:45,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[933] at socketTextStream at Transform.scala:18 of time 1560216765000 ms
   [druid] 2019-06-11 09:32:45,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216755000 ms
   [druid] 2019-06-11 09:32:45,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216755000 ms
   [druid] 2019-06-11 09:32:50,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216770000 ms
   [druid] 2019-06-11 09:32:50,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216770000 ms.0 from job set of time 1560216770000 ms
   [druid] 2019-06-11 09:32:50,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:50,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:32:50,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 948 (map at Transform.scala:20)
   [druid] 2019-06-11 09:32:50,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 271 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:50,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 813 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:50,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 811, ShuffleMapStage 812)
   [druid] 2019-06-11 09:32:50,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 811)
   [druid] 2019-06-11 09:32:50,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 811 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:32:50,007 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_406 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:50,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_406_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:32:50,009 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_406_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:50,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 406 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 811 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:32:50,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 811.0 with 2 tasks
   [druid] 2019-06-11 09:32:50,009 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 811.0 (TID 541, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:50,009 [er for task 541] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 811.0 (TID 541)
   [druid] 2019-06-11 09:32:50,018 [er for task 541] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 811.0 (TID 541). 853 bytes result sent to driver
   [druid] 2019-06-11 09:32:50,019 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 811.0 (TID 542, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:50,019 [er for task 542] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 811.0 (TID 542)
   [druid] 2019-06-11 09:32:50,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 811.0 (TID 541) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:32:50,027 [er for task 542] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 811.0 (TID 542). 810 bytes result sent to driver
   [druid] 2019-06-11 09:32:50,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 811.0 (TID 542) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:32:50,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 811.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 811 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:32:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:32:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:32:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 813)
   [druid] 2019-06-11 09:32:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:32:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 813 (MapPartitionsRDD[953] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:50,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_407 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:50,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_407_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:50,029 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_407_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:50,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 407 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:50,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 813 (MapPartitionsRDD[953] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:32:50,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 813.0 with 1 tasks
   [druid] 2019-06-11 09:32:50,030 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 813.0 (TID 543, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:50,031 [er for task 543] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 813.0 (TID 543)
   [druid] 2019-06-11 09:32:50,032 [er for task 543] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:50,032 [er for task 543] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:50,032 [er for task 543] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:50,032 [er for task 543] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:50,033 [er for task 543] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 813.0 (TID 543). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:32:50,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 813.0 (TID 543) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:50,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 813.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 813 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:32:50,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 271 finished: print at Transform.scala:36, took 0.026880 s
   [druid] 2019-06-11 09:32:50,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:50,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 271 is 160 bytes
   [druid] 2019-06-11 09:32:50,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 270 is 83 bytes
   [druid] 2019-06-11 09:32:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 272 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 816 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 814, ShuffleMapStage 815)
   [druid] 2019-06-11 09:32:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:32:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 816 (MapPartitionsRDD[953] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:50,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_408 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:50,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_408_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:50,038 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_408_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:50,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 408 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:50,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 816 (MapPartitionsRDD[953] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:32:50,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 816.0 with 1 tasks
   [druid] 2019-06-11 09:32:50,038 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 816.0 (TID 544, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:50,039 [er for task 544] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 816.0 (TID 544)
   [druid] 2019-06-11 09:32:50,039 [er for task 544] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:50,039 [er for task 544] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:50,039 [er for task 544] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:50,039 [er for task 544] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:50,040 [er for task 544] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 816.0 (TID 544). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:50,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 816.0 (TID 544) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:50,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 816.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:50,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 816 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:50,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 272 finished: print at Transform.scala:36, took 0.004517 s
   [druid] 2019-06-11 09:32:50,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216770000 ms.0 from job set of time 1560216770000 ms
   [druid] 2019-06-11 09:32:50,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560216770000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:32:50,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 946 from persistence list
   [druid] 2019-06-11 09:32:50,041 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 946
   [druid] 2019-06-11 09:32:50,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 941 from persistence list
   [druid] 2019-06-11 09:32:50,041 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 941
   [druid] 2019-06-11 09:32:50,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 940 from persistence list
   [druid] 2019-06-11 09:32:50,041 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 940
   [druid] 2019-06-11 09:32:50,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[940] at socketTextStream at Transform.scala:18 of time 1560216770000 ms
   [druid] 2019-06-11 09:32:50,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216760000 ms
   [druid] 2019-06-11 09:32:50,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216760000 ms
   [druid] 2019-06-11 09:32:55,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216775000 ms
   [druid] 2019-06-11 09:32:55,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216775000 ms.0 from job set of time 1560216775000 ms
   [druid] 2019-06-11 09:32:55,006 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:55,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 955 (map at Transform.scala:20)
   [druid] 2019-06-11 09:32:55,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:32:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 273 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 819 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 818, ShuffleMapStage 817)
   [druid] 2019-06-11 09:32:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 818)
   [druid] 2019-06-11 09:32:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 818 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:32:55,007 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_409 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:55,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_409_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:32:55,009 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_409_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 409 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 818 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:32:55,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 818.0 with 2 tasks
   [druid] 2019-06-11 09:32:55,009 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 818.0 (TID 545, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:55,010 [er for task 545] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 818.0 (TID 545)
   [druid] 2019-06-11 09:32:55,018 [er for task 545] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 818.0 (TID 545). 810 bytes result sent to driver
   [druid] 2019-06-11 09:32:55,018 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 818.0 (TID 546, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:32:55,018 [er for task 546] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 818.0 (TID 546)
   [druid] 2019-06-11 09:32:55,018 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 818.0 (TID 545) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:32:55,026 [er for task 546] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 818.0 (TID 546). 810 bytes result sent to driver
   [druid] 2019-06-11 09:32:55,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 818.0 (TID 546) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:32:55,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 818.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:55,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 818 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:32:55,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:32:55,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:32:55,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 819)
   [druid] 2019-06-11 09:32:55,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:32:55,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 819 (MapPartitionsRDD[960] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:55,027 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_410 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:55,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_410_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:55,028 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_410_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 410 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 819 (MapPartitionsRDD[960] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:32:55,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 819.0 with 1 tasks
   [druid] 2019-06-11 09:32:55,029 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 819.0 (TID 547, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:55,029 [er for task 547] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 819.0 (TID 547)
   [druid] 2019-06-11 09:32:55,030 [er for task 547] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:55,030 [er for task 547] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:55,030 [er for task 547] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:55,030 [er for task 547] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:55,030 [er for task 547] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 819.0 (TID 547). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:55,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 819.0 (TID 547) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:55,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 819.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:55,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 819 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:32:55,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 273 finished: print at Transform.scala:36, took 0.024529 s
   [druid] 2019-06-11 09:32:55,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:32:55,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 272 is 83 bytes
   [druid] 2019-06-11 09:32:55,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 273 is 160 bytes
   [druid] 2019-06-11 09:32:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 274 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:32:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 822 (print at Transform.scala:36)
   [druid] 2019-06-11 09:32:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 820, ShuffleMapStage 821)
   [druid] 2019-06-11 09:32:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:32:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 822 (MapPartitionsRDD[960] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:32:55,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_411 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:55,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_411_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:32:55,044 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_409_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,045 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_411_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,047 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 411 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:32:55,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 269
   [druid] 2019-06-11 09:32:55,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 822 (MapPartitionsRDD[960] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:32:55,049 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 822.0 with 1 tasks
   [druid] 2019-06-11 09:32:55,049 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 822.0 (TID 548, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:32:55,049 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_402_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,049 [er for task 548] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 822.0 (TID 548)
   [druid] 2019-06-11 09:32:55,050 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_394_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,050 [er for task 548] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:32:55,050 [er for task 548] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:55,050 [er for task 548] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:32:55,050 [er for task 548] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:32:55,051 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_406_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,051 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_410_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,051 [er for task 548] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 822.0 (TID 548). 966 bytes result sent to driver
   [druid] 2019-06-11 09:32:55,051 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 822.0 (TID 548) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:32:55,051 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 822.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:32:55,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 267
   [druid] 2019-06-11 09:32:55,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 822 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:32:55,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 274 finished: print at Transform.scala:36, took 0.018330 s
   [druid] 2019-06-11 09:32:55,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216775000 ms.0 from job set of time 1560216775000 ms
   [druid] 2019-06-11 09:32:55,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.052 s for time 1560216775000 ms (execution: 0.047 s)
   [druid] 2019-06-11 09:32:55,052 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_404_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 953 from persistence list
   [druid] 2019-06-11 09:32:55,053 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 953
   [druid] 2019-06-11 09:32:55,053 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 948 from persistence list
   [druid] 2019-06-11 09:32:55,053 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 948
   [druid] 2019-06-11 09:32:55,053 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 947 from persistence list
   [druid] 2019-06-11 09:32:55,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 261
   [druid] 2019-06-11 09:32:55,053 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 947
   [druid] 2019-06-11 09:32:55,053 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[947] at socketTextStream at Transform.scala:18 of time 1560216775000 ms
   [druid] 2019-06-11 09:32:55,054 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216765000 ms
   [druid] 2019-06-11 09:32:55,054 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216765000 ms
   [druid] 2019-06-11 09:32:55,054 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_408_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,054 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 265
   [druid] 2019-06-11 09:32:55,055 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_397_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,056 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_403_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,057 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_407_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,057 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_398_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,058 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 264
   [druid] 2019-06-11 09:32:55,058 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 268
   [druid] 2019-06-11 09:32:55,059 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_405_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,059 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 263
   [druid] 2019-06-11 09:32:55,059 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_400_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,060 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 266
   [druid] 2019-06-11 09:32:55,061 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_399_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,061 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_395_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,062 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_401_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:32:55,062 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 262
   [druid] 2019-06-11 09:32:55,063 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 260
   [druid] 2019-06-11 09:32:55,063 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_396_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:00,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216780000 ms
   [druid] 2019-06-11 09:33:00,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216780000 ms.0 from job set of time 1560216780000 ms
   [druid] 2019-06-11 09:33:00,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:33:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 962 (map at Transform.scala:20)
   [druid] 2019-06-11 09:33:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 275 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 825 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 823, ShuffleMapStage 824)
   [druid] 2019-06-11 09:33:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 823)
   [druid] 2019-06-11 09:33:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 823 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:33:00,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_412 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:00,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_412_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:33:00,011 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_412_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:00,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 412 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 823 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:33:00,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 823.0 with 2 tasks
   [druid] 2019-06-11 09:33:00,012 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 823.0 (TID 549, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:00,012 [er for task 549] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 823.0 (TID 549)
   [druid] 2019-06-11 09:33:00,022 [er for task 549] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 823.0 (TID 549). 810 bytes result sent to driver
   [druid] 2019-06-11 09:33:00,022 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 823.0 (TID 550, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:00,022 [er for task 550] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 823.0 (TID 550)
   [druid] 2019-06-11 09:33:00,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 823.0 (TID 549) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:33:00,030 [er for task 550] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 823.0 (TID 550). 810 bytes result sent to driver
   [druid] 2019-06-11 09:33:00,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 823.0 (TID 550) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:33:00,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 823.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 823 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:33:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:33:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:33:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 825)
   [druid] 2019-06-11 09:33:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:33:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 825 (MapPartitionsRDD[967] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:00,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_413 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:00,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_413_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:00,032 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_413_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:00,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 413 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:00,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 825 (MapPartitionsRDD[967] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:33:00,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 825.0 with 1 tasks
   [druid] 2019-06-11 09:33:00,033 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 825.0 (TID 551, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:00,033 [er for task 551] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 825.0 (TID 551)
   [druid] 2019-06-11 09:33:00,033 [er for task 551] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:00,033 [er for task 551] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:00,034 [er for task 551] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:00,034 [er for task 551] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:00,034 [er for task 551] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 825.0 (TID 551). 966 bytes result sent to driver
   [druid] 2019-06-11 09:33:00,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 825.0 (TID 551) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:00,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 825.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:00,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 825 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:33:00,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 275 finished: print at Transform.scala:36, took 0.025868 s
   [druid] 2019-06-11 09:33:00,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:00,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 275 is 160 bytes
   [druid] 2019-06-11 09:33:00,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 274 is 83 bytes
   [druid] 2019-06-11 09:33:00,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 276 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:00,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 828 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:00,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 826, ShuffleMapStage 827)
   [druid] 2019-06-11 09:33:00,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:33:00,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 828 (MapPartitionsRDD[967] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:00,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_414 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:00,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_414_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:00,040 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_414_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:00,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 414 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 828 (MapPartitionsRDD[967] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:33:00,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 828.0 with 1 tasks
   [druid] 2019-06-11 09:33:00,040 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 828.0 (TID 552, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:00,041 [er for task 552] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 828.0 (TID 552)
   [druid] 2019-06-11 09:33:00,041 [er for task 552] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:00,041 [er for task 552] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:00,041 [er for task 552] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:00,041 [er for task 552] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:00,042 [er for task 552] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 828.0 (TID 552). 966 bytes result sent to driver
   [druid] 2019-06-11 09:33:00,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 828.0 (TID 552) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:00,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 828.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:00,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 828 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:33:00,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 276 finished: print at Transform.scala:36, took 0.004949 s
   [druid] 2019-06-11 09:33:00,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216780000 ms.0 from job set of time 1560216780000 ms
   [druid] 2019-06-11 09:33:00,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560216780000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:33:00,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 960 from persistence list
   [druid] 2019-06-11 09:33:00,043 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 960
   [druid] 2019-06-11 09:33:00,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 955 from persistence list
   [druid] 2019-06-11 09:33:00,043 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 955
   [druid] 2019-06-11 09:33:00,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 954 from persistence list
   [druid] 2019-06-11 09:33:00,043 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 954
   [druid] 2019-06-11 09:33:00,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[954] at socketTextStream at Transform.scala:18 of time 1560216780000 ms
   [druid] 2019-06-11 09:33:00,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216770000 ms
   [druid] 2019-06-11 09:33:00,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216770000 ms
   [druid] 2019-06-11 09:33:05,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216785000 ms
   [druid] 2019-06-11 09:33:05,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216785000 ms.0 from job set of time 1560216785000 ms
   [druid] 2019-06-11 09:33:05,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 969 (map at Transform.scala:20)
   [druid] 2019-06-11 09:33:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:33:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 277 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 831 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 829, ShuffleMapStage 830)
   [druid] 2019-06-11 09:33:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 830)
   [druid] 2019-06-11 09:33:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 830 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:33:05,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_415 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:05,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_415_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:33:05,009 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_415_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:05,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 415 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 830 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:33:05,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 830.0 with 2 tasks
   [druid] 2019-06-11 09:33:05,009 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 830.0 (TID 553, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:05,010 [er for task 553] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 830.0 (TID 553)
   [druid] 2019-06-11 09:33:05,018 [er for task 553] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 830.0 (TID 553). 810 bytes result sent to driver
   [druid] 2019-06-11 09:33:05,019 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 830.0 (TID 554, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:05,019 [er for task 554] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 830.0 (TID 554)
   [druid] 2019-06-11 09:33:05,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 830.0 (TID 553) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:33:05,026 [er for task 554] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 830.0 (TID 554). 767 bytes result sent to driver
   [druid] 2019-06-11 09:33:05,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 830.0 (TID 554) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:33:05,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 830.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 830 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:33:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:33:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:33:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 831)
   [druid] 2019-06-11 09:33:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:33:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 831 (MapPartitionsRDD[974] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:05,027 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_416 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:05,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_416_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:05,028 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_416_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:05,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 416 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 831 (MapPartitionsRDD[974] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:33:05,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 831.0 with 1 tasks
   [druid] 2019-06-11 09:33:05,029 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 831.0 (TID 555, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:05,029 [er for task 555] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 831.0 (TID 555)
   [druid] 2019-06-11 09:33:05,030 [er for task 555] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:05,030 [er for task 555] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:05,030 [er for task 555] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:05,030 [er for task 555] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:05,030 [er for task 555] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 831.0 (TID 555). 966 bytes result sent to driver
   [druid] 2019-06-11 09:33:05,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 831.0 (TID 555) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:05,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 831.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 831 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:33:05,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 277 finished: print at Transform.scala:36, took 0.023705 s
   [druid] 2019-06-11 09:33:05,033 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:05,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 276 is 83 bytes
   [druid] 2019-06-11 09:33:05,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 277 is 160 bytes
   [druid] 2019-06-11 09:33:05,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 278 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:05,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 834 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:05,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 832, ShuffleMapStage 833)
   [druid] 2019-06-11 09:33:05,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:33:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 834 (MapPartitionsRDD[974] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:05,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_417 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:05,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_417_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:05,035 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_417_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:05,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 417 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 834 (MapPartitionsRDD[974] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:33:05,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 834.0 with 1 tasks
   [druid] 2019-06-11 09:33:05,036 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 834.0 (TID 556, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:05,036 [er for task 556] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 834.0 (TID 556)
   [druid] 2019-06-11 09:33:05,037 [er for task 556] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:05,037 [er for task 556] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:05,038 [er for task 556] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:05,038 [er for task 556] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:05,038 [er for task 556] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 834.0 (TID 556). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:33:05,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 834.0 (TID 556) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:05,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 834.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 834 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:33:05,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 278 finished: print at Transform.scala:36, took 0.005982 s
   [druid] 2019-06-11 09:33:05,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216785000 ms.0 from job set of time 1560216785000 ms
   [druid] 2019-06-11 09:33:05,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.039 s for time 1560216785000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:33:05,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 967 from persistence list
   [druid] 2019-06-11 09:33:05,039 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 967
   [druid] 2019-06-11 09:33:05,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 962 from persistence list
   [druid] 2019-06-11 09:33:05,040 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 962
   [druid] 2019-06-11 09:33:05,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 961 from persistence list
   [druid] 2019-06-11 09:33:05,040 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 961
   [druid] 2019-06-11 09:33:05,040 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[961] at socketTextStream at Transform.scala:18 of time 1560216785000 ms
   [druid] 2019-06-11 09:33:05,040 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216775000 ms
   [druid] 2019-06-11 09:33:05,040 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216775000 ms
   [druid] 2019-06-11 09:33:10,010 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216790000 ms
   [druid] 2019-06-11 09:33:10,010 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216790000 ms.0 from job set of time 1560216790000 ms
   [druid] 2019-06-11 09:33:10,012 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:33:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 976 (map at Transform.scala:20)
   [druid] 2019-06-11 09:33:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 279 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 837 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 836, ShuffleMapStage 835)
   [druid] 2019-06-11 09:33:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 835)
   [druid] 2019-06-11 09:33:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 835 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:33:10,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_418 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:10,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_418_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:33:10,014 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_418_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:10,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 418 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:10,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 835 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:33:10,014 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 835.0 with 2 tasks
   [druid] 2019-06-11 09:33:10,015 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 835.0 (TID 557, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:10,015 [er for task 557] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 835.0 (TID 557)
   [druid] 2019-06-11 09:33:10,023 [er for task 557] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 835.0 (TID 557). 810 bytes result sent to driver
   [druid] 2019-06-11 09:33:10,024 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 835.0 (TID 558, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:10,024 [er for task 558] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 835.0 (TID 558)
   [druid] 2019-06-11 09:33:10,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 835.0 (TID 557) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:33:10,031 [er for task 558] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 835.0 (TID 558). 810 bytes result sent to driver
   [druid] 2019-06-11 09:33:10,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 835.0 (TID 558) in 7 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:33:10,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 835.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:10,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 835 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:33:10,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:33:10,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:33:10,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 837)
   [druid] 2019-06-11 09:33:10,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:33:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 837 (MapPartitionsRDD[981] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:10,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_419 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:10,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_419_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:10,033 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_419_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:10,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 419 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 837 (MapPartitionsRDD[981] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:33:10,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 837.0 with 1 tasks
   [druid] 2019-06-11 09:33:10,033 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 837.0 (TID 559, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:10,034 [er for task 559] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 837.0 (TID 559)
   [druid] 2019-06-11 09:33:10,034 [er for task 559] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:10,034 [er for task 559] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:10,034 [er for task 559] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:10,034 [er for task 559] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:10,035 [er for task 559] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 837.0 (TID 559). 966 bytes result sent to driver
   [druid] 2019-06-11 09:33:10,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 837.0 (TID 559) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:10,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 837.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:10,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 837 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:33:10,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 279 finished: print at Transform.scala:36, took 0.023509 s
   [druid] 2019-06-11 09:33:10,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:10,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 279 is 160 bytes
   [druid] 2019-06-11 09:33:10,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 278 is 83 bytes
   [druid] 2019-06-11 09:33:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 280 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 840 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 838, ShuffleMapStage 839)
   [druid] 2019-06-11 09:33:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:33:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 840 (MapPartitionsRDD[981] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:10,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_420 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:10,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_420_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:10,040 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_420_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:10,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 420 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 840 (MapPartitionsRDD[981] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:33:10,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 840.0 with 1 tasks
   [druid] 2019-06-11 09:33:10,041 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 840.0 (TID 560, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:10,041 [er for task 560] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 840.0 (TID 560)
   [druid] 2019-06-11 09:33:10,042 [er for task 560] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:10,042 [er for task 560] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:10,042 [er for task 560] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:10,042 [er for task 560] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:10,043 [er for task 560] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 840.0 (TID 560). 966 bytes result sent to driver
   [druid] 2019-06-11 09:33:10,043 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 840.0 (TID 560) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:10,043 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 840.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 840 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:33:10,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 280 finished: print at Transform.scala:36, took 0.005674 s
   [druid] 2019-06-11 09:33:10,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216790000 ms.0 from job set of time 1560216790000 ms
   [druid] 2019-06-11 09:33:10,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216790000 ms (execution: 0.033 s)
   [druid] 2019-06-11 09:33:10,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 974 from persistence list
   [druid] 2019-06-11 09:33:10,043 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 974
   [druid] 2019-06-11 09:33:10,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 969 from persistence list
   [druid] 2019-06-11 09:33:10,044 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 969
   [druid] 2019-06-11 09:33:10,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 968 from persistence list
   [druid] 2019-06-11 09:33:10,044 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 968
   [druid] 2019-06-11 09:33:10,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[968] at socketTextStream at Transform.scala:18 of time 1560216790000 ms
   [druid] 2019-06-11 09:33:10,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216780000 ms
   [druid] 2019-06-11 09:33:10,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216780000 ms
   [druid] 2019-06-11 09:33:15,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216795000 ms
   [druid] 2019-06-11 09:33:15,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216795000 ms.0 from job set of time 1560216795000 ms
   [druid] 2019-06-11 09:33:15,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 983 (map at Transform.scala:20)
   [druid] 2019-06-11 09:33:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:33:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 281 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 843 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 841, ShuffleMapStage 842)
   [druid] 2019-06-11 09:33:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 842)
   [druid] 2019-06-11 09:33:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 842 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:33:15,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_421 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:15,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_421_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:33:15,009 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_421_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:15,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 421 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 842 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:33:15,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 842.0 with 2 tasks
   [druid] 2019-06-11 09:33:15,010 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 842.0 (TID 561, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:15,010 [er for task 561] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 842.0 (TID 561)
   [druid] 2019-06-11 09:33:15,019 [er for task 561] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 842.0 (TID 561). 853 bytes result sent to driver
   [druid] 2019-06-11 09:33:15,019 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 842.0 (TID 562, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:15,020 [er for task 562] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 842.0 (TID 562)
   [druid] 2019-06-11 09:33:15,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 842.0 (TID 561) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:33:15,027 [er for task 562] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 842.0 (TID 562). 767 bytes result sent to driver
   [druid] 2019-06-11 09:33:15,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 842.0 (TID 562) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:33:15,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 842.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 842 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:33:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:33:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:33:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 843)
   [druid] 2019-06-11 09:33:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:33:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 843 (MapPartitionsRDD[988] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:15,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_422 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:15,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_422_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:15,029 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_422_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:15,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 422 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:15,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 843 (MapPartitionsRDD[988] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:33:15,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 843.0 with 1 tasks
   [druid] 2019-06-11 09:33:15,030 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 843.0 (TID 563, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:15,030 [er for task 563] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 843.0 (TID 563)
   [druid] 2019-06-11 09:33:15,030 [er for task 563] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:15,030 [er for task 563] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:15,030 [er for task 563] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:15,030 [er for task 563] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:15,031 [er for task 563] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 843.0 (TID 563). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:33:15,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 843.0 (TID 563) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:15,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 843.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:15,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 843 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:33:15,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 281 finished: print at Transform.scala:36, took 0.024120 s
   [druid] 2019-06-11 09:33:15,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:15,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 280 is 83 bytes
   [druid] 2019-06-11 09:33:15,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 281 is 160 bytes
   [druid] 2019-06-11 09:33:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 282 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 846 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 844, ShuffleMapStage 845)
   [druid] 2019-06-11 09:33:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:33:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 846 (MapPartitionsRDD[988] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:15,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_423 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:15,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_423_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:15,036 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_423_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:15,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 423 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:15,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 846 (MapPartitionsRDD[988] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:33:15,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 846.0 with 1 tasks
   [druid] 2019-06-11 09:33:15,036 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 846.0 (TID 564, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:15,036 [er for task 564] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 846.0 (TID 564)
   [druid] 2019-06-11 09:33:15,037 [er for task 564] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:15,037 [er for task 564] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:15,037 [er for task 564] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:15,037 [er for task 564] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:15,038 [er for task 564] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 846.0 (TID 564). 966 bytes result sent to driver
   [druid] 2019-06-11 09:33:15,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 846.0 (TID 564) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:15,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 846.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:15,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 846 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:33:15,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 282 finished: print at Transform.scala:36, took 0.005272 s
   [druid] 2019-06-11 09:33:15,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216795000 ms.0 from job set of time 1560216795000 ms
   [druid] 2019-06-11 09:33:15,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.039 s for time 1560216795000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:33:15,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 981 from persistence list
   [druid] 2019-06-11 09:33:15,040 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 981
   [druid] 2019-06-11 09:33:15,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 976 from persistence list
   [druid] 2019-06-11 09:33:15,040 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 976
   [druid] 2019-06-11 09:33:15,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 975 from persistence list
   [druid] 2019-06-11 09:33:15,040 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 975
   [druid] 2019-06-11 09:33:15,040 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[975] at socketTextStream at Transform.scala:18 of time 1560216795000 ms
   [druid] 2019-06-11 09:33:15,040 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216785000 ms
   [druid] 2019-06-11 09:33:15,040 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216785000 ms
   [druid] 2019-06-11 09:33:20,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216800000 ms
   [druid] 2019-06-11 09:33:20,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216800000 ms.0 from job set of time 1560216800000 ms
   [druid] 2019-06-11 09:33:20,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:33:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 990 (map at Transform.scala:20)
   [druid] 2019-06-11 09:33:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 283 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 849 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 847, ShuffleMapStage 848)
   [druid] 2019-06-11 09:33:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 847)
   [druid] 2019-06-11 09:33:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 847 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:33:20,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_424 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:20,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_424_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:33:20,010 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_424_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:20,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 424 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 847 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:33:20,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 847.0 with 2 tasks
   [druid] 2019-06-11 09:33:20,010 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 847.0 (TID 565, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:20,010 [er for task 565] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 847.0 (TID 565)
   [druid] 2019-06-11 09:33:20,019 [er for task 565] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 847.0 (TID 565). 853 bytes result sent to driver
   [druid] 2019-06-11 09:33:20,019 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 847.0 (TID 566, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:20,019 [er for task 566] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 847.0 (TID 566)
   [druid] 2019-06-11 09:33:20,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 847.0 (TID 565) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:33:20,027 [er for task 566] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 847.0 (TID 566). 810 bytes result sent to driver
   [druid] 2019-06-11 09:33:20,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 847.0 (TID 566) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:33:20,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 847.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 847 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:33:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:33:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:33:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 849)
   [druid] 2019-06-11 09:33:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:33:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 849 (MapPartitionsRDD[995] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:20,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_425 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:20,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_425_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:20,029 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_425_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:20,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 425 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 849 (MapPartitionsRDD[995] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:33:20,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 849.0 with 1 tasks
   [druid] 2019-06-11 09:33:20,030 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 849.0 (TID 567, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:20,030 [er for task 567] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 849.0 (TID 567)
   [druid] 2019-06-11 09:33:20,031 [er for task 567] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:20,031 [er for task 567] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:20,031 [er for task 567] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:20,031 [er for task 567] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:20,031 [er for task 567] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 849.0 (TID 567). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:33:20,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 849.0 (TID 567) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:20,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 849.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 849 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:33:20,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 283 finished: print at Transform.scala:36, took 0.024630 s
   [druid] 2019-06-11 09:33:20,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:20,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 283 is 160 bytes
   [druid] 2019-06-11 09:33:20,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 282 is 83 bytes
   [druid] 2019-06-11 09:33:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 284 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 852 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 850, ShuffleMapStage 851)
   [druid] 2019-06-11 09:33:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:33:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 852 (MapPartitionsRDD[995] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:20,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_426 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:20,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_426_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:20,037 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_426_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:20,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 426 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 852 (MapPartitionsRDD[995] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:33:20,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 852.0 with 1 tasks
   [druid] 2019-06-11 09:33:20,037 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 852.0 (TID 568, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:20,037 [er for task 568] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 852.0 (TID 568)
   [druid] 2019-06-11 09:33:20,038 [er for task 568] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:20,038 [er for task 568] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:20,038 [er for task 568] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:20,038 [er for task 568] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:20,039 [er for task 568] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 852.0 (TID 568). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:33:20,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 852.0 (TID 568) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:20,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 852.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 852 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:33:20,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 284 finished: print at Transform.scala:36, took 0.005809 s
   [druid] 2019-06-11 09:33:20,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216800000 ms.0 from job set of time 1560216800000 ms
   [druid] 2019-06-11 09:33:20,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560216800000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:33:20,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 988 from persistence list
   [druid] 2019-06-11 09:33:20,040 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 988
   [druid] 2019-06-11 09:33:20,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 983 from persistence list
   [druid] 2019-06-11 09:33:20,041 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 983
   [druid] 2019-06-11 09:33:20,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 982 from persistence list
   [druid] 2019-06-11 09:33:20,041 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 982
   [druid] 2019-06-11 09:33:20,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[982] at socketTextStream at Transform.scala:18 of time 1560216800000 ms
   [druid] 2019-06-11 09:33:20,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216790000 ms
   [druid] 2019-06-11 09:33:20,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216790000 ms
   [druid] 2019-06-11 09:33:25,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216805000 ms
   [druid] 2019-06-11 09:33:25,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216805000 ms.0 from job set of time 1560216805000 ms
   [druid] 2019-06-11 09:33:25,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:33:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 997 (map at Transform.scala:20)
   [druid] 2019-06-11 09:33:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 285 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 855 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 854, ShuffleMapStage 853)
   [druid] 2019-06-11 09:33:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 853)
   [druid] 2019-06-11 09:33:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 853 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:33:25,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_427 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:25,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_427_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:33:25,017 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_421_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,019 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_427_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,021 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 427 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:25,021 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_413_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,021 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 853 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:33:25,021 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 853.0 with 2 tasks
   [druid] 2019-06-11 09:33:25,021 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 853.0 (TID 569, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:25,021 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 273
   [druid] 2019-06-11 09:33:25,021 [er for task 569] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 853.0 (TID 569)
   [druid] 2019-06-11 09:33:25,022 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_420_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,022 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 281
   [druid] 2019-06-11 09:33:25,023 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 271
   [druid] 2019-06-11 09:33:25,023 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_425_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,024 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_411_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,025 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 270
   [druid] 2019-06-11 09:33:25,025 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 274
   [druid] 2019-06-11 09:33:25,025 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 272
   [druid] 2019-06-11 09:33:25,026 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_417_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,027 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 276
   [druid] 2019-06-11 09:33:25,027 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_415_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,028 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 280
   [druid] 2019-06-11 09:33:25,028 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_423_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,029 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_412_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,030 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_419_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,030 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_426_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,031 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_422_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,032 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_418_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,032 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 275
   [druid] 2019-06-11 09:33:25,032 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 278
   [druid] 2019-06-11 09:33:25,033 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_414_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,033 [er for task 569] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 853.0 (TID 569). 853 bytes result sent to driver
   [druid] 2019-06-11 09:33:25,033 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 853.0 (TID 570, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:25,033 [er for task 570] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 853.0 (TID 570)
   [druid] 2019-06-11 09:33:25,033 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 853.0 (TID 569) in 12 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:33:25,033 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_416_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,034 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_424_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,034 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 279
   [druid] 2019-06-11 09:33:25,034 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 277
   [druid] 2019-06-11 09:33:25,043 [er for task 570] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 853.0 (TID 570). 767 bytes result sent to driver
   [druid] 2019-06-11 09:33:25,044 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 853.0 (TID 570) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:33:25,044 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 853.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 853 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:33:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:33:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:33:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 855)
   [druid] 2019-06-11 09:33:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:33:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 855 (MapPartitionsRDD[1002] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:25,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_428 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:25,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_428_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:25,045 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_428_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 428 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:25,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 855 (MapPartitionsRDD[1002] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:33:25,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 855.0 with 1 tasks
   [druid] 2019-06-11 09:33:25,046 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 855.0 (TID 571, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:25,046 [er for task 571] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 855.0 (TID 571)
   [druid] 2019-06-11 09:33:25,047 [er for task 571] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:25,047 [er for task 571] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:25,047 [er for task 571] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:25,047 [er for task 571] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:25,048 [er for task 571] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 855.0 (TID 571). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:33:25,049 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 855.0 (TID 571) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:25,049 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 855.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:25,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 855 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:33:25,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 285 finished: print at Transform.scala:36, took 0.042216 s
   [druid] 2019-06-11 09:33:25,051 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:25,051 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 285 is 160 bytes
   [druid] 2019-06-11 09:33:25,051 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 284 is 83 bytes
   [druid] 2019-06-11 09:33:25,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 286 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:25,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 858 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:25,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 856, ShuffleMapStage 857)
   [druid] 2019-06-11 09:33:25,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:33:25,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 858 (MapPartitionsRDD[1002] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:25,052 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_429 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:25,053 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_429_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:25,053 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_429_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:25,053 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 429 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:25,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 858 (MapPartitionsRDD[1002] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:33:25,053 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 858.0 with 1 tasks
   [druid] 2019-06-11 09:33:25,054 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 858.0 (TID 572, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:25,054 [er for task 572] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 858.0 (TID 572)
   [druid] 2019-06-11 09:33:25,054 [er for task 572] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:25,054 [er for task 572] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:25,054 [er for task 572] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:25,054 [er for task 572] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:25,055 [er for task 572] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 858.0 (TID 572). 966 bytes result sent to driver
   [druid] 2019-06-11 09:33:25,055 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 858.0 (TID 572) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:25,055 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 858.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:25,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 858 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:33:25,056 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 286 finished: print at Transform.scala:36, took 0.004317 s
   [druid] 2019-06-11 09:33:25,056 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216805000 ms.0 from job set of time 1560216805000 ms
   [druid] 2019-06-11 09:33:25,056 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.056 s for time 1560216805000 ms (execution: 0.051 s)
   [druid] 2019-06-11 09:33:25,056 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 995 from persistence list
   [druid] 2019-06-11 09:33:25,056 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 995
   [druid] 2019-06-11 09:33:25,056 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 990 from persistence list
   [druid] 2019-06-11 09:33:25,056 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 990
   [druid] 2019-06-11 09:33:25,056 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 989 from persistence list
   [druid] 2019-06-11 09:33:25,056 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 989
   [druid] 2019-06-11 09:33:25,057 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[989] at socketTextStream at Transform.scala:18 of time 1560216805000 ms
   [druid] 2019-06-11 09:33:25,057 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216795000 ms
   [druid] 2019-06-11 09:33:25,057 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216795000 ms
   [druid] 2019-06-11 09:33:30,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216810000 ms
   [druid] 2019-06-11 09:33:30,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216810000 ms.0 from job set of time 1560216810000 ms
   [druid] 2019-06-11 09:33:30,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1004 (map at Transform.scala:20)
   [druid] 2019-06-11 09:33:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:33:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 287 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 861 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 859, ShuffleMapStage 860)
   [druid] 2019-06-11 09:33:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 860)
   [druid] 2019-06-11 09:33:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 860 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:33:30,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_430 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:30,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_430_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:33:30,009 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_430_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:30,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 430 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 860 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:33:30,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 860.0 with 2 tasks
   [druid] 2019-06-11 09:33:30,010 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 860.0 (TID 573, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:30,010 [er for task 573] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 860.0 (TID 573)
   [druid] 2019-06-11 09:33:30,019 [er for task 573] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 860.0 (TID 573). 810 bytes result sent to driver
   [druid] 2019-06-11 09:33:30,019 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 860.0 (TID 574, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:30,019 [er for task 574] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 860.0 (TID 574)
   [druid] 2019-06-11 09:33:30,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 860.0 (TID 573) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:33:30,027 [er for task 574] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 860.0 (TID 574). 767 bytes result sent to driver
   [druid] 2019-06-11 09:33:30,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 860.0 (TID 574) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:33:30,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 860.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:30,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 860 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:33:30,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:33:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:33:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 861)
   [druid] 2019-06-11 09:33:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:33:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 861 (MapPartitionsRDD[1009] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:30,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_431 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:30,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_431_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:30,029 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_431_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:30,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 431 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 861 (MapPartitionsRDD[1009] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:33:30,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 861.0 with 1 tasks
   [druid] 2019-06-11 09:33:30,030 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 861.0 (TID 575, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:30,030 [er for task 575] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 861.0 (TID 575)
   [druid] 2019-06-11 09:33:30,030 [er for task 575] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:30,030 [er for task 575] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:30,030 [er for task 575] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:30,030 [er for task 575] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:30,031 [er for task 575] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 861.0 (TID 575). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:33:30,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 861.0 (TID 575) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:30,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 861.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 861 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:33:30,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 287 finished: print at Transform.scala:36, took 0.024095 s
   [druid] 2019-06-11 09:33:30,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:30,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 286 is 83 bytes
   [druid] 2019-06-11 09:33:30,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 287 is 160 bytes
   [druid] 2019-06-11 09:33:30,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 288 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:30,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 864 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:30,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 862, ShuffleMapStage 863)
   [druid] 2019-06-11 09:33:30,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:33:30,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 864 (MapPartitionsRDD[1009] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:30,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_432 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:30,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_432_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:30,037 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_432_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:30,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 432 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 864 (MapPartitionsRDD[1009] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:33:30,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 864.0 with 1 tasks
   [druid] 2019-06-11 09:33:30,038 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 864.0 (TID 576, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:30,038 [er for task 576] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 864.0 (TID 576)
   [druid] 2019-06-11 09:33:30,038 [er for task 576] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:30,038 [er for task 576] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:30,039 [er for task 576] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:30,039 [er for task 576] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:33:30,039 [er for task 576] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 864.0 (TID 576). 966 bytes result sent to driver
   [druid] 2019-06-11 09:33:30,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 864.0 (TID 576) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:30,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 864.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:30,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 864 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:33:30,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 288 finished: print at Transform.scala:36, took 0.006009 s
   [druid] 2019-06-11 09:33:30,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216810000 ms.0 from job set of time 1560216810000 ms
   [druid] 2019-06-11 09:33:30,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560216810000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:33:30,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1002 from persistence list
   [druid] 2019-06-11 09:33:30,040 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1002
   [druid] 2019-06-11 09:33:30,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 997 from persistence list
   [druid] 2019-06-11 09:33:30,041 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 997
   [druid] 2019-06-11 09:33:30,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 996 from persistence list
   [druid] 2019-06-11 09:33:30,041 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 996
   [druid] 2019-06-11 09:33:30,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[996] at socketTextStream at Transform.scala:18 of time 1560216810000 ms
   [druid] 2019-06-11 09:33:30,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216800000 ms
   [druid] 2019-06-11 09:33:30,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216800000 ms
   [druid] 2019-06-11 09:33:35,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216815000 ms
   [druid] 2019-06-11 09:33:35,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216815000 ms.0 from job set of time 1560216815000 ms
   [druid] 2019-06-11 09:33:35,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:33:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1011 (map at Transform.scala:20)
   [druid] 2019-06-11 09:33:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 289 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 867 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 865, ShuffleMapStage 866)
   [druid] 2019-06-11 09:33:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 865)
   [druid] 2019-06-11 09:33:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 865 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:33:35,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_433 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_433_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:33:35,009 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_433_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:35,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 433 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:35,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 865 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:33:35,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 865.0 with 2 tasks
   [druid] 2019-06-11 09:33:35,010 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 865.0 (TID 577, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:35,010 [er for task 577] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 865.0 (TID 577)
   [druid] 2019-06-11 09:33:35,019 [er for task 577] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 865.0 (TID 577). 853 bytes result sent to driver
   [druid] 2019-06-11 09:33:35,020 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 865.0 (TID 578, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:35,020 [er for task 578] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 865.0 (TID 578)
   [druid] 2019-06-11 09:33:35,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 865.0 (TID 577) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:33:35,027 [er for task 578] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 865.0 (TID 578). 767 bytes result sent to driver
   [druid] 2019-06-11 09:33:35,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 865.0 (TID 578) in 7 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:33:35,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 865.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 865 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:33:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:33:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:33:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 867)
   [druid] 2019-06-11 09:33:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:33:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 867 (MapPartitionsRDD[1016] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:35,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_434 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:35,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_434_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:35,029 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_434_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:35,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 434 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:35,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 867 (MapPartitionsRDD[1016] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:33:35,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 867.0 with 1 tasks
   [druid] 2019-06-11 09:33:35,030 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 867.0 (TID 579, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:35,030 [er for task 579] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 867.0 (TID 579)
   [druid] 2019-06-11 09:33:35,030 [er for task 579] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:35,030 [er for task 579] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:35,031 [er for task 579] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:35,031 [er for task 579] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:33:35,031 [er for task 579] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 867.0 (TID 579). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:33:35,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 867.0 (TID 579) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:35,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 867.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 867 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:33:35,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 289 finished: print at Transform.scala:36, took 0.024506 s
   [druid] 2019-06-11 09:33:35,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:35,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 289 is 160 bytes
   [druid] 2019-06-11 09:33:35,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 288 is 83 bytes
   [druid] 2019-06-11 09:33:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 290 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 870 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 868, ShuffleMapStage 869)
   [druid] 2019-06-11 09:33:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:33:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 870 (MapPartitionsRDD[1016] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:35,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_435 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:35,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_435_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:35,036 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_435_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:35,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 435 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 870 (MapPartitionsRDD[1016] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:33:35,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 870.0 with 1 tasks
   [druid] 2019-06-11 09:33:35,037 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 870.0 (TID 580, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:35,037 [er for task 580] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 870.0 (TID 580)
   [druid] 2019-06-11 09:33:35,038 [er for task 580] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:35,038 [er for task 580] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:35,039 [er for task 580] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:35,039 [er for task 580] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:35,040 [er for task 580] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 870.0 (TID 580). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:33:35,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 870.0 (TID 580) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:35,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 870.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 870 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:33:35,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 290 finished: print at Transform.scala:36, took 0.006312 s
   [druid] 2019-06-11 09:33:35,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216815000 ms.0 from job set of time 1560216815000 ms
   [druid] 2019-06-11 09:33:35,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560216815000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:33:35,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1009 from persistence list
   [druid] 2019-06-11 09:33:35,041 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1009
   [druid] 2019-06-11 09:33:35,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1004 from persistence list
   [druid] 2019-06-11 09:33:35,041 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1004
   [druid] 2019-06-11 09:33:35,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1003 from persistence list
   [druid] 2019-06-11 09:33:35,041 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1003
   [druid] 2019-06-11 09:33:35,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1003] at socketTextStream at Transform.scala:18 of time 1560216815000 ms
   [druid] 2019-06-11 09:33:35,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216805000 ms
   [druid] 2019-06-11 09:33:35,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216805000 ms
   [druid] 2019-06-11 09:33:40,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216820000 ms
   [druid] 2019-06-11 09:33:40,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216820000 ms.0 from job set of time 1560216820000 ms
   [druid] 2019-06-11 09:33:40,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1018 (map at Transform.scala:20)
   [druid] 2019-06-11 09:33:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:33:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 291 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 873 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 871, ShuffleMapStage 872)
   [druid] 2019-06-11 09:33:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 872)
   [druid] 2019-06-11 09:33:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 872 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:33:40,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_436 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:40,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_436_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:33:40,010 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_436_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:40,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 436 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 872 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:33:40,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 872.0 with 2 tasks
   [druid] 2019-06-11 09:33:40,011 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 872.0 (TID 581, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:40,011 [er for task 581] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 872.0 (TID 581)
   [druid] 2019-06-11 09:33:40,022 [er for task 581] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 872.0 (TID 581). 810 bytes result sent to driver
   [druid] 2019-06-11 09:33:40,022 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 872.0 (TID 582, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:40,022 [er for task 582] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 872.0 (TID 582)
   [druid] 2019-06-11 09:33:40,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 872.0 (TID 581) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:33:40,030 [er for task 582] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 872.0 (TID 582). 810 bytes result sent to driver
   [druid] 2019-06-11 09:33:40,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 872.0 (TID 582) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:33:40,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 872.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 872 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:33:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:33:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:33:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 873)
   [druid] 2019-06-11 09:33:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:33:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 873 (MapPartitionsRDD[1023] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:40,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_437 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:40,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_437_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:40,032 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_437_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:40,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 437 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 873 (MapPartitionsRDD[1023] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:33:40,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 873.0 with 1 tasks
   [druid] 2019-06-11 09:33:40,032 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 873.0 (TID 583, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:40,032 [er for task 583] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 873.0 (TID 583)
   [druid] 2019-06-11 09:33:40,033 [er for task 583] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:40,033 [er for task 583] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:40,033 [er for task 583] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:40,033 [er for task 583] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:40,034 [er for task 583] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 873.0 (TID 583). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:33:40,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 873.0 (TID 583) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:40,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 873.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 873 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:33:40,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 291 finished: print at Transform.scala:36, took 0.026754 s
   [druid] 2019-06-11 09:33:40,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:40,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 290 is 83 bytes
   [druid] 2019-06-11 09:33:40,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 291 is 160 bytes
   [druid] 2019-06-11 09:33:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 292 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 876 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 874, ShuffleMapStage 875)
   [druid] 2019-06-11 09:33:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:33:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 876 (MapPartitionsRDD[1023] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:40,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_438 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:40,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_438_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:40,040 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_438_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:40,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 438 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:40,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 876 (MapPartitionsRDD[1023] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:33:40,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 876.0 with 1 tasks
   [druid] 2019-06-11 09:33:40,041 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 876.0 (TID 584, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:40,041 [er for task 584] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 876.0 (TID 584)
   [druid] 2019-06-11 09:33:40,041 [er for task 584] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:40,041 [er for task 584] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:40,041 [er for task 584] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:40,041 [er for task 584] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:40,042 [er for task 584] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 876.0 (TID 584). 966 bytes result sent to driver
   [druid] 2019-06-11 09:33:40,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 876.0 (TID 584) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:40,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 876.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 876 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:33:40,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 292 finished: print at Transform.scala:36, took 0.005082 s
   [druid] 2019-06-11 09:33:40,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216820000 ms.0 from job set of time 1560216820000 ms
   [druid] 2019-06-11 09:33:40,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216820000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:33:40,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1016 from persistence list
   [druid] 2019-06-11 09:33:40,043 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1016
   [druid] 2019-06-11 09:33:40,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1011 from persistence list
   [druid] 2019-06-11 09:33:40,043 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1011
   [druid] 2019-06-11 09:33:40,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1010 from persistence list
   [druid] 2019-06-11 09:33:40,044 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1010
   [druid] 2019-06-11 09:33:40,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1010] at socketTextStream at Transform.scala:18 of time 1560216820000 ms
   [druid] 2019-06-11 09:33:40,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216810000 ms
   [druid] 2019-06-11 09:33:40,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216810000 ms
   [druid] 2019-06-11 09:33:45,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216825000 ms
   [druid] 2019-06-11 09:33:45,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216825000 ms.0 from job set of time 1560216825000 ms
   [druid] 2019-06-11 09:33:45,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:33:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1025 (map at Transform.scala:20)
   [druid] 2019-06-11 09:33:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 293 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 879 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 877, ShuffleMapStage 878)
   [druid] 2019-06-11 09:33:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 877)
   [druid] 2019-06-11 09:33:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 877 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:33:45,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_439 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:45,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_439_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:33:45,010 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_439_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:45,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 439 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:45,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 877 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:33:45,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 877.0 with 2 tasks
   [druid] 2019-06-11 09:33:45,011 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 877.0 (TID 585, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:45,011 [er for task 585] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 877.0 (TID 585)
   [druid] 2019-06-11 09:33:45,021 [er for task 585] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 877.0 (TID 585). 810 bytes result sent to driver
   [druid] 2019-06-11 09:33:45,021 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 877.0 (TID 586, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:45,021 [er for task 586] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 877.0 (TID 586)
   [druid] 2019-06-11 09:33:45,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 877.0 (TID 585) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:33:45,030 [er for task 586] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 877.0 (TID 586). 810 bytes result sent to driver
   [druid] 2019-06-11 09:33:45,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 877.0 (TID 586) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:33:45,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 877.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:45,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 877 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:33:45,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:33:45,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:33:45,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 879)
   [druid] 2019-06-11 09:33:45,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:33:45,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 879 (MapPartitionsRDD[1030] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:45,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_440 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:45,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_440_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:45,033 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_440_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:45,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 440 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 879 (MapPartitionsRDD[1030] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:33:45,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 879.0 with 1 tasks
   [druid] 2019-06-11 09:33:45,033 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 879.0 (TID 587, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:45,033 [er for task 587] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 879.0 (TID 587)
   [druid] 2019-06-11 09:33:45,034 [er for task 587] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:45,034 [er for task 587] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:45,034 [er for task 587] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:45,034 [er for task 587] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:45,035 [er for task 587] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 879.0 (TID 587). 966 bytes result sent to driver
   [druid] 2019-06-11 09:33:45,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 879.0 (TID 587) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:45,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 879.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 879 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:33:45,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 293 finished: print at Transform.scala:36, took 0.027183 s
   [druid] 2019-06-11 09:33:45,038 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:45,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 293 is 160 bytes
   [druid] 2019-06-11 09:33:45,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 292 is 83 bytes
   [druid] 2019-06-11 09:33:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 294 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 882 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 880, ShuffleMapStage 881)
   [druid] 2019-06-11 09:33:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:33:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 882 (MapPartitionsRDD[1030] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:45,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_441 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:45,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_441_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:45,040 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_441_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:45,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 441 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 882 (MapPartitionsRDD[1030] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:33:45,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 882.0 with 1 tasks
   [druid] 2019-06-11 09:33:45,040 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 882.0 (TID 588, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:45,041 [er for task 588] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 882.0 (TID 588)
   [druid] 2019-06-11 09:33:45,041 [er for task 588] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:45,041 [er for task 588] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:45,041 [er for task 588] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:45,041 [er for task 588] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:45,042 [er for task 588] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 882.0 (TID 588). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:33:45,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 882.0 (TID 588) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:45,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 882.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 882 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:33:45,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 294 finished: print at Transform.scala:36, took 0.005338 s
   [druid] 2019-06-11 09:33:45,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216825000 ms.0 from job set of time 1560216825000 ms
   [druid] 2019-06-11 09:33:45,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216825000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:33:45,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1023 from persistence list
   [druid] 2019-06-11 09:33:45,043 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1023
   [druid] 2019-06-11 09:33:45,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1018 from persistence list
   [druid] 2019-06-11 09:33:45,044 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1018
   [druid] 2019-06-11 09:33:45,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1017 from persistence list
   [druid] 2019-06-11 09:33:45,044 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1017
   [druid] 2019-06-11 09:33:45,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1017] at socketTextStream at Transform.scala:18 of time 1560216825000 ms
   [druid] 2019-06-11 09:33:45,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216815000 ms
   [druid] 2019-06-11 09:33:45,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216815000 ms
   [druid] 2019-06-11 09:33:50,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216830000 ms
   [druid] 2019-06-11 09:33:50,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216830000 ms.0 from job set of time 1560216830000 ms
   [druid] 2019-06-11 09:33:50,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:33:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1032 (map at Transform.scala:20)
   [druid] 2019-06-11 09:33:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 295 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 885 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 883, ShuffleMapStage 884)
   [druid] 2019-06-11 09:33:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 883)
   [druid] 2019-06-11 09:33:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 883 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:33:50,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_442 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:50,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_442_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:33:50,010 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_442_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 442 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:50,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 883 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:33:50,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 883.0 with 2 tasks
   [druid] 2019-06-11 09:33:50,012 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 883.0 (TID 589, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:50,012 [er for task 589] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 883.0 (TID 589)
   [druid] 2019-06-11 09:33:50,021 [er for task 589] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 883.0 (TID 589). 810 bytes result sent to driver
   [druid] 2019-06-11 09:33:50,021 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 883.0 (TID 590, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:50,021 [er for task 590] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 883.0 (TID 590)
   [druid] 2019-06-11 09:33:50,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 883.0 (TID 589) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:33:50,029 [er for task 590] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 883.0 (TID 590). 767 bytes result sent to driver
   [druid] 2019-06-11 09:33:50,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 883.0 (TID 590) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:33:50,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 883.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 883 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:33:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:33:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:33:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 885)
   [druid] 2019-06-11 09:33:50,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:33:50,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 885 (MapPartitionsRDD[1037] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:50,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_443 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:50,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_443_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:33:50,037 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 291
   [druid] 2019-06-11 09:33:50,040 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_443_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,041 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_438_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 443 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:50,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 885 (MapPartitionsRDD[1037] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:33:50,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 885.0 with 1 tasks
   [druid] 2019-06-11 09:33:50,043 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 885.0 (TID 591, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:50,043 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_435_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,043 [er for task 591] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 885.0 (TID 591)
   [druid] 2019-06-11 09:33:50,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 286
   [druid] 2019-06-11 09:33:50,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 285
   [druid] 2019-06-11 09:33:50,044 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_434_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,045 [er for task 591] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:50,045 [er for task 591] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:50,045 [er for task 591] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:50,045 [er for task 591] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:50,045 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_427_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,046 [er for task 591] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 885.0 (TID 591). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:33:50,046 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_437_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,046 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 885.0 (TID 591) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:50,046 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 885.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:50,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 885 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:33:50,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 295 finished: print at Transform.scala:36, took 0.038193 s
   [druid] 2019-06-11 09:33:50,047 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_433_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,048 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_428_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,049 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_431_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,050 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_441_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,051 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_430_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,052 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:50,052 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_439_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,052 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 295 is 160 bytes
   [druid] 2019-06-11 09:33:50,052 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 294 is 83 bytes
   [druid] 2019-06-11 09:33:50,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 288
   [druid] 2019-06-11 09:33:50,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 296 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:50,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 888 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:50,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 887, ShuffleMapStage 886)
   [druid] 2019-06-11 09:33:50,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:33:50,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 888 (MapPartitionsRDD[1037] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:50,053 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_432_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,053 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_444 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:50,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 282
   [druid] 2019-06-11 09:33:50,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 284
   [druid] 2019-06-11 09:33:50,054 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 290
   [druid] 2019-06-11 09:33:50,054 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_444_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:50,054 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_429_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,055 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_444_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 287
   [druid] 2019-06-11 09:33:50,055 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 444 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:50,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 888 (MapPartitionsRDD[1037] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:33:50,056 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 888.0 with 1 tasks
   [druid] 2019-06-11 09:33:50,056 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 888.0 (TID 592, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:50,056 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_440_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,056 [er for task 592] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 888.0 (TID 592)
   [druid] 2019-06-11 09:33:50,057 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_436_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:50,057 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 289
   [druid] 2019-06-11 09:33:50,058 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 283
   [druid] 2019-06-11 09:33:50,058 [er for task 592] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:50,058 [er for task 592] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:50,058 [er for task 592] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:50,058 [er for task 592] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:50,059 [er for task 592] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 888.0 (TID 592). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:33:50,060 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 888.0 (TID 592) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:50,060 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 888.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:50,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 888 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:33:50,060 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 296 finished: print at Transform.scala:36, took 0.008085 s
   [druid] 2019-06-11 09:33:50,060 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216830000 ms.0 from job set of time 1560216830000 ms
   [druid] 2019-06-11 09:33:50,060 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.060 s for time 1560216830000 ms (execution: 0.054 s)
   [druid] 2019-06-11 09:33:50,060 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1030 from persistence list
   [druid] 2019-06-11 09:33:50,060 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1030
   [druid] 2019-06-11 09:33:50,061 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1025 from persistence list
   [druid] 2019-06-11 09:33:50,061 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1025
   [druid] 2019-06-11 09:33:50,061 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1024 from persistence list
   [druid] 2019-06-11 09:33:50,061 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1024
   [druid] 2019-06-11 09:33:50,061 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1024] at socketTextStream at Transform.scala:18 of time 1560216830000 ms
   [druid] 2019-06-11 09:33:50,061 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216820000 ms
   [druid] 2019-06-11 09:33:50,061 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216820000 ms
   [druid] 2019-06-11 09:33:55,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216835000 ms
   [druid] 2019-06-11 09:33:55,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216835000 ms.0 from job set of time 1560216835000 ms
   [druid] 2019-06-11 09:33:55,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1039 (map at Transform.scala:20)
   [druid] 2019-06-11 09:33:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:33:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 297 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 891 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 889, ShuffleMapStage 890)
   [druid] 2019-06-11 09:33:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 890)
   [druid] 2019-06-11 09:33:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 890 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:33:55,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_445 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:55,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_445_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:33:55,010 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_445_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:55,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 445 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:55,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 890 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:33:55,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 890.0 with 2 tasks
   [druid] 2019-06-11 09:33:55,011 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 890.0 (TID 593, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:55,011 [er for task 593] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 890.0 (TID 593)
   [druid] 2019-06-11 09:33:55,020 [er for task 593] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 890.0 (TID 593). 853 bytes result sent to driver
   [druid] 2019-06-11 09:33:55,020 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 890.0 (TID 594, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:33:55,020 [er for task 594] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 890.0 (TID 594)
   [druid] 2019-06-11 09:33:55,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 890.0 (TID 593) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:33:55,029 [er for task 594] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 890.0 (TID 594). 810 bytes result sent to driver
   [druid] 2019-06-11 09:33:55,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 890.0 (TID 594) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:33:55,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 890.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 890 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:33:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:33:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:33:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 891)
   [druid] 2019-06-11 09:33:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:33:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 891 (MapPartitionsRDD[1044] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:55,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_446 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:55,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_446_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:55,031 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_446_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:55,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 446 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:55,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 891 (MapPartitionsRDD[1044] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:33:55,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 891.0 with 1 tasks
   [druid] 2019-06-11 09:33:55,032 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 891.0 (TID 595, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:55,032 [er for task 595] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 891.0 (TID 595)
   [druid] 2019-06-11 09:33:55,034 [er for task 595] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:55,034 [er for task 595] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:55,034 [er for task 595] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:55,034 [er for task 595] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:55,035 [er for task 595] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 891.0 (TID 595). 966 bytes result sent to driver
   [druid] 2019-06-11 09:33:55,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 891.0 (TID 595) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:55,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 891.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 891 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:33:55,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 297 finished: print at Transform.scala:36, took 0.026627 s
   [druid] 2019-06-11 09:33:55,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:33:55,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 296 is 83 bytes
   [druid] 2019-06-11 09:33:55,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 297 is 160 bytes
   [druid] 2019-06-11 09:33:55,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 298 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:33:55,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 894 (print at Transform.scala:36)
   [druid] 2019-06-11 09:33:55,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 892, ShuffleMapStage 893)
   [druid] 2019-06-11 09:33:55,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:33:55,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 894 (MapPartitionsRDD[1044] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:33:55,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_447 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:55,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_447_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:33:55,040 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_447_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:33:55,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 447 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:33:55,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 894 (MapPartitionsRDD[1044] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:33:55,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 894.0 with 1 tasks
   [druid] 2019-06-11 09:33:55,040 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 894.0 (TID 596, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:33:55,040 [er for task 596] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 894.0 (TID 596)
   [druid] 2019-06-11 09:33:55,041 [er for task 596] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:33:55,041 [er for task 596] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:55,041 [er for task 596] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:33:55,041 [er for task 596] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:33:55,042 [er for task 596] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 894.0 (TID 596). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:33:55,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 894.0 (TID 596) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:33:55,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 894.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:33:55,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 894 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:33:55,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 298 finished: print at Transform.scala:36, took 0.005106 s
   [druid] 2019-06-11 09:33:55,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216835000 ms.0 from job set of time 1560216835000 ms
   [druid] 2019-06-11 09:33:55,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216835000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:33:55,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1037 from persistence list
   [druid] 2019-06-11 09:33:55,043 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1037
   [druid] 2019-06-11 09:33:55,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1032 from persistence list
   [druid] 2019-06-11 09:33:55,043 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1032
   [druid] 2019-06-11 09:33:55,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1031 from persistence list
   [druid] 2019-06-11 09:33:55,044 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1031
   [druid] 2019-06-11 09:33:55,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1031] at socketTextStream at Transform.scala:18 of time 1560216835000 ms
   [druid] 2019-06-11 09:33:55,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216825000 ms
   [druid] 2019-06-11 09:33:55,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216825000 ms
   [druid] 2019-06-11 09:34:00,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216840000 ms
   [druid] 2019-06-11 09:34:00,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216840000 ms.0 from job set of time 1560216840000 ms
   [druid] 2019-06-11 09:34:00,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1046 (map at Transform.scala:20)
   [druid] 2019-06-11 09:34:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:34:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 299 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 897 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 895, ShuffleMapStage 896)
   [druid] 2019-06-11 09:34:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 896)
   [druid] 2019-06-11 09:34:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 896 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:34:00,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_448 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:00,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_448_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:34:00,012 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_448_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:00,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 448 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 896 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:34:00,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 896.0 with 2 tasks
   [druid] 2019-06-11 09:34:00,013 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 896.0 (TID 597, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:00,013 [er for task 597] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 896.0 (TID 597)
   [druid] 2019-06-11 09:34:00,022 [er for task 597] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 896.0 (TID 597). 810 bytes result sent to driver
   [druid] 2019-06-11 09:34:00,022 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 896.0 (TID 598, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:00,023 [er for task 598] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 896.0 (TID 598)
   [druid] 2019-06-11 09:34:00,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 896.0 (TID 597) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:34:00,030 [er for task 598] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 896.0 (TID 598). 767 bytes result sent to driver
   [druid] 2019-06-11 09:34:00,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 896.0 (TID 598) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:34:00,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 896.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 896 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:34:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:34:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:34:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 897)
   [druid] 2019-06-11 09:34:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:34:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 897 (MapPartitionsRDD[1051] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:00,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_449 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:00,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_449_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:00,032 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_449_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:00,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 449 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 897 (MapPartitionsRDD[1051] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:34:00,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 897.0 with 1 tasks
   [druid] 2019-06-11 09:34:00,033 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 897.0 (TID 599, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:00,033 [er for task 599] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 897.0 (TID 599)
   [druid] 2019-06-11 09:34:00,034 [er for task 599] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:00,034 [er for task 599] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:00,034 [er for task 599] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:00,034 [er for task 599] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:00,034 [er for task 599] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 897.0 (TID 599). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:00,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 897.0 (TID 599) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:00,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 897.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:00,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 897 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:00,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 299 finished: print at Transform.scala:36, took 0.024233 s
   [druid] 2019-06-11 09:34:00,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:00,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 298 is 83 bytes
   [druid] 2019-06-11 09:34:00,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 299 is 160 bytes
   [druid] 2019-06-11 09:34:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 300 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 900 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 898, ShuffleMapStage 899)
   [druid] 2019-06-11 09:34:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:34:00,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 900 (MapPartitionsRDD[1051] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:00,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_450 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:00,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_450_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:00,040 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_450_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:00,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 450 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 900 (MapPartitionsRDD[1051] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:34:00,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 900.0 with 1 tasks
   [druid] 2019-06-11 09:34:00,040 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 900.0 (TID 600, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:00,041 [er for task 600] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 900.0 (TID 600)
   [druid] 2019-06-11 09:34:00,041 [er for task 600] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:00,041 [er for task 600] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:00,041 [er for task 600] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:00,041 [er for task 600] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:00,042 [er for task 600] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 900.0 (TID 600). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:34:00,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 900.0 (TID 600) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:00,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 900.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:00,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 900 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:00,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 300 finished: print at Transform.scala:36, took 0.006041 s
   [druid] 2019-06-11 09:34:00,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216840000 ms.0 from job set of time 1560216840000 ms
   [druid] 2019-06-11 09:34:00,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216840000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:34:00,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1044 from persistence list
   [druid] 2019-06-11 09:34:00,043 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1044
   [druid] 2019-06-11 09:34:00,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1039 from persistence list
   [druid] 2019-06-11 09:34:00,043 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1039
   [druid] 2019-06-11 09:34:00,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1038 from persistence list
   [druid] 2019-06-11 09:34:00,044 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1038
   [druid] 2019-06-11 09:34:00,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1038] at socketTextStream at Transform.scala:18 of time 1560216840000 ms
   [druid] 2019-06-11 09:34:00,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216830000 ms
   [druid] 2019-06-11 09:34:00,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216830000 ms
   [druid] 2019-06-11 09:34:05,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216845000 ms
   [druid] 2019-06-11 09:34:05,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216845000 ms.0 from job set of time 1560216845000 ms
   [druid] 2019-06-11 09:34:05,006 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1053 (map at Transform.scala:20)
   [druid] 2019-06-11 09:34:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:34:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 301 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 903 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 901, ShuffleMapStage 902)
   [druid] 2019-06-11 09:34:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 902)
   [druid] 2019-06-11 09:34:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 902 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:34:05,007 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_451 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:05,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_451_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:34:05,009 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_451_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:05,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 451 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 902 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:34:05,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 902.0 with 2 tasks
   [druid] 2019-06-11 09:34:05,009 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 902.0 (TID 601, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:05,009 [er for task 601] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 902.0 (TID 601)
   [druid] 2019-06-11 09:34:05,019 [er for task 601] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 902.0 (TID 601). 853 bytes result sent to driver
   [druid] 2019-06-11 09:34:05,019 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 902.0 (TID 602, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:05,019 [er for task 602] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 902.0 (TID 602)
   [druid] 2019-06-11 09:34:05,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 902.0 (TID 601) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:34:05,027 [er for task 602] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 902.0 (TID 602). 810 bytes result sent to driver
   [druid] 2019-06-11 09:34:05,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 902.0 (TID 602) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:34:05,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 902.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 902 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:34:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:34:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:34:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 903)
   [druid] 2019-06-11 09:34:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:34:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 903 (MapPartitionsRDD[1058] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:05,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_452 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:05,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_452_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:05,029 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_452_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:05,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 452 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 903 (MapPartitionsRDD[1058] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:34:05,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 903.0 with 1 tasks
   [druid] 2019-06-11 09:34:05,029 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 903.0 (TID 603, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:05,029 [er for task 603] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 903.0 (TID 603)
   [druid] 2019-06-11 09:34:05,030 [er for task 603] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:05,030 [er for task 603] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:05,030 [er for task 603] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:05,030 [er for task 603] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:05,031 [er for task 603] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 903.0 (TID 603). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:34:05,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 903.0 (TID 603) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:05,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 903.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 903 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:05,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 301 finished: print at Transform.scala:36, took 0.025118 s
   [druid] 2019-06-11 09:34:05,033 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:05,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 300 is 83 bytes
   [druid] 2019-06-11 09:34:05,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 301 is 160 bytes
   [druid] 2019-06-11 09:34:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 302 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 906 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 905, ShuffleMapStage 904)
   [druid] 2019-06-11 09:34:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:34:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 906 (MapPartitionsRDD[1058] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:05,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_453 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:05,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_453_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:05,035 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_453_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:05,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 453 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 906 (MapPartitionsRDD[1058] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:34:05,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 906.0 with 1 tasks
   [druid] 2019-06-11 09:34:05,036 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 906.0 (TID 604, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:05,036 [er for task 604] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 906.0 (TID 604)
   [druid] 2019-06-11 09:34:05,037 [er for task 604] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:05,037 [er for task 604] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:05,038 [er for task 604] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:05,038 [er for task 604] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:05,039 [er for task 604] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 906.0 (TID 604). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:05,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 906.0 (TID 604) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:05,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 906.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 906 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:34:05,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 302 finished: print at Transform.scala:36, took 0.006138 s
   [druid] 2019-06-11 09:34:05,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216845000 ms.0 from job set of time 1560216845000 ms
   [druid] 2019-06-11 09:34:05,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.039 s for time 1560216845000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:34:05,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1051 from persistence list
   [druid] 2019-06-11 09:34:05,040 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1051
   [druid] 2019-06-11 09:34:05,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1046 from persistence list
   [druid] 2019-06-11 09:34:05,040 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1046
   [druid] 2019-06-11 09:34:05,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1045 from persistence list
   [druid] 2019-06-11 09:34:05,040 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1045
   [druid] 2019-06-11 09:34:05,040 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1045] at socketTextStream at Transform.scala:18 of time 1560216845000 ms
   [druid] 2019-06-11 09:34:05,040 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216835000 ms
   [druid] 2019-06-11 09:34:05,040 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216835000 ms
   [druid] 2019-06-11 09:34:10,012 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216850000 ms
   [druid] 2019-06-11 09:34:10,012 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216850000 ms.0 from job set of time 1560216850000 ms
   [druid] 2019-06-11 09:34:10,014 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:10,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1060 (map at Transform.scala:20)
   [druid] 2019-06-11 09:34:10,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:34:10,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 303 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:10,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 909 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:10,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 907, ShuffleMapStage 908)
   [druid] 2019-06-11 09:34:10,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 908)
   [druid] 2019-06-11 09:34:10,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 908 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:34:10,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_454 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:10,016 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_454_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:34:10,016 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_454_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:10,016 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 454 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:10,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 908 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:34:10,017 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 908.0 with 2 tasks
   [druid] 2019-06-11 09:34:10,017 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 908.0 (TID 605, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:10,017 [er for task 605] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 908.0 (TID 605)
   [druid] 2019-06-11 09:34:10,031 [er for task 605] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 908.0 (TID 605). 853 bytes result sent to driver
   [druid] 2019-06-11 09:34:10,031 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 908.0 (TID 606, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:10,031 [er for task 606] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 908.0 (TID 606)
   [druid] 2019-06-11 09:34:10,031 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 908.0 (TID 605) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:34:10,041 [er for task 606] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 908.0 (TID 606). 767 bytes result sent to driver
   [druid] 2019-06-11 09:34:10,041 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 908.0 (TID 606) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:34:10,041 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 908.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 908 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:34:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:34:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:34:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 909)
   [druid] 2019-06-11 09:34:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:34:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 909 (MapPartitionsRDD[1065] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:10,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_455 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:10,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_455_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:10,042 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_455_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:10,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 455 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 909 (MapPartitionsRDD[1065] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:34:10,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 909.0 with 1 tasks
   [druid] 2019-06-11 09:34:10,043 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 909.0 (TID 607, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:10,043 [er for task 607] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 909.0 (TID 607)
   [druid] 2019-06-11 09:34:10,044 [er for task 607] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:10,044 [er for task 607] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:10,044 [er for task 607] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:10,044 [er for task 607] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:10,044 [er for task 607] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 909.0 (TID 607). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:10,045 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 909.0 (TID 607) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:10,045 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 909.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:10,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 909 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:10,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 303 finished: print at Transform.scala:36, took 0.031049 s
   [druid] 2019-06-11 09:34:10,049 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:10,049 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 302 is 83 bytes
   [druid] 2019-06-11 09:34:10,049 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 303 is 160 bytes
   [druid] 2019-06-11 09:34:10,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 304 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:10,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 912 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:10,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 910, ShuffleMapStage 911)
   [druid] 2019-06-11 09:34:10,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:34:10,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 912 (MapPartitionsRDD[1065] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:10,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_456 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:10,051 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_456_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:10,051 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_456_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:10,052 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 456 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:10,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 912 (MapPartitionsRDD[1065] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:34:10,052 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 912.0 with 1 tasks
   [druid] 2019-06-11 09:34:10,052 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 912.0 (TID 608, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:10,052 [er for task 608] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 912.0 (TID 608)
   [druid] 2019-06-11 09:34:10,053 [er for task 608] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:10,053 [er for task 608] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:10,053 [er for task 608] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:10,053 [er for task 608] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:10,054 [er for task 608] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 912.0 (TID 608). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:10,054 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 912.0 (TID 608) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:10,054 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 912.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:10,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 912 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:10,054 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 304 finished: print at Transform.scala:36, took 0.005581 s
   [druid] 2019-06-11 09:34:10,055 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216850000 ms.0 from job set of time 1560216850000 ms
   [druid] 2019-06-11 09:34:10,055 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.055 s for time 1560216850000 ms (execution: 0.043 s)
   [druid] 2019-06-11 09:34:10,055 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1058 from persistence list
   [druid] 2019-06-11 09:34:10,055 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1058
   [druid] 2019-06-11 09:34:10,055 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1053 from persistence list
   [druid] 2019-06-11 09:34:10,055 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1053
   [druid] 2019-06-11 09:34:10,055 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1052 from persistence list
   [druid] 2019-06-11 09:34:10,055 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1052
   [druid] 2019-06-11 09:34:10,055 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1052] at socketTextStream at Transform.scala:18 of time 1560216850000 ms
   [druid] 2019-06-11 09:34:10,055 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216840000 ms
   [druid] 2019-06-11 09:34:10,056 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216840000 ms
   [druid] 2019-06-11 09:34:15,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216855000 ms
   [druid] 2019-06-11 09:34:15,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216855000 ms.0 from job set of time 1560216855000 ms
   [druid] 2019-06-11 09:34:15,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1067 (map at Transform.scala:20)
   [druid] 2019-06-11 09:34:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:34:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 305 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 915 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 913, ShuffleMapStage 914)
   [druid] 2019-06-11 09:34:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 914)
   [druid] 2019-06-11 09:34:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 914 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:34:15,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_457 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:15,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_457_piece0 stored as bytes in memory (estimated size 1310.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:34:15,010 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_457_piece0 in memory on 192.168.91.1:63518 (size: 1310.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 457 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 914 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:34:15,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 914.0 with 2 tasks
   [druid] 2019-06-11 09:34:15,010 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 914.0 (TID 609, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:15,011 [er for task 609] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 914.0 (TID 609)
   [druid] 2019-06-11 09:34:15,019 [er for task 609] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 914.0 (TID 609). 810 bytes result sent to driver
   [druid] 2019-06-11 09:34:15,020 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 914.0 (TID 610, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:15,020 [er for task 610] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 914.0 (TID 610)
   [druid] 2019-06-11 09:34:15,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 914.0 (TID 609) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:34:15,028 [er for task 610] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 914.0 (TID 610). 767 bytes result sent to driver
   [druid] 2019-06-11 09:34:15,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 914.0 (TID 610) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:34:15,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 914.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 914 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:34:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:34:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:34:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 915)
   [druid] 2019-06-11 09:34:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:34:15,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 915 (MapPartitionsRDD[1072] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:15,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_458 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:15,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_458_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:15,030 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_458_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 458 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 915 (MapPartitionsRDD[1072] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:34:15,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 915.0 with 1 tasks
   [druid] 2019-06-11 09:34:15,030 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 915.0 (TID 611, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:15,031 [er for task 611] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 915.0 (TID 611)
   [druid] 2019-06-11 09:34:15,031 [er for task 611] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:15,031 [er for task 611] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:15,031 [er for task 611] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:15,031 [er for task 611] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:15,032 [er for task 611] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 915.0 (TID 611). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:34:15,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 915.0 (TID 611) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:15,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 915.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:15,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 915 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:15,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 305 finished: print at Transform.scala:36, took 0.023944 s
   [druid] 2019-06-11 09:34:15,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:15,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 304 is 83 bytes
   [druid] 2019-06-11 09:34:15,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 305 is 160 bytes
   [druid] 2019-06-11 09:34:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 306 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 918 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 916, ShuffleMapStage 917)
   [druid] 2019-06-11 09:34:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:34:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 918 (MapPartitionsRDD[1072] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:15,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_459 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:15,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_459_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:15,045 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_451_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,046 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_459_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,048 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 459 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:15,048 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 292
   [druid] 2019-06-11 09:34:15,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 918 (MapPartitionsRDD[1072] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:34:15,049 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 918.0 with 1 tasks
   [druid] 2019-06-11 09:34:15,049 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_443_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,049 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 918.0 (TID 612, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:15,050 [er for task 612] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 918.0 (TID 612)
   [druid] 2019-06-11 09:34:15,050 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_449_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,050 [er for task 612] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:15,050 [er for task 612] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:15,051 [er for task 612] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:15,051 [er for task 612] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:34:15,051 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_446_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,051 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_444_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,051 [er for task 612] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 918.0 (TID 612). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:15,051 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 918.0 (TID 612) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:15,051 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 918.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:15,051 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_452_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 918 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:15,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 306 finished: print at Transform.scala:36, took 0.017249 s
   [druid] 2019-06-11 09:34:15,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216855000 ms.0 from job set of time 1560216855000 ms
   [druid] 2019-06-11 09:34:15,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.052 s for time 1560216855000 ms (execution: 0.046 s)
   [druid] 2019-06-11 09:34:15,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1065 from persistence list
   [druid] 2019-06-11 09:34:15,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 299
   [druid] 2019-06-11 09:34:15,052 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1065
   [druid] 2019-06-11 09:34:15,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 297
   [druid] 2019-06-11 09:34:15,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1060 from persistence list
   [druid] 2019-06-11 09:34:15,052 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1060
   [druid] 2019-06-11 09:34:15,052 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1059 from persistence list
   [druid] 2019-06-11 09:34:15,052 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1059
   [druid] 2019-06-11 09:34:15,052 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1059] at socketTextStream at Transform.scala:18 of time 1560216855000 ms
   [druid] 2019-06-11 09:34:15,052 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216845000 ms
   [druid] 2019-06-11 09:34:15,052 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_450_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,052 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216845000 ms
   [druid] 2019-06-11 09:34:15,053 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_458_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 296
   [druid] 2019-06-11 09:34:15,054 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_448_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,055 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_445_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,055 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_455_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,056 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_442_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,056 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 300
   [druid] 2019-06-11 09:34:15,057 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 293
   [druid] 2019-06-11 09:34:15,057 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 301
   [druid] 2019-06-11 09:34:15,057 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_454_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,058 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_456_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,059 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_457_piece0 on 192.168.91.1:63518 in memory (size: 1310.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,059 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 295
   [druid] 2019-06-11 09:34:15,059 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 298
   [druid] 2019-06-11 09:34:15,060 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_447_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:15,060 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 294
   [druid] 2019-06-11 09:34:15,061 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_453_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:20,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216860000 ms
   [druid] 2019-06-11 09:34:20,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216860000 ms.0 from job set of time 1560216860000 ms
   [druid] 2019-06-11 09:34:20,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:34:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1074 (map at Transform.scala:20)
   [druid] 2019-06-11 09:34:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 307 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 921 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 919, ShuffleMapStage 920)
   [druid] 2019-06-11 09:34:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 919)
   [druid] 2019-06-11 09:34:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 919 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:34:20,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_460 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:20,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_460_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:34:20,009 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_460_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:20,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 460 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 919 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:34:20,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 919.0 with 2 tasks
   [druid] 2019-06-11 09:34:20,010 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 919.0 (TID 613, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:20,010 [er for task 613] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 919.0 (TID 613)
   [druid] 2019-06-11 09:34:20,019 [er for task 613] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 919.0 (TID 613). 853 bytes result sent to driver
   [druid] 2019-06-11 09:34:20,019 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 919.0 (TID 614, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:20,019 [er for task 614] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 919.0 (TID 614)
   [druid] 2019-06-11 09:34:20,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 919.0 (TID 613) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:34:20,027 [er for task 614] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 919.0 (TID 614). 767 bytes result sent to driver
   [druid] 2019-06-11 09:34:20,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 919.0 (TID 614) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:34:20,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 919.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:20,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 919 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:34:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:34:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:34:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 921)
   [druid] 2019-06-11 09:34:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:34:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 921 (MapPartitionsRDD[1079] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:20,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_461 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:20,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_461_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:20,029 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_461_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:20,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 461 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 921 (MapPartitionsRDD[1079] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:34:20,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 921.0 with 1 tasks
   [druid] 2019-06-11 09:34:20,030 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 921.0 (TID 615, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:20,030 [er for task 615] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 921.0 (TID 615)
   [druid] 2019-06-11 09:34:20,030 [er for task 615] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:20,030 [er for task 615] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:20,030 [er for task 615] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:20,030 [er for task 615] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:20,031 [er for task 615] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 921.0 (TID 615). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:34:20,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 921.0 (TID 615) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:20,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 921.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 921 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:34:20,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 307 finished: print at Transform.scala:36, took 0.023936 s
   [druid] 2019-06-11 09:34:20,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:20,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 307 is 160 bytes
   [druid] 2019-06-11 09:34:20,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 306 is 83 bytes
   [druid] 2019-06-11 09:34:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 308 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 924 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 922, ShuffleMapStage 923)
   [druid] 2019-06-11 09:34:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:34:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 924 (MapPartitionsRDD[1079] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:20,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_462 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:20,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_462_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:20,036 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_462_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:20,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 462 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 924 (MapPartitionsRDD[1079] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:34:20,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 924.0 with 1 tasks
   [druid] 2019-06-11 09:34:20,037 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 924.0 (TID 616, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:20,037 [er for task 616] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 924.0 (TID 616)
   [druid] 2019-06-11 09:34:20,038 [er for task 616] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:20,038 [er for task 616] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:20,038 [er for task 616] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:20,038 [er for task 616] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:20,038 [er for task 616] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 924.0 (TID 616). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:20,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 924.0 (TID 616) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:20,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 924.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 924 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:20,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 308 finished: print at Transform.scala:36, took 0.004665 s
   [druid] 2019-06-11 09:34:20,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216860000 ms.0 from job set of time 1560216860000 ms
   [druid] 2019-06-11 09:34:20,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.039 s for time 1560216860000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:34:20,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1072 from persistence list
   [druid] 2019-06-11 09:34:20,040 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1072
   [druid] 2019-06-11 09:34:20,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1067 from persistence list
   [druid] 2019-06-11 09:34:20,040 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1067
   [druid] 2019-06-11 09:34:20,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1066 from persistence list
   [druid] 2019-06-11 09:34:20,040 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1066
   [druid] 2019-06-11 09:34:20,040 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1066] at socketTextStream at Transform.scala:18 of time 1560216860000 ms
   [druid] 2019-06-11 09:34:20,040 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216850000 ms
   [druid] 2019-06-11 09:34:20,040 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216850000 ms
   [druid] 2019-06-11 09:34:25,010 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216865000 ms
   [druid] 2019-06-11 09:34:25,010 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216865000 ms.0 from job set of time 1560216865000 ms
   [druid] 2019-06-11 09:34:25,011 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:34:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1081 (map at Transform.scala:20)
   [druid] 2019-06-11 09:34:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 309 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 927 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 925, ShuffleMapStage 926)
   [druid] 2019-06-11 09:34:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 925)
   [druid] 2019-06-11 09:34:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 925 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:34:25,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_463 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:25,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_463_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:34:25,013 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_463_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:25,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 463 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:25,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 925 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:34:25,014 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 925.0 with 2 tasks
   [druid] 2019-06-11 09:34:25,014 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 925.0 (TID 617, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:25,014 [er for task 617] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 925.0 (TID 617)
   [druid] 2019-06-11 09:34:25,023 [er for task 617] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 925.0 (TID 617). 810 bytes result sent to driver
   [druid] 2019-06-11 09:34:25,023 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 925.0 (TID 618, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:25,023 [er for task 618] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 925.0 (TID 618)
   [druid] 2019-06-11 09:34:25,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 925.0 (TID 617) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:34:25,032 [er for task 618] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 925.0 (TID 618). 853 bytes result sent to driver
   [druid] 2019-06-11 09:34:25,032 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 925.0 (TID 618) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:34:25,032 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 925.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 925 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:34:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:34:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:34:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 927)
   [druid] 2019-06-11 09:34:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:34:25,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 927 (MapPartitionsRDD[1086] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:25,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_464 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:25,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_464_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:25,034 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_464_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:25,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 464 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 927 (MapPartitionsRDD[1086] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:34:25,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 927.0 with 1 tasks
   [druid] 2019-06-11 09:34:25,035 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 927.0 (TID 619, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:25,035 [er for task 619] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 927.0 (TID 619)
   [druid] 2019-06-11 09:34:25,035 [er for task 619] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:25,035 [er for task 619] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:25,035 [er for task 619] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:25,035 [er for task 619] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:25,036 [er for task 619] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 927.0 (TID 619). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:25,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 927.0 (TID 619) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:25,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 927.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:25,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 927 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:34:25,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 309 finished: print at Transform.scala:36, took 0.025102 s
   [druid] 2019-06-11 09:34:25,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:25,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 309 is 160 bytes
   [druid] 2019-06-11 09:34:25,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 308 is 83 bytes
   [druid] 2019-06-11 09:34:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 310 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 930 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 928, ShuffleMapStage 929)
   [druid] 2019-06-11 09:34:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:34:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 930 (MapPartitionsRDD[1086] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:25,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_465 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:25,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_465_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:25,041 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_465_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:25,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 465 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:25,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 930 (MapPartitionsRDD[1086] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:34:25,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 930.0 with 1 tasks
   [druid] 2019-06-11 09:34:25,042 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 930.0 (TID 620, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:25,042 [er for task 620] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 930.0 (TID 620)
   [druid] 2019-06-11 09:34:25,043 [er for task 620] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:25,043 [er for task 620] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:25,043 [er for task 620] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:25,043 [er for task 620] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:25,043 [er for task 620] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 930.0 (TID 620). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:25,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 930.0 (TID 620) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:25,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 930.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 930 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:25,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 310 finished: print at Transform.scala:36, took 0.004776 s
   [druid] 2019-06-11 09:34:25,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216865000 ms.0 from job set of time 1560216865000 ms
   [druid] 2019-06-11 09:34:25,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.044 s for time 1560216865000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:34:25,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1079 from persistence list
   [druid] 2019-06-11 09:34:25,045 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1079
   [druid] 2019-06-11 09:34:25,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1074 from persistence list
   [druid] 2019-06-11 09:34:25,045 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1074
   [druid] 2019-06-11 09:34:25,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1073 from persistence list
   [druid] 2019-06-11 09:34:25,045 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1073
   [druid] 2019-06-11 09:34:25,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1073] at socketTextStream at Transform.scala:18 of time 1560216865000 ms
   [druid] 2019-06-11 09:34:25,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216855000 ms
   [druid] 2019-06-11 09:34:25,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216855000 ms
   [druid] 2019-06-11 09:34:30,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216870000 ms
   [druid] 2019-06-11 09:34:30,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216870000 ms.0 from job set of time 1560216870000 ms
   [druid] 2019-06-11 09:34:30,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:34:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1088 (map at Transform.scala:20)
   [druid] 2019-06-11 09:34:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 311 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 933 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 931, ShuffleMapStage 932)
   [druid] 2019-06-11 09:34:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 931)
   [druid] 2019-06-11 09:34:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 931 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:34:30,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_466 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:30,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_466_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:34:30,009 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_466_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:30,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 466 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 931 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:34:30,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 931.0 with 2 tasks
   [druid] 2019-06-11 09:34:30,010 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 931.0 (TID 621, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:30,010 [er for task 621] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 931.0 (TID 621)
   [druid] 2019-06-11 09:34:30,019 [er for task 621] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 931.0 (TID 621). 853 bytes result sent to driver
   [druid] 2019-06-11 09:34:30,019 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 931.0 (TID 622, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:30,019 [er for task 622] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 931.0 (TID 622)
   [druid] 2019-06-11 09:34:30,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 931.0 (TID 621) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:34:30,027 [er for task 622] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 931.0 (TID 622). 767 bytes result sent to driver
   [druid] 2019-06-11 09:34:30,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 931.0 (TID 622) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:34:30,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 931.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:30,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 931 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:34:30,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:34:30,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:34:30,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 933)
   [druid] 2019-06-11 09:34:30,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:34:30,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 933 (MapPartitionsRDD[1093] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:30,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_467 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:30,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_467_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:30,029 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_467_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:30,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 467 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 933 (MapPartitionsRDD[1093] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:34:30,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 933.0 with 1 tasks
   [druid] 2019-06-11 09:34:30,029 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 933.0 (TID 623, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:30,029 [er for task 623] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 933.0 (TID 623)
   [druid] 2019-06-11 09:34:30,030 [er for task 623] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:30,030 [er for task 623] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:30,030 [er for task 623] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:30,030 [er for task 623] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:30,031 [er for task 623] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 933.0 (TID 623). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:34:30,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 933.0 (TID 623) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:30,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 933.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 933 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:30,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 311 finished: print at Transform.scala:36, took 0.023357 s
   [druid] 2019-06-11 09:34:30,033 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:30,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 311 is 160 bytes
   [druid] 2019-06-11 09:34:30,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 310 is 83 bytes
   [druid] 2019-06-11 09:34:30,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 312 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:30,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 936 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:30,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 934, ShuffleMapStage 935)
   [druid] 2019-06-11 09:34:30,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:34:30,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 936 (MapPartitionsRDD[1093] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:30,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_468 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:30,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_468_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:30,035 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_468_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:30,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 468 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 936 (MapPartitionsRDD[1093] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:34:30,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 936.0 with 1 tasks
   [druid] 2019-06-11 09:34:30,036 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 936.0 (TID 624, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:30,036 [er for task 624] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 936.0 (TID 624)
   [druid] 2019-06-11 09:34:30,037 [er for task 624] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:30,037 [er for task 624] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:30,037 [er for task 624] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:30,037 [er for task 624] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:30,037 [er for task 624] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 936.0 (TID 624). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:30,038 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 936.0 (TID 624) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:30,038 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 936.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 936 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:30,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 312 finished: print at Transform.scala:36, took 0.004853 s
   [druid] 2019-06-11 09:34:30,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216870000 ms.0 from job set of time 1560216870000 ms
   [druid] 2019-06-11 09:34:30,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.038 s for time 1560216870000 ms (execution: 0.033 s)
   [druid] 2019-06-11 09:34:30,038 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1086 from persistence list
   [druid] 2019-06-11 09:34:30,039 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1086
   [druid] 2019-06-11 09:34:30,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1081 from persistence list
   [druid] 2019-06-11 09:34:30,039 [-thread-pool-15] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1081
   [druid] 2019-06-11 09:34:30,039 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1080 from persistence list
   [druid] 2019-06-11 09:34:30,039 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1080
   [druid] 2019-06-11 09:34:30,039 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1080] at socketTextStream at Transform.scala:18 of time 1560216870000 ms
   [druid] 2019-06-11 09:34:30,039 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216860000 ms
   [druid] 2019-06-11 09:34:30,039 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216860000 ms
   [druid] 2019-06-11 09:34:35,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216875000 ms
   [druid] 2019-06-11 09:34:35,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216875000 ms.0 from job set of time 1560216875000 ms
   [druid] 2019-06-11 09:34:35,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:34:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1095 (map at Transform.scala:20)
   [druid] 2019-06-11 09:34:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 313 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 939 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 938, ShuffleMapStage 937)
   [druid] 2019-06-11 09:34:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 937)
   [druid] 2019-06-11 09:34:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 937 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:34:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_469 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:35,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_469_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:34:35,010 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_469_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:35,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 469 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 937 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:34:35,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 937.0 with 2 tasks
   [druid] 2019-06-11 09:34:35,011 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 937.0 (TID 625, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:35,011 [er for task 625] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 937.0 (TID 625)
   [druid] 2019-06-11 09:34:35,020 [er for task 625] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 937.0 (TID 625). 853 bytes result sent to driver
   [druid] 2019-06-11 09:34:35,020 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 937.0 (TID 626, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:35,020 [er for task 626] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 937.0 (TID 626)
   [druid] 2019-06-11 09:34:35,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 937.0 (TID 625) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:34:35,028 [er for task 626] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 937.0 (TID 626). 810 bytes result sent to driver
   [druid] 2019-06-11 09:34:35,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 937.0 (TID 626) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:34:35,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 937.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 937 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:34:35,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:34:35,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:34:35,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 939)
   [druid] 2019-06-11 09:34:35,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:34:35,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 939 (MapPartitionsRDD[1100] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:35,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_470 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:35,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_470_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:35,030 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_470_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:35,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 470 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 939 (MapPartitionsRDD[1100] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:34:35,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 939.0 with 1 tasks
   [druid] 2019-06-11 09:34:35,030 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 939.0 (TID 627, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:35,031 [er for task 627] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 939.0 (TID 627)
   [druid] 2019-06-11 09:34:35,032 [er for task 627] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:35,032 [er for task 627] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:35,032 [er for task 627] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:35,032 [er for task 627] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:35,033 [er for task 627] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 939.0 (TID 627). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:35,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 939.0 (TID 627) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:35,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 939.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:35,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 939 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:34:35,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 313 finished: print at Transform.scala:36, took 0.024361 s
   [druid] 2019-06-11 09:34:35,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:35,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 313 is 160 bytes
   [druid] 2019-06-11 09:34:35,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 312 is 83 bytes
   [druid] 2019-06-11 09:34:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 314 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 942 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 940, ShuffleMapStage 941)
   [druid] 2019-06-11 09:34:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:34:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 942 (MapPartitionsRDD[1100] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:35,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_471 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:35,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_471_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:35,037 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_471_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:35,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 471 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 942 (MapPartitionsRDD[1100] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:34:35,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 942.0 with 1 tasks
   [druid] 2019-06-11 09:34:35,038 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 942.0 (TID 628, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:35,038 [er for task 628] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 942.0 (TID 628)
   [druid] 2019-06-11 09:34:35,039 [er for task 628] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:35,039 [er for task 628] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:35,039 [er for task 628] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:35,039 [er for task 628] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:35,040 [er for task 628] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 942.0 (TID 628). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:34:35,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 942.0 (TID 628) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:35,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 942.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 942 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:35,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 314 finished: print at Transform.scala:36, took 0.005423 s
   [druid] 2019-06-11 09:34:35,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216875000 ms.0 from job set of time 1560216875000 ms
   [druid] 2019-06-11 09:34:35,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560216875000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:34:35,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1093 from persistence list
   [druid] 2019-06-11 09:34:35,041 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1093
   [druid] 2019-06-11 09:34:35,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1088 from persistence list
   [druid] 2019-06-11 09:34:35,041 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1088
   [druid] 2019-06-11 09:34:35,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1087 from persistence list
   [druid] 2019-06-11 09:34:35,041 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1087
   [druid] 2019-06-11 09:34:35,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1087] at socketTextStream at Transform.scala:18 of time 1560216875000 ms
   [druid] 2019-06-11 09:34:35,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216865000 ms
   [druid] 2019-06-11 09:34:35,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216865000 ms
   [druid] 2019-06-11 09:34:40,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216880000 ms
   [druid] 2019-06-11 09:34:40,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216880000 ms.0 from job set of time 1560216880000 ms
   [druid] 2019-06-11 09:34:40,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:34:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1102 (map at Transform.scala:20)
   [druid] 2019-06-11 09:34:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 315 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 945 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 943, ShuffleMapStage 944)
   [druid] 2019-06-11 09:34:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 943)
   [druid] 2019-06-11 09:34:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 943 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:34:40,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_472 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:40,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_472_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:34:40,010 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_472_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:40,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 472 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 943 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:34:40,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 943.0 with 2 tasks
   [druid] 2019-06-11 09:34:40,010 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 943.0 (TID 629, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:40,011 [er for task 629] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 943.0 (TID 629)
   [druid] 2019-06-11 09:34:40,020 [er for task 629] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 943.0 (TID 629). 810 bytes result sent to driver
   [druid] 2019-06-11 09:34:40,020 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 943.0 (TID 630, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:40,020 [er for task 630] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 943.0 (TID 630)
   [druid] 2019-06-11 09:34:40,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 943.0 (TID 629) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:34:40,028 [er for task 630] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 943.0 (TID 630). 810 bytes result sent to driver
   [druid] 2019-06-11 09:34:40,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 943.0 (TID 630) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:34:40,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 943.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 943 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:34:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:34:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:34:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 945)
   [druid] 2019-06-11 09:34:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:34:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 945 (MapPartitionsRDD[1107] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:40,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_473 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:40,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_473_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:40,030 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_473_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:40,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 473 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 945 (MapPartitionsRDD[1107] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:34:40,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 945.0 with 1 tasks
   [druid] 2019-06-11 09:34:40,030 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 945.0 (TID 631, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:40,030 [er for task 631] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 945.0 (TID 631)
   [druid] 2019-06-11 09:34:40,031 [er for task 631] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:40,031 [er for task 631] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:40,031 [er for task 631] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:40,031 [er for task 631] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:40,032 [er for task 631] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 945.0 (TID 631). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:40,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 945.0 (TID 631) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:40,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 945.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 945 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:40,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 315 finished: print at Transform.scala:36, took 0.024030 s
   [druid] 2019-06-11 09:34:40,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:40,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 315 is 160 bytes
   [druid] 2019-06-11 09:34:40,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 314 is 83 bytes
   [druid] 2019-06-11 09:34:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 316 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 948 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 946, ShuffleMapStage 947)
   [druid] 2019-06-11 09:34:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:34:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 948 (MapPartitionsRDD[1107] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:40,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_474 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:40,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_474_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:34:40,037 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_474_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:40,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 474 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 948 (MapPartitionsRDD[1107] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:34:40,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 948.0 with 1 tasks
   [druid] 2019-06-11 09:34:40,038 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 948.0 (TID 632, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:40,038 [er for task 632] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 948.0 (TID 632)
   [druid] 2019-06-11 09:34:40,039 [er for task 632] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:40,039 [er for task 632] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:40,039 [er for task 632] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:40,039 [er for task 632] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:40,040 [er for task 632] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 948.0 (TID 632). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:34:40,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 948.0 (TID 632) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:40,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 948.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:40,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 948 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:40,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 316 finished: print at Transform.scala:36, took 0.006206 s
   [druid] 2019-06-11 09:34:40,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216880000 ms.0 from job set of time 1560216880000 ms
   [druid] 2019-06-11 09:34:40,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560216880000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:34:40,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1100 from persistence list
   [druid] 2019-06-11 09:34:40,041 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1100
   [druid] 2019-06-11 09:34:40,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1095 from persistence list
   [druid] 2019-06-11 09:34:40,041 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1095
   [druid] 2019-06-11 09:34:40,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1094 from persistence list
   [druid] 2019-06-11 09:34:40,042 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1094
   [druid] 2019-06-11 09:34:40,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1094] at socketTextStream at Transform.scala:18 of time 1560216880000 ms
   [druid] 2019-06-11 09:34:40,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216870000 ms
   [druid] 2019-06-11 09:34:40,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216870000 ms
   [druid] 2019-06-11 09:34:45,009 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 306
   [druid] 2019-06-11 09:34:45,012 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 307
   [druid] 2019-06-11 09:34:45,015 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_472_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,017 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_474_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,018 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_464_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,018 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216885000 ms
   [druid] 2019-06-11 09:34:45,018 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216885000 ms.0 from job set of time 1560216885000 ms
   [druid] 2019-06-11 09:34:45,018 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_463_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,019 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 303
   [druid] 2019-06-11 09:34:45,019 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_470_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,020 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_473_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,021 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_460_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,022 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_467_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,022 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:45,022 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_461_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1109 (map at Transform.scala:20)
   [druid] 2019-06-11 09:34:45,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:34:45,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 317 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:45,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 951 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:45,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 949, ShuffleMapStage 950)
   [druid] 2019-06-11 09:34:45,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 950)
   [druid] 2019-06-11 09:34:45,023 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_465_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 950 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:34:45,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 304
   [druid] 2019-06-11 09:34:45,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 309
   [druid] 2019-06-11 09:34:45,024 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_475 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:45,025 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 310
   [druid] 2019-06-11 09:34:45,025 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_459_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,026 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_475_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:34:45,026 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_475_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,026 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_466_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,026 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 475 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:45,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 950 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:34:45,027 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 950.0 with 2 tasks
   [druid] 2019-06-11 09:34:45,027 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_469_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,027 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 950.0 (TID 633, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:45,027 [er for task 633] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 950.0 (TID 633)
   [druid] 2019-06-11 09:34:45,028 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_471_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,029 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 313
   [druid] 2019-06-11 09:34:45,029 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 305
   [druid] 2019-06-11 09:34:45,030 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_468_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,030 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 311
   [druid] 2019-06-11 09:34:45,030 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 308
   [druid] 2019-06-11 09:34:45,031 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_462_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,032 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 312
   [druid] 2019-06-11 09:34:45,032 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 302
   [druid] 2019-06-11 09:34:45,039 [er for task 633] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 950.0 (TID 633). 810 bytes result sent to driver
   [druid] 2019-06-11 09:34:45,039 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 950.0 (TID 634, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:45,039 [er for task 634] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 950.0 (TID 634)
   [druid] 2019-06-11 09:34:45,039 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 950.0 (TID 633) in 12 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:34:45,049 [er for task 634] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 950.0 (TID 634). 853 bytes result sent to driver
   [druid] 2019-06-11 09:34:45,050 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 950.0 (TID 634) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:34:45,050 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 950.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 950 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:34:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:34:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:34:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 951)
   [druid] 2019-06-11 09:34:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:34:45,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 951 (MapPartitionsRDD[1114] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:45,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_476 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:45,051 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_476_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:45,051 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_476_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,052 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 476 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 951 (MapPartitionsRDD[1114] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:34:45,052 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 951.0 with 1 tasks
   [druid] 2019-06-11 09:34:45,052 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 951.0 (TID 635, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:45,053 [er for task 635] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 951.0 (TID 635)
   [druid] 2019-06-11 09:34:45,053 [er for task 635] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:45,053 [er for task 635] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:45,053 [er for task 635] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:45,053 [er for task 635] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:45,054 [er for task 635] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 951.0 (TID 635). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:45,054 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 951.0 (TID 635) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:45,055 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 951.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:45,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 951 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:34:45,055 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 317 finished: print at Transform.scala:36, took 0.032279 s
   [druid] 2019-06-11 09:34:45,057 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:45,057 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 316 is 83 bytes
   [druid] 2019-06-11 09:34:45,057 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 317 is 160 bytes
   [druid] 2019-06-11 09:34:45,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 318 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:45,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 954 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:45,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 952, ShuffleMapStage 953)
   [druid] 2019-06-11 09:34:45,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:34:45,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 954 (MapPartitionsRDD[1114] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:45,057 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_477 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:45,058 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_477_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:45,058 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_477_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:45,058 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 477 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:45,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 954 (MapPartitionsRDD[1114] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:34:45,059 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 954.0 with 1 tasks
   [druid] 2019-06-11 09:34:45,060 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 954.0 (TID 636, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:45,060 [er for task 636] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 954.0 (TID 636)
   [druid] 2019-06-11 09:34:45,060 [er for task 636] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:45,060 [er for task 636] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:45,060 [er for task 636] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:45,060 [er for task 636] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:45,061 [er for task 636] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 954.0 (TID 636). 923 bytes result sent to driver
   [druid] 2019-06-11 09:34:45,061 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 954.0 (TID 636) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:45,061 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 954.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:45,061 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 954 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:45,061 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 318 finished: print at Transform.scala:36, took 0.004805 s
   [druid] 2019-06-11 09:34:45,061 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216885000 ms.0 from job set of time 1560216885000 ms
   [druid] 2019-06-11 09:34:45,061 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.061 s for time 1560216885000 ms (execution: 0.043 s)
   [druid] 2019-06-11 09:34:45,061 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1107 from persistence list
   [druid] 2019-06-11 09:34:45,062 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1107
   [druid] 2019-06-11 09:34:45,062 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1102 from persistence list
   [druid] 2019-06-11 09:34:45,062 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1102
   [druid] 2019-06-11 09:34:45,062 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1101 from persistence list
   [druid] 2019-06-11 09:34:45,062 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1101] at socketTextStream at Transform.scala:18 of time 1560216885000 ms
   [druid] 2019-06-11 09:34:45,062 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216875000 ms
   [druid] 2019-06-11 09:34:45,062 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216875000 ms
   [druid] 2019-06-11 09:34:45,062 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1101
   [druid] 2019-06-11 09:34:50,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216890000 ms
   [druid] 2019-06-11 09:34:50,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216890000 ms.0 from job set of time 1560216890000 ms
   [druid] 2019-06-11 09:34:50,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:34:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1116 (map at Transform.scala:20)
   [druid] 2019-06-11 09:34:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 319 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 957 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 956, ShuffleMapStage 955)
   [druid] 2019-06-11 09:34:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 955)
   [druid] 2019-06-11 09:34:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 955 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:34:50,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_478 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:50,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_478_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:34:50,010 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_478_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:50,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 478 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:50,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 955 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:34:50,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 955.0 with 2 tasks
   [druid] 2019-06-11 09:34:50,011 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 955.0 (TID 637, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:50,011 [er for task 637] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 955.0 (TID 637)
   [druid] 2019-06-11 09:34:50,019 [er for task 637] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 955.0 (TID 637). 853 bytes result sent to driver
   [druid] 2019-06-11 09:34:50,020 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 955.0 (TID 638, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:50,020 [er for task 638] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 955.0 (TID 638)
   [druid] 2019-06-11 09:34:50,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 955.0 (TID 637) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:34:50,027 [er for task 638] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 955.0 (TID 638). 767 bytes result sent to driver
   [druid] 2019-06-11 09:34:50,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 955.0 (TID 638) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:34:50,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 955.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 955 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:34:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:34:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:34:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 957)
   [druid] 2019-06-11 09:34:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:34:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 957 (MapPartitionsRDD[1121] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:50,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_479 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:50,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_479_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:50,029 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_479_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:50,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 479 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:50,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 957 (MapPartitionsRDD[1121] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:34:50,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 957.0 with 1 tasks
   [druid] 2019-06-11 09:34:50,030 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 957.0 (TID 639, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:50,030 [er for task 639] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 957.0 (TID 639)
   [druid] 2019-06-11 09:34:50,031 [er for task 639] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:50,031 [er for task 639] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:50,031 [er for task 639] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:50,031 [er for task 639] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:50,032 [er for task 639] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 957.0 (TID 639). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:34:50,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 957.0 (TID 639) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:50,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 957.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:50,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 957 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:50,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 319 finished: print at Transform.scala:36, took 0.024078 s
   [druid] 2019-06-11 09:34:50,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:50,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 319 is 160 bytes
   [druid] 2019-06-11 09:34:50,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 318 is 83 bytes
   [druid] 2019-06-11 09:34:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 320 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 960 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 958, ShuffleMapStage 959)
   [druid] 2019-06-11 09:34:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:34:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 960 (MapPartitionsRDD[1121] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:50,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_480 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:50,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_480_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:50,037 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_480_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:50,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 480 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 960 (MapPartitionsRDD[1121] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:34:50,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 960.0 with 1 tasks
   [druid] 2019-06-11 09:34:50,038 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 960.0 (TID 640, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:50,038 [er for task 640] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 960.0 (TID 640)
   [druid] 2019-06-11 09:34:50,039 [er for task 640] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:50,039 [er for task 640] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:50,039 [er for task 640] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:50,039 [er for task 640] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:50,040 [er for task 640] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 960.0 (TID 640). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:50,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 960.0 (TID 640) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:50,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 960.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:50,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 960 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:34:50,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 320 finished: print at Transform.scala:36, took 0.006370 s
   [druid] 2019-06-11 09:34:50,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216890000 ms.0 from job set of time 1560216890000 ms
   [druid] 2019-06-11 09:34:50,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560216890000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:34:50,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1114 from persistence list
   [druid] 2019-06-11 09:34:50,041 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1114
   [druid] 2019-06-11 09:34:50,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1109 from persistence list
   [druid] 2019-06-11 09:34:50,042 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1109
   [druid] 2019-06-11 09:34:50,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1108 from persistence list
   [druid] 2019-06-11 09:34:50,042 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1108
   [druid] 2019-06-11 09:34:50,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1108] at socketTextStream at Transform.scala:18 of time 1560216890000 ms
   [druid] 2019-06-11 09:34:50,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216880000 ms
   [druid] 2019-06-11 09:34:50,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216880000 ms
   [druid] 2019-06-11 09:34:55,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216895000 ms
   [druid] 2019-06-11 09:34:55,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216895000 ms.0 from job set of time 1560216895000 ms
   [druid] 2019-06-11 09:34:55,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:34:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1123 (map at Transform.scala:20)
   [druid] 2019-06-11 09:34:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 321 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 963 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 961, ShuffleMapStage 962)
   [druid] 2019-06-11 09:34:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 961)
   [druid] 2019-06-11 09:34:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 961 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:34:55,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_481 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:55,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_481_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:34:55,009 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_481_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:55,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 481 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 961 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:34:55,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 961.0 with 2 tasks
   [druid] 2019-06-11 09:34:55,010 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 961.0 (TID 641, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:55,010 [er for task 641] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 961.0 (TID 641)
   [druid] 2019-06-11 09:34:55,019 [er for task 641] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 961.0 (TID 641). 810 bytes result sent to driver
   [druid] 2019-06-11 09:34:55,019 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 961.0 (TID 642, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:34:55,019 [er for task 642] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 961.0 (TID 642)
   [druid] 2019-06-11 09:34:55,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 961.0 (TID 641) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:34:55,027 [er for task 642] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 961.0 (TID 642). 810 bytes result sent to driver
   [druid] 2019-06-11 09:34:55,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 961.0 (TID 642) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:34:55,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 961.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 961 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:34:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:34:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:34:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 963)
   [druid] 2019-06-11 09:34:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:34:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 963 (MapPartitionsRDD[1128] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:55,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_482 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:55,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_482_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:55,029 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_482_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:55,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 482 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 963 (MapPartitionsRDD[1128] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:34:55,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 963.0 with 1 tasks
   [druid] 2019-06-11 09:34:55,030 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 963.0 (TID 643, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:55,030 [er for task 643] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 963.0 (TID 643)
   [druid] 2019-06-11 09:34:55,030 [er for task 643] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:55,030 [er for task 643] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:55,031 [er for task 643] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:55,031 [er for task 643] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:55,031 [er for task 643] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 963.0 (TID 643). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:34:55,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 963.0 (TID 643) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:55,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 963.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:55,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 963 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:34:55,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 321 finished: print at Transform.scala:36, took 0.024590 s
   [druid] 2019-06-11 09:34:55,033 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:34:55,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 321 is 160 bytes
   [druid] 2019-06-11 09:34:55,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 320 is 83 bytes
   [druid] 2019-06-11 09:34:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 322 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:34:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 966 (print at Transform.scala:36)
   [druid] 2019-06-11 09:34:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 964, ShuffleMapStage 965)
   [druid] 2019-06-11 09:34:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:34:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 966 (MapPartitionsRDD[1128] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:34:55,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_483 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:55,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_483_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:34:55,036 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_483_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:34:55,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 483 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:34:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 966 (MapPartitionsRDD[1128] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:34:55,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 966.0 with 1 tasks
   [druid] 2019-06-11 09:34:55,036 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 966.0 (TID 644, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:34:55,037 [er for task 644] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 966.0 (TID 644)
   [druid] 2019-06-11 09:34:55,037 [er for task 644] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:34:55,037 [er for task 644] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:55,037 [er for task 644] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:34:55,037 [er for task 644] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:34:55,038 [er for task 644] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 966.0 (TID 644). 966 bytes result sent to driver
   [druid] 2019-06-11 09:34:55,038 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 966.0 (TID 644) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:34:55,038 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 966.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:34:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 966 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:34:55,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 322 finished: print at Transform.scala:36, took 0.005016 s
   [druid] 2019-06-11 09:34:55,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216895000 ms.0 from job set of time 1560216895000 ms
   [druid] 2019-06-11 09:34:55,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.039 s for time 1560216895000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:34:55,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1121 from persistence list
   [druid] 2019-06-11 09:34:55,039 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1121
   [druid] 2019-06-11 09:34:55,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1116 from persistence list
   [druid] 2019-06-11 09:34:55,039 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1116
   [druid] 2019-06-11 09:34:55,039 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1115 from persistence list
   [druid] 2019-06-11 09:34:55,040 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1115
   [druid] 2019-06-11 09:34:55,040 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1115] at socketTextStream at Transform.scala:18 of time 1560216895000 ms
   [druid] 2019-06-11 09:34:55,040 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216885000 ms
   [druid] 2019-06-11 09:34:55,040 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216885000 ms
   [druid] 2019-06-11 09:35:00,012 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216900000 ms
   [druid] 2019-06-11 09:35:00,012 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216900000 ms.0 from job set of time 1560216900000 ms
   [druid] 2019-06-11 09:35:00,013 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:00,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:35:00,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1130 (map at Transform.scala:20)
   [druid] 2019-06-11 09:35:00,113 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 323 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:00,113 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 969 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:00,113 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 967, ShuffleMapStage 968)
   [druid] 2019-06-11 09:35:00,113 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 967)
   [druid] 2019-06-11 09:35:00,113 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 967 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:35:00,113 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_484 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:00,114 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_484_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:35:00,114 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_484_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:00,115 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 484 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:00,115 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 967 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:35:00,115 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 967.0 with 2 tasks
   [druid] 2019-06-11 09:35:00,115 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 967.0 (TID 645, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:00,115 [er for task 645] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 967.0 (TID 645)
   [druid] 2019-06-11 09:35:00,123 [er for task 645] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 967.0 (TID 645). 810 bytes result sent to driver
   [druid] 2019-06-11 09:35:00,124 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 967.0 (TID 646, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:00,124 [er for task 646] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 967.0 (TID 646)
   [druid] 2019-06-11 09:35:00,124 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 967.0 (TID 645) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:35:00,133 [er for task 646] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 967.0 (TID 646). 767 bytes result sent to driver
   [druid] 2019-06-11 09:35:00,133 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 967.0 (TID 646) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:35:00,133 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 967.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:00,133 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 967 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:35:00,133 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:35:00,133 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:35:00,133 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 969)
   [druid] 2019-06-11 09:35:00,133 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:35:00,134 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 969 (MapPartitionsRDD[1135] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:00,134 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_485 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:00,135 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_485_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:00,135 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_485_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:00,135 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 485 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:00,135 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 969 (MapPartitionsRDD[1135] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:35:00,135 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 969.0 with 1 tasks
   [druid] 2019-06-11 09:35:00,136 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 969.0 (TID 647, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:00,136 [er for task 647] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 969.0 (TID 647)
   [druid] 2019-06-11 09:35:00,137 [er for task 647] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:00,137 [er for task 647] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:35:00,137 [er for task 647] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:00,137 [er for task 647] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:00,137 [er for task 647] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 969.0 (TID 647). 966 bytes result sent to driver
   [druid] 2019-06-11 09:35:00,138 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 969.0 (TID 647) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:00,138 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 969.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:00,138 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 969 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:35:00,138 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 323 finished: print at Transform.scala:36, took 0.124457 s
   [druid] 2019-06-11 09:35:00,142 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:00,142 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 323 is 160 bytes
   [druid] 2019-06-11 09:35:00,142 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 322 is 83 bytes
   [druid] 2019-06-11 09:35:00,142 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 324 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:00,142 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 972 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:00,142 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 970, ShuffleMapStage 971)
   [druid] 2019-06-11 09:35:00,142 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:35:00,143 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 972 (MapPartitionsRDD[1135] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:00,143 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_486 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:00,144 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_486_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:00,144 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_486_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:00,144 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 486 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:00,145 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 972 (MapPartitionsRDD[1135] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:35:00,145 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 972.0 with 1 tasks
   [druid] 2019-06-11 09:35:00,145 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 972.0 (TID 648, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:00,145 [er for task 648] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 972.0 (TID 648)
   [druid] 2019-06-11 09:35:00,146 [er for task 648] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:00,146 [er for task 648] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:00,146 [er for task 648] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:00,146 [er for task 648] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:00,146 [er for task 648] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 972.0 (TID 648). 966 bytes result sent to driver
   [druid] 2019-06-11 09:35:00,147 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 972.0 (TID 648) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:00,147 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 972.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:00,147 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 972 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:00,147 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 324 finished: print at Transform.scala:36, took 0.005216 s
   [druid] 2019-06-11 09:35:00,147 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216900000 ms.0 from job set of time 1560216900000 ms
   [druid] 2019-06-11 09:35:00,147 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.147 s for time 1560216900000 ms (execution: 0.135 s)
   [druid] 2019-06-11 09:35:00,147 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1128 from persistence list
   [druid] 2019-06-11 09:35:00,147 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1128
   [druid] 2019-06-11 09:35:00,147 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1123 from persistence list
   [druid] 2019-06-11 09:35:00,148 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1123
   [druid] 2019-06-11 09:35:00,148 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1122 from persistence list
   [druid] 2019-06-11 09:35:00,148 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1122
   [druid] 2019-06-11 09:35:00,148 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1122] at socketTextStream at Transform.scala:18 of time 1560216900000 ms
   [druid] 2019-06-11 09:35:00,148 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216890000 ms
   [druid] 2019-06-11 09:35:00,148 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216890000 ms
   [druid] 2019-06-11 09:35:05,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216905000 ms
   [druid] 2019-06-11 09:35:05,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216905000 ms.0 from job set of time 1560216905000 ms
   [druid] 2019-06-11 09:35:05,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1137 (map at Transform.scala:20)
   [druid] 2019-06-11 09:35:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:35:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 325 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 975 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 974, ShuffleMapStage 973)
   [druid] 2019-06-11 09:35:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 974)
   [druid] 2019-06-11 09:35:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 974 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:35:05,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_487 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:05,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_487_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:35:05,009 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_487_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:05,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 487 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 974 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:35:05,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 974.0 with 2 tasks
   [druid] 2019-06-11 09:35:05,009 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 974.0 (TID 649, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:05,009 [er for task 649] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 974.0 (TID 649)
   [druid] 2019-06-11 09:35:05,019 [er for task 649] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 974.0 (TID 649). 853 bytes result sent to driver
   [druid] 2019-06-11 09:35:05,019 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 974.0 (TID 650, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:05,019 [er for task 650] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 974.0 (TID 650)
   [druid] 2019-06-11 09:35:05,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 974.0 (TID 649) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:35:05,027 [er for task 650] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 974.0 (TID 650). 767 bytes result sent to driver
   [druid] 2019-06-11 09:35:05,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 974.0 (TID 650) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:35:05,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 974.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 974 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:35:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:35:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:35:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 975)
   [druid] 2019-06-11 09:35:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:35:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 975 (MapPartitionsRDD[1142] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:05,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_488 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:05,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_488_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:05,029 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_488_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:05,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 488 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 975 (MapPartitionsRDD[1142] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:35:05,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 975.0 with 1 tasks
   [druid] 2019-06-11 09:35:05,030 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 975.0 (TID 651, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:05,030 [er for task 651] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 975.0 (TID 651)
   [druid] 2019-06-11 09:35:05,031 [er for task 651] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:05,031 [er for task 651] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:05,031 [er for task 651] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:05,031 [er for task 651] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:05,031 [er for task 651] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 975.0 (TID 651). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:35:05,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 975.0 (TID 651) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:05,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 975.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:05,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 975 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:05,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 325 finished: print at Transform.scala:36, took 0.024720 s
   [druid] 2019-06-11 09:35:05,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:05,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 324 is 83 bytes
   [druid] 2019-06-11 09:35:05,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 325 is 160 bytes
   [druid] 2019-06-11 09:35:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 326 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 978 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 976, ShuffleMapStage 977)
   [druid] 2019-06-11 09:35:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:35:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 978 (MapPartitionsRDD[1142] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:05,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_489 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:05,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_489_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:05,036 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_489_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:05,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 489 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 978 (MapPartitionsRDD[1142] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:35:05,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 978.0 with 1 tasks
   [druid] 2019-06-11 09:35:05,037 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 978.0 (TID 652, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:05,037 [er for task 652] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 978.0 (TID 652)
   [druid] 2019-06-11 09:35:05,038 [er for task 652] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:05,038 [er for task 652] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:05,038 [er for task 652] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:05,039 [er for task 652] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:35:05,040 [er for task 652] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 978.0 (TID 652). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:35:05,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 978.0 (TID 652) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:05,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 978.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:05,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 978 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:35:05,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 326 finished: print at Transform.scala:36, took 0.005860 s
   [druid] 2019-06-11 09:35:05,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216905000 ms.0 from job set of time 1560216905000 ms
   [druid] 2019-06-11 09:35:05,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560216905000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:35:05,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1135 from persistence list
   [druid] 2019-06-11 09:35:05,040 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1135
   [druid] 2019-06-11 09:35:05,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1130 from persistence list
   [druid] 2019-06-11 09:35:05,041 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1130
   [druid] 2019-06-11 09:35:05,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1129 from persistence list
   [druid] 2019-06-11 09:35:05,041 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1129
   [druid] 2019-06-11 09:35:05,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1129] at socketTextStream at Transform.scala:18 of time 1560216905000 ms
   [druid] 2019-06-11 09:35:05,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216895000 ms
   [druid] 2019-06-11 09:35:05,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216895000 ms
   [druid] 2019-06-11 09:35:10,010 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216910000 ms
   [druid] 2019-06-11 09:35:10,010 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216910000 ms.0 from job set of time 1560216910000 ms
   [druid] 2019-06-11 09:35:10,012 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:35:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1144 (map at Transform.scala:20)
   [druid] 2019-06-11 09:35:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 327 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 981 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 979, ShuffleMapStage 980)
   [druid] 2019-06-11 09:35:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 979)
   [druid] 2019-06-11 09:35:10,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 979 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:35:10,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_490 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:10,018 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_490_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:35:10,020 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_490_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,021 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_477_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,021 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 490 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:10,024 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 979 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:35:10,024 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_488_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,024 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 979.0 with 2 tasks
   [druid] 2019-06-11 09:35:10,025 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 979.0 (TID 653, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:10,025 [er for task 653] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 979.0 (TID 653)
   [druid] 2019-06-11 09:35:10,025 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_484_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,026 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 317
   [druid] 2019-06-11 09:35:10,026 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 318
   [druid] 2019-06-11 09:35:10,027 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_487_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,027 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_479_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,028 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 314
   [druid] 2019-06-11 09:35:10,028 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_480_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,028 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 322
   [druid] 2019-06-11 09:35:10,029 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_478_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,029 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 319
   [druid] 2019-06-11 09:35:10,029 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 316
   [druid] 2019-06-11 09:35:10,029 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 320
   [druid] 2019-06-11 09:35:10,030 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_485_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,030 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_486_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,031 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 321
   [druid] 2019-06-11 09:35:10,031 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 323
   [druid] 2019-06-11 09:35:10,031 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_481_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,032 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 315
   [druid] 2019-06-11 09:35:10,032 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_475_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,036 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_489_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,037 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_483_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,037 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_482_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,038 [er for task 653] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 979.0 (TID 653). 810 bytes result sent to driver
   [druid] 2019-06-11 09:35:10,038 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_476_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,038 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 979.0 (TID 654, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:10,038 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 979.0 (TID 653) in 13 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:35:10,038 [er for task 654] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 979.0 (TID 654)
   [druid] 2019-06-11 09:35:10,046 [er for task 654] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 979.0 (TID 654). 810 bytes result sent to driver
   [druid] 2019-06-11 09:35:10,046 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 979.0 (TID 654) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:35:10,046 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 979.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 979 (parallelize at Transform.scala:16) finished in 0.021 s
   [druid] 2019-06-11 09:35:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:35:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:35:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 981)
   [druid] 2019-06-11 09:35:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:35:10,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 981 (MapPartitionsRDD[1149] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:10,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_491 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:10,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_491_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:10,049 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_491_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,049 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 491 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:10,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 981 (MapPartitionsRDD[1149] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:35:10,049 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 981.0 with 1 tasks
   [druid] 2019-06-11 09:35:10,050 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 981.0 (TID 655, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:10,050 [er for task 655] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 981.0 (TID 655)
   [druid] 2019-06-11 09:35:10,050 [er for task 655] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:10,050 [er for task 655] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:10,051 [er for task 655] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:10,051 [er for task 655] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:35:10,051 [er for task 655] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 981.0 (TID 655). 966 bytes result sent to driver
   [druid] 2019-06-11 09:35:10,051 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 981.0 (TID 655) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:10,051 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 981.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:10,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 981 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:10,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 327 finished: print at Transform.scala:36, took 0.039485 s
   [druid] 2019-06-11 09:35:10,054 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:10,054 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 327 is 160 bytes
   [druid] 2019-06-11 09:35:10,054 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 326 is 83 bytes
   [druid] 2019-06-11 09:35:10,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 328 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:10,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 984 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:10,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 982, ShuffleMapStage 983)
   [druid] 2019-06-11 09:35:10,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:35:10,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 984 (MapPartitionsRDD[1149] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:10,054 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_492 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:10,055 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_492_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:10,055 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_492_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:10,056 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 492 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:10,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 984 (MapPartitionsRDD[1149] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:35:10,056 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 984.0 with 1 tasks
   [druid] 2019-06-11 09:35:10,056 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 984.0 (TID 656, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:10,056 [er for task 656] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 984.0 (TID 656)
   [druid] 2019-06-11 09:35:10,057 [er for task 656] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:10,057 [er for task 656] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:10,057 [er for task 656] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:10,057 [er for task 656] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:10,057 [er for task 656] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 984.0 (TID 656). 966 bytes result sent to driver
   [druid] 2019-06-11 09:35:10,058 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 984.0 (TID 656) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:10,058 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 984.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:10,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 984 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:10,058 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 328 finished: print at Transform.scala:36, took 0.004189 s
   [druid] 2019-06-11 09:35:10,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216910000 ms.0 from job set of time 1560216910000 ms
   [druid] 2019-06-11 09:35:10,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.058 s for time 1560216910000 ms (execution: 0.048 s)
   [druid] 2019-06-11 09:35:10,058 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1142 from persistence list
   [druid] 2019-06-11 09:35:10,058 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1142
   [druid] 2019-06-11 09:35:10,058 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1137 from persistence list
   [druid] 2019-06-11 09:35:10,059 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1137
   [druid] 2019-06-11 09:35:10,059 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1136 from persistence list
   [druid] 2019-06-11 09:35:10,059 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1136
   [druid] 2019-06-11 09:35:10,059 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1136] at socketTextStream at Transform.scala:18 of time 1560216910000 ms
   [druid] 2019-06-11 09:35:10,059 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216900000 ms
   [druid] 2019-06-11 09:35:10,059 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216900000 ms
   [druid] 2019-06-11 09:35:15,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216915000 ms
   [druid] 2019-06-11 09:35:15,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216915000 ms.0 from job set of time 1560216915000 ms
   [druid] 2019-06-11 09:35:15,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1151 (map at Transform.scala:20)
   [druid] 2019-06-11 09:35:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:35:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 329 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 987 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 985, ShuffleMapStage 986)
   [druid] 2019-06-11 09:35:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 986)
   [druid] 2019-06-11 09:35:15,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 986 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:35:15,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_493 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:15,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_493_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:35:15,009 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_493_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:15,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 493 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 986 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:35:15,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 986.0 with 2 tasks
   [druid] 2019-06-11 09:35:15,009 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 986.0 (TID 657, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:15,009 [er for task 657] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 986.0 (TID 657)
   [druid] 2019-06-11 09:35:15,018 [er for task 657] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 986.0 (TID 657). 810 bytes result sent to driver
   [druid] 2019-06-11 09:35:15,018 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 986.0 (TID 658, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:15,018 [er for task 658] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 986.0 (TID 658)
   [druid] 2019-06-11 09:35:15,018 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 986.0 (TID 657) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:35:15,026 [er for task 658] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 986.0 (TID 658). 810 bytes result sent to driver
   [druid] 2019-06-11 09:35:15,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 986.0 (TID 658) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:35:15,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 986.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:15,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 986 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:35:15,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:35:15,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:35:15,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 987)
   [druid] 2019-06-11 09:35:15,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:35:15,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 987 (MapPartitionsRDD[1156] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:15,027 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_494 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:15,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_494_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:15,029 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_494_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:15,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 494 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:15,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 987 (MapPartitionsRDD[1156] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:35:15,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 987.0 with 1 tasks
   [druid] 2019-06-11 09:35:15,029 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 987.0 (TID 659, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:15,029 [er for task 659] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 987.0 (TID 659)
   [druid] 2019-06-11 09:35:15,030 [er for task 659] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:15,030 [er for task 659] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:15,030 [er for task 659] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:15,030 [er for task 659] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:15,031 [er for task 659] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 987.0 (TID 659). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:35:15,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 987.0 (TID 659) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:15,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 987.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:15,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 987 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:15,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 329 finished: print at Transform.scala:36, took 0.024235 s
   [druid] 2019-06-11 09:35:15,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:15,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 328 is 83 bytes
   [druid] 2019-06-11 09:35:15,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 329 is 160 bytes
   [druid] 2019-06-11 09:35:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 330 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 990 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 988, ShuffleMapStage 989)
   [druid] 2019-06-11 09:35:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:35:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 990 (MapPartitionsRDD[1156] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:15,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_495 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:15,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_495_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:15,037 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_495_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:15,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 495 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 990 (MapPartitionsRDD[1156] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:35:15,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 990.0 with 1 tasks
   [druid] 2019-06-11 09:35:15,037 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 990.0 (TID 660, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:15,037 [er for task 660] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 990.0 (TID 660)
   [druid] 2019-06-11 09:35:15,038 [er for task 660] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:15,038 [er for task 660] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:15,038 [er for task 660] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:15,038 [er for task 660] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:15,039 [er for task 660] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 990.0 (TID 660). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:35:15,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 990.0 (TID 660) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:15,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 990.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:15,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 990 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:15,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 330 finished: print at Transform.scala:36, took 0.004801 s
   [druid] 2019-06-11 09:35:15,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216915000 ms.0 from job set of time 1560216915000 ms
   [druid] 2019-06-11 09:35:15,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.039 s for time 1560216915000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:35:15,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1149 from persistence list
   [druid] 2019-06-11 09:35:15,040 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1149
   [druid] 2019-06-11 09:35:15,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1144 from persistence list
   [druid] 2019-06-11 09:35:15,040 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1144
   [druid] 2019-06-11 09:35:15,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1143 from persistence list
   [druid] 2019-06-11 09:35:15,040 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1143
   [druid] 2019-06-11 09:35:15,040 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1143] at socketTextStream at Transform.scala:18 of time 1560216915000 ms
   [druid] 2019-06-11 09:35:15,040 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216905000 ms
   [druid] 2019-06-11 09:35:15,040 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216905000 ms
   [druid] 2019-06-11 09:35:20,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216920000 ms
   [druid] 2019-06-11 09:35:20,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216920000 ms.0 from job set of time 1560216920000 ms
   [druid] 2019-06-11 09:35:20,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:35:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1158 (map at Transform.scala:20)
   [druid] 2019-06-11 09:35:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 331 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 993 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 991, ShuffleMapStage 992)
   [druid] 2019-06-11 09:35:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 991)
   [druid] 2019-06-11 09:35:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 991 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:35:20,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_496 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:20,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_496_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:35:20,009 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_496_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:20,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 496 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 991 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:35:20,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 991.0 with 2 tasks
   [druid] 2019-06-11 09:35:20,010 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 991.0 (TID 661, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:20,010 [er for task 661] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 991.0 (TID 661)
   [druid] 2019-06-11 09:35:20,018 [er for task 661] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 991.0 (TID 661). 810 bytes result sent to driver
   [druid] 2019-06-11 09:35:20,019 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 991.0 (TID 662, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:20,019 [er for task 662] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 991.0 (TID 662)
   [druid] 2019-06-11 09:35:20,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 991.0 (TID 661) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:35:20,027 [er for task 662] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 991.0 (TID 662). 767 bytes result sent to driver
   [druid] 2019-06-11 09:35:20,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 991.0 (TID 662) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:35:20,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 991.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:20,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 991 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:35:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:35:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:35:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 993)
   [druid] 2019-06-11 09:35:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:35:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 993 (MapPartitionsRDD[1163] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:20,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_497 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:20,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_497_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:20,029 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_497_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:20,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 497 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 993 (MapPartitionsRDD[1163] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:35:20,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 993.0 with 1 tasks
   [druid] 2019-06-11 09:35:20,030 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 993.0 (TID 663, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:20,030 [er for task 663] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 993.0 (TID 663)
   [druid] 2019-06-11 09:35:20,030 [er for task 663] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:20,030 [er for task 663] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:20,031 [er for task 663] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:20,034 [er for task 663] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 3 ms
   [druid] 2019-06-11 09:35:20,035 [er for task 663] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 993.0 (TID 663). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:35:20,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 993.0 (TID 663) in 5 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:20,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 993.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 993 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:35:20,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 331 finished: print at Transform.scala:36, took 0.028520 s
   [druid] 2019-06-11 09:35:20,038 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:20,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 331 is 160 bytes
   [druid] 2019-06-11 09:35:20,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 330 is 83 bytes
   [druid] 2019-06-11 09:35:20,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 332 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:20,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 996 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:20,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 994, ShuffleMapStage 995)
   [druid] 2019-06-11 09:35:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:35:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 996 (MapPartitionsRDD[1163] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:20,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_498 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:20,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_498_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:20,040 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_498_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:20,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 498 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 996 (MapPartitionsRDD[1163] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:35:20,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 996.0 with 1 tasks
   [druid] 2019-06-11 09:35:20,040 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 996.0 (TID 664, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:20,041 [er for task 664] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 996.0 (TID 664)
   [druid] 2019-06-11 09:35:20,041 [er for task 664] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:20,041 [er for task 664] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:20,041 [er for task 664] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:20,041 [er for task 664] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:20,042 [er for task 664] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 996.0 (TID 664). 966 bytes result sent to driver
   [druid] 2019-06-11 09:35:20,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 996.0 (TID 664) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:20,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 996.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:20,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 996 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:20,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 332 finished: print at Transform.scala:36, took 0.005019 s
   [druid] 2019-06-11 09:35:20,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216920000 ms.0 from job set of time 1560216920000 ms
   [druid] 2019-06-11 09:35:20,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216920000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:35:20,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1156 from persistence list
   [druid] 2019-06-11 09:35:20,043 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1156
   [druid] 2019-06-11 09:35:20,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1151 from persistence list
   [druid] 2019-06-11 09:35:20,043 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1151
   [druid] 2019-06-11 09:35:20,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1150 from persistence list
   [druid] 2019-06-11 09:35:20,043 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1150
   [druid] 2019-06-11 09:35:20,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1150] at socketTextStream at Transform.scala:18 of time 1560216920000 ms
   [druid] 2019-06-11 09:35:20,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216910000 ms
   [druid] 2019-06-11 09:35:20,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216910000 ms
   [druid] 2019-06-11 09:35:25,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216925000 ms
   [druid] 2019-06-11 09:35:25,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216925000 ms.0 from job set of time 1560216925000 ms
   [druid] 2019-06-11 09:35:25,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:35:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1165 (map at Transform.scala:20)
   [druid] 2019-06-11 09:35:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 333 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 999 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 997, ShuffleMapStage 998)
   [druid] 2019-06-11 09:35:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 997)
   [druid] 2019-06-11 09:35:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 997 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:35:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_499 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_499_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:35:25,010 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_499_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:25,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 499 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 997 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:35:25,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 997.0 with 2 tasks
   [druid] 2019-06-11 09:35:25,010 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 997.0 (TID 665, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:25,011 [er for task 665] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 997.0 (TID 665)
   [druid] 2019-06-11 09:35:25,020 [er for task 665] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 997.0 (TID 665). 810 bytes result sent to driver
   [druid] 2019-06-11 09:35:25,020 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 997.0 (TID 666, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:25,020 [er for task 666] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 997.0 (TID 666)
   [druid] 2019-06-11 09:35:25,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 997.0 (TID 665) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:35:25,030 [er for task 666] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 997.0 (TID 666). 810 bytes result sent to driver
   [druid] 2019-06-11 09:35:25,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 997.0 (TID 666) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:35:25,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 997.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 997 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:35:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:35:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:35:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 999)
   [druid] 2019-06-11 09:35:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:35:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 999 (MapPartitionsRDD[1170] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:25,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_500 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:25,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_500_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:25,033 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_500_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:25,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 500 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:25,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 999 (MapPartitionsRDD[1170] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:35:25,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 999.0 with 1 tasks
   [druid] 2019-06-11 09:35:25,034 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 999.0 (TID 667, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:25,034 [er for task 667] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 999.0 (TID 667)
   [druid] 2019-06-11 09:35:25,035 [er for task 667] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:25,035 [er for task 667] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:25,035 [er for task 667] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:25,035 [er for task 667] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:25,036 [er for task 667] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 999.0 (TID 667). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:35:25,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 999.0 (TID 667) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:25,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 999.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:25,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 999 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:25,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 333 finished: print at Transform.scala:36, took 0.029049 s
   [druid] 2019-06-11 09:35:25,040 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:25,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 333 is 160 bytes
   [druid] 2019-06-11 09:35:25,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 332 is 83 bytes
   [druid] 2019-06-11 09:35:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 334 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1002 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1000, ShuffleMapStage 1001)
   [druid] 2019-06-11 09:35:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:35:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1002 (MapPartitionsRDD[1170] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:25,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_501 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:25,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_501_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:25,042 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_501_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:25,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 501 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:25,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1002 (MapPartitionsRDD[1170] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:35:25,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1002.0 with 1 tasks
   [druid] 2019-06-11 09:35:25,043 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1002.0 (TID 668, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:25,043 [er for task 668] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1002.0 (TID 668)
   [druid] 2019-06-11 09:35:25,044 [er for task 668] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:25,044 [er for task 668] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:25,044 [er for task 668] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:25,044 [er for task 668] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:25,045 [er for task 668] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1002.0 (TID 668). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:35:25,045 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1002.0 (TID 668) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:25,045 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1002.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:25,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1002 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:35:25,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 334 finished: print at Transform.scala:36, took 0.006045 s
   [druid] 2019-06-11 09:35:25,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216925000 ms.0 from job set of time 1560216925000 ms
   [druid] 2019-06-11 09:35:25,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.046 s for time 1560216925000 ms (execution: 0.041 s)
   [druid] 2019-06-11 09:35:25,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1163 from persistence list
   [druid] 2019-06-11 09:35:25,046 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1163
   [druid] 2019-06-11 09:35:25,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1158 from persistence list
   [druid] 2019-06-11 09:35:25,047 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1158
   [druid] 2019-06-11 09:35:25,047 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1157 from persistence list
   [druid] 2019-06-11 09:35:25,047 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1157
   [druid] 2019-06-11 09:35:25,047 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1157] at socketTextStream at Transform.scala:18 of time 1560216925000 ms
   [druid] 2019-06-11 09:35:25,047 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216915000 ms
   [druid] 2019-06-11 09:35:25,047 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216915000 ms
   [druid] 2019-06-11 09:35:30,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216930000 ms
   [druid] 2019-06-11 09:35:30,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216930000 ms.0 from job set of time 1560216930000 ms
   [druid] 2019-06-11 09:35:30,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:35:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1172 (map at Transform.scala:20)
   [druid] 2019-06-11 09:35:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 335 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1005 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1003, ShuffleMapStage 1004)
   [druid] 2019-06-11 09:35:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1003)
   [druid] 2019-06-11 09:35:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1003 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:35:30,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_502 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:30,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_502_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:35:30,011 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_502_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:30,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 502 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1003 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:35:30,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1003.0 with 2 tasks
   [druid] 2019-06-11 09:35:30,011 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1003.0 (TID 669, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:30,011 [er for task 669] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1003.0 (TID 669)
   [druid] 2019-06-11 09:35:30,021 [er for task 669] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1003.0 (TID 669). 853 bytes result sent to driver
   [druid] 2019-06-11 09:35:30,021 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1003.0 (TID 670, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:30,021 [er for task 670] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1003.0 (TID 670)
   [druid] 2019-06-11 09:35:30,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1003.0 (TID 669) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:35:30,029 [er for task 670] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1003.0 (TID 670). 767 bytes result sent to driver
   [druid] 2019-06-11 09:35:30,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1003.0 (TID 670) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:35:30,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1003.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1003 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:35:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:35:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:35:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1005)
   [druid] 2019-06-11 09:35:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:35:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1005 (MapPartitionsRDD[1177] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:30,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_503 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:30,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_503_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:30,031 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_503_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:30,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 503 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1005 (MapPartitionsRDD[1177] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:35:30,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1005.0 with 1 tasks
   [druid] 2019-06-11 09:35:30,032 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1005.0 (TID 671, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:30,032 [er for task 671] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1005.0 (TID 671)
   [druid] 2019-06-11 09:35:30,033 [er for task 671] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:30,033 [er for task 671] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:30,033 [er for task 671] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:30,033 [er for task 671] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:30,034 [er for task 671] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1005.0 (TID 671). 966 bytes result sent to driver
   [druid] 2019-06-11 09:35:30,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1005.0 (TID 671) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:30,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1005.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:30,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1005 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:35:30,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 335 finished: print at Transform.scala:36, took 0.025833 s
   [druid] 2019-06-11 09:35:30,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:30,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 335 is 160 bytes
   [druid] 2019-06-11 09:35:30,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 334 is 83 bytes
   [druid] 2019-06-11 09:35:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 336 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1008 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1007, ShuffleMapStage 1006)
   [druid] 2019-06-11 09:35:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:35:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1008 (MapPartitionsRDD[1177] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:30,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_504 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:30,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_504_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:30,038 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_504_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:30,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 504 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:30,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1008 (MapPartitionsRDD[1177] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:35:30,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1008.0 with 1 tasks
   [druid] 2019-06-11 09:35:30,039 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1008.0 (TID 672, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:30,039 [er for task 672] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1008.0 (TID 672)
   [druid] 2019-06-11 09:35:30,040 [er for task 672] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:30,040 [er for task 672] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:30,040 [er for task 672] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:30,040 [er for task 672] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:30,040 [er for task 672] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1008.0 (TID 672). 966 bytes result sent to driver
   [druid] 2019-06-11 09:35:30,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1008.0 (TID 672) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:30,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1008.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:30,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1008 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:30,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 336 finished: print at Transform.scala:36, took 0.004750 s
   [druid] 2019-06-11 09:35:30,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216930000 ms.0 from job set of time 1560216930000 ms
   [druid] 2019-06-11 09:35:30,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560216930000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:35:30,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1170 from persistence list
   [druid] 2019-06-11 09:35:30,041 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1170
   [druid] 2019-06-11 09:35:30,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1165 from persistence list
   [druid] 2019-06-11 09:35:30,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1164 from persistence list
   [druid] 2019-06-11 09:35:30,042 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1165
   [druid] 2019-06-11 09:35:30,042 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1164
   [druid] 2019-06-11 09:35:30,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1164] at socketTextStream at Transform.scala:18 of time 1560216930000 ms
   [druid] 2019-06-11 09:35:30,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216920000 ms
   [druid] 2019-06-11 09:35:30,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216920000 ms
   [druid] 2019-06-11 09:35:35,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216935000 ms
   [druid] 2019-06-11 09:35:35,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216935000 ms.0 from job set of time 1560216935000 ms
   [druid] 2019-06-11 09:35:35,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:35:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1179 (map at Transform.scala:20)
   [druid] 2019-06-11 09:35:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 337 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1011 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1009, ShuffleMapStage 1010)
   [druid] 2019-06-11 09:35:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1009)
   [druid] 2019-06-11 09:35:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1009 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:35:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_505 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:35,017 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 330
   [druid] 2019-06-11 09:35:35,017 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_505_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:35:35,018 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 333
   [druid] 2019-06-11 09:35:35,019 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_505_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,022 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 329
   [druid] 2019-06-11 09:35:35,022 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 505 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:35,022 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1009 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:35:35,022 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1009.0 with 2 tasks
   [druid] 2019-06-11 09:35:35,023 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1009.0 (TID 673, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:35,023 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_496_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,023 [er for task 673] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1009.0 (TID 673)
   [druid] 2019-06-11 09:35:35,023 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_490_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 327
   [druid] 2019-06-11 09:35:35,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 326
   [druid] 2019-06-11 09:35:35,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 332
   [druid] 2019-06-11 09:35:35,025 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_504_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,026 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_502_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,027 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_503_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,028 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_491_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,028 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_494_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,029 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_500_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,030 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_501_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,031 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_497_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,032 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 331
   [druid] 2019-06-11 09:35:35,032 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_492_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,033 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_493_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,034 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 324
   [druid] 2019-06-11 09:35:35,034 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_499_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,035 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 328
   [druid] 2019-06-11 09:35:35,035 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_498_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,036 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 325
   [druid] 2019-06-11 09:35:35,036 [er for task 673] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1009.0 (TID 673). 853 bytes result sent to driver
   [druid] 2019-06-11 09:35:35,036 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1009.0 (TID 674, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:35,036 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1009.0 (TID 673) in 13 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:35:35,036 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_495_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,037 [er for task 674] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1009.0 (TID 674)
   [druid] 2019-06-11 09:35:35,045 [er for task 674] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1009.0 (TID 674). 767 bytes result sent to driver
   [druid] 2019-06-11 09:35:35,046 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1009.0 (TID 674) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:35:35,046 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1009.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1009 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:35:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:35:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:35:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1011)
   [druid] 2019-06-11 09:35:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:35:35,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1011 (MapPartitionsRDD[1184] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:35,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_506 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:35,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_506_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:35,048 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_506_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,048 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 506 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:35,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1011 (MapPartitionsRDD[1184] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:35:35,048 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1011.0 with 1 tasks
   [druid] 2019-06-11 09:35:35,049 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1011.0 (TID 675, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:35,049 [er for task 675] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1011.0 (TID 675)
   [druid] 2019-06-11 09:35:35,050 [er for task 675] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:35,050 [er for task 675] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:35,050 [er for task 675] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:35,050 [er for task 675] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:35,050 [er for task 675] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1011.0 (TID 675). 966 bytes result sent to driver
   [druid] 2019-06-11 09:35:35,051 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1011.0 (TID 675) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:35,051 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1011.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:35,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1011 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:35,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 337 finished: print at Transform.scala:36, took 0.043167 s
   [druid] 2019-06-11 09:35:35,053 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:35,053 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 337 is 160 bytes
   [druid] 2019-06-11 09:35:35,053 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 336 is 83 bytes
   [druid] 2019-06-11 09:35:35,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 338 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:35,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1014 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:35,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1012, ShuffleMapStage 1013)
   [druid] 2019-06-11 09:35:35,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:35:35,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1014 (MapPartitionsRDD[1184] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:35,054 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_507 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:35,055 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_507_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:35,055 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_507_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:35,055 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 507 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:35,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1014 (MapPartitionsRDD[1184] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:35:35,056 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1014.0 with 1 tasks
   [druid] 2019-06-11 09:35:35,056 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1014.0 (TID 676, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:35,056 [er for task 676] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1014.0 (TID 676)
   [druid] 2019-06-11 09:35:35,057 [er for task 676] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:35,057 [er for task 676] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:35,057 [er for task 676] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:35,057 [er for task 676] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:35,057 [er for task 676] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1014.0 (TID 676). 966 bytes result sent to driver
   [druid] 2019-06-11 09:35:35,058 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1014.0 (TID 676) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:35,058 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1014.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:35,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1014 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:35,058 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 338 finished: print at Transform.scala:36, took 0.004849 s
   [druid] 2019-06-11 09:35:35,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216935000 ms.0 from job set of time 1560216935000 ms
   [druid] 2019-06-11 09:35:35,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.058 s for time 1560216935000 ms (execution: 0.052 s)
   [druid] 2019-06-11 09:35:35,058 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1177 from persistence list
   [druid] 2019-06-11 09:35:35,058 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1177
   [druid] 2019-06-11 09:35:35,058 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1172 from persistence list
   [druid] 2019-06-11 09:35:35,059 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1172
   [druid] 2019-06-11 09:35:35,059 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1171 from persistence list
   [druid] 2019-06-11 09:35:35,059 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1171
   [druid] 2019-06-11 09:35:35,059 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1171] at socketTextStream at Transform.scala:18 of time 1560216935000 ms
   [druid] 2019-06-11 09:35:35,059 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216925000 ms
   [druid] 2019-06-11 09:35:35,059 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216925000 ms
   [druid] 2019-06-11 09:35:40,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216940000 ms
   [druid] 2019-06-11 09:35:40,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216940000 ms.0 from job set of time 1560216940000 ms
   [druid] 2019-06-11 09:35:40,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:35:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1186 (map at Transform.scala:20)
   [druid] 2019-06-11 09:35:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 339 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1017 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1015, ShuffleMapStage 1016)
   [druid] 2019-06-11 09:35:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1015)
   [druid] 2019-06-11 09:35:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1015 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:35:40,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_508 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:40,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_508_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:35:40,010 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_508_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:40,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 508 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:40,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1015 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:35:40,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1015.0 with 2 tasks
   [druid] 2019-06-11 09:35:40,011 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1015.0 (TID 677, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:40,011 [er for task 677] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1015.0 (TID 677)
   [druid] 2019-06-11 09:35:40,020 [er for task 677] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1015.0 (TID 677). 853 bytes result sent to driver
   [druid] 2019-06-11 09:35:40,020 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1015.0 (TID 678, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:40,020 [er for task 678] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1015.0 (TID 678)
   [druid] 2019-06-11 09:35:40,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1015.0 (TID 677) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:35:40,029 [er for task 678] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1015.0 (TID 678). 810 bytes result sent to driver
   [druid] 2019-06-11 09:35:40,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1015.0 (TID 678) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:35:40,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1015.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1015 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:35:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:35:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:35:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1017)
   [druid] 2019-06-11 09:35:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:35:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1017 (MapPartitionsRDD[1191] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:40,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_509 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:40,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_509_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:40,036 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_509_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:40,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 509 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1017 (MapPartitionsRDD[1191] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:35:40,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1017.0 with 1 tasks
   [druid] 2019-06-11 09:35:40,037 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1017.0 (TID 679, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:40,037 [er for task 679] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1017.0 (TID 679)
   [druid] 2019-06-11 09:35:40,037 [er for task 679] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:40,037 [er for task 679] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:40,038 [er for task 679] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:40,038 [er for task 679] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:40,038 [er for task 679] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1017.0 (TID 679). 966 bytes result sent to driver
   [druid] 2019-06-11 09:35:40,039 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1017.0 (TID 679) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:40,039 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1017.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:40,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1017 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:40,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 339 finished: print at Transform.scala:36, took 0.030663 s
   [druid] 2019-06-11 09:35:40,041 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:40,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 339 is 160 bytes
   [druid] 2019-06-11 09:35:40,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 338 is 83 bytes
   [druid] 2019-06-11 09:35:40,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 340 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1020 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1018, ShuffleMapStage 1019)
   [druid] 2019-06-11 09:35:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:35:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1020 (MapPartitionsRDD[1191] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:40,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_510 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:40,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_510_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:40,043 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_510_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:40,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 510 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:40,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1020 (MapPartitionsRDD[1191] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:35:40,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1020.0 with 1 tasks
   [druid] 2019-06-11 09:35:40,044 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1020.0 (TID 680, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:40,044 [er for task 680] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1020.0 (TID 680)
   [druid] 2019-06-11 09:35:40,045 [er for task 680] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:40,045 [er for task 680] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:40,045 [er for task 680] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:40,045 [er for task 680] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:40,045 [er for task 680] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1020.0 (TID 680). 966 bytes result sent to driver
   [druid] 2019-06-11 09:35:40,046 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1020.0 (TID 680) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:40,046 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1020.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:40,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1020 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:40,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 340 finished: print at Transform.scala:36, took 0.004985 s
   [druid] 2019-06-11 09:35:40,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216940000 ms.0 from job set of time 1560216940000 ms
   [druid] 2019-06-11 09:35:40,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.046 s for time 1560216940000 ms (execution: 0.040 s)
   [druid] 2019-06-11 09:35:40,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1184 from persistence list
   [druid] 2019-06-11 09:35:40,047 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1184
   [druid] 2019-06-11 09:35:40,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1179 from persistence list
   [druid] 2019-06-11 09:35:40,047 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1179
   [druid] 2019-06-11 09:35:40,047 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1178 from persistence list
   [druid] 2019-06-11 09:35:40,047 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1178
   [druid] 2019-06-11 09:35:40,047 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1178] at socketTextStream at Transform.scala:18 of time 1560216940000 ms
   [druid] 2019-06-11 09:35:40,047 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216930000 ms
   [druid] 2019-06-11 09:35:40,047 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216930000 ms
   [druid] 2019-06-11 09:35:45,017 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216945000 ms
   [druid] 2019-06-11 09:35:45,024 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216945000 ms.0 from job set of time 1560216945000 ms
   [druid] 2019-06-11 09:35:45,027 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1193 (map at Transform.scala:20)
   [druid] 2019-06-11 09:35:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:35:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 341 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1023 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1021, ShuffleMapStage 1022)
   [druid] 2019-06-11 09:35:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1022)
   [druid] 2019-06-11 09:35:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1022 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:35:45,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_511 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:45,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_511_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:35:45,043 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_511_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:45,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 511 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:45,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1022 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:35:45,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1022.0 with 2 tasks
   [druid] 2019-06-11 09:35:45,045 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1022.0 (TID 681, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:45,045 [er for task 681] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1022.0 (TID 681)
   [druid] 2019-06-11 09:35:45,081 [er for task 681] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1022.0 (TID 681). 896 bytes result sent to driver
   [druid] 2019-06-11 09:35:45,081 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1022.0 (TID 682, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:45,081 [er for task 682] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1022.0 (TID 682)
   [druid] 2019-06-11 09:35:45,082 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1022.0 (TID 681) in 37 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:35:45,118 [er for task 682] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1022.0 (TID 682). 810 bytes result sent to driver
   [druid] 2019-06-11 09:35:45,120 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1022.0 (TID 682) in 39 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:35:45,120 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1022.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:45,126 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1022 (parallelize at Transform.scala:16) finished in 0.080 s
   [druid] 2019-06-11 09:35:45,126 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:35:45,126 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:35:45,126 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1023)
   [druid] 2019-06-11 09:35:45,126 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:35:45,126 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1023 (MapPartitionsRDD[1198] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:45,138 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_512 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:45,140 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_512_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:45,141 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_512_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:45,142 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 512 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:45,144 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1023 (MapPartitionsRDD[1198] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:35:45,144 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1023.0 with 1 tasks
   [druid] 2019-06-11 09:35:45,145 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1023.0 (TID 683, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:45,145 [er for task 683] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1023.0 (TID 683)
   [druid] 2019-06-11 09:35:45,146 [er for task 683] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:45,146 [er for task 683] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:45,146 [er for task 683] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:45,146 [er for task 683] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:45,149 [er for task 683] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1023.0 (TID 683). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:35:45,149 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1023.0 (TID 683) in 5 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:45,150 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1023.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:45,150 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1023 (print at Transform.scala:36) finished in 0.006 s
   [druid] 2019-06-11 09:35:45,150 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 341 finished: print at Transform.scala:36, took 0.122539 s
   [druid] 2019-06-11 09:35:45,156 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:45,157 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 340 is 83 bytes
   [druid] 2019-06-11 09:35:45,157 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 341 is 160 bytes
   [druid] 2019-06-11 09:35:45,157 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 342 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:45,157 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1026 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:45,158 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1025, ShuffleMapStage 1024)
   [druid] 2019-06-11 09:35:45,158 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:35:45,158 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1026 (MapPartitionsRDD[1198] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:45,159 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_513 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:45,160 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_513_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:35:45,161 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_513_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:45,161 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 513 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:45,162 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1026 (MapPartitionsRDD[1198] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:35:45,162 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1026.0 with 1 tasks
   [druid] 2019-06-11 09:35:45,162 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1026.0 (TID 684, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:45,162 [er for task 684] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1026.0 (TID 684)
   [druid] 2019-06-11 09:35:45,164 [er for task 684] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:45,164 [er for task 684] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:45,164 [er for task 684] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:45,164 [er for task 684] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:45,165 [er for task 684] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1026.0 (TID 684). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:35:45,166 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1026.0 (TID 684) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:45,166 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1026.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:45,166 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1026 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:35:45,166 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 342 finished: print at Transform.scala:36, took 0.009927 s
   [druid] 2019-06-11 09:35:45,166 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216945000 ms.0 from job set of time 1560216945000 ms
   [druid] 2019-06-11 09:35:45,166 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.166 s for time 1560216945000 ms (execution: 0.142 s)
   [druid] 2019-06-11 09:35:45,166 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1191 from persistence list
   [druid] 2019-06-11 09:35:45,168 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1186 from persistence list
   [druid] 2019-06-11 09:35:45,168 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1191
   [druid] 2019-06-11 09:35:45,170 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1186
   [druid] 2019-06-11 09:35:45,171 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1185 from persistence list
   [druid] 2019-06-11 09:35:45,172 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1185] at socketTextStream at Transform.scala:18 of time 1560216945000 ms
   [druid] 2019-06-11 09:35:45,172 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1185
   [druid] 2019-06-11 09:35:45,172 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216935000 ms
   [druid] 2019-06-11 09:35:45,172 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216935000 ms
   [druid] 2019-06-11 09:35:50,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216950000 ms
   [druid] 2019-06-11 09:35:50,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216950000 ms.0 from job set of time 1560216950000 ms
   [druid] 2019-06-11 09:35:50,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1200 (map at Transform.scala:20)
   [druid] 2019-06-11 09:35:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:35:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 343 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1029 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1027, ShuffleMapStage 1028)
   [druid] 2019-06-11 09:35:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1028)
   [druid] 2019-06-11 09:35:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1028 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:35:50,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_514 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:50,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_514_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:35:50,009 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_514_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:50,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 514 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:50,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1028 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:35:50,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1028.0 with 2 tasks
   [druid] 2019-06-11 09:35:50,010 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1028.0 (TID 685, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:50,010 [er for task 685] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1028.0 (TID 685)
   [druid] 2019-06-11 09:35:50,020 [er for task 685] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1028.0 (TID 685). 853 bytes result sent to driver
   [druid] 2019-06-11 09:35:50,020 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1028.0 (TID 686, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:50,020 [er for task 686] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1028.0 (TID 686)
   [druid] 2019-06-11 09:35:50,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1028.0 (TID 685) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:35:50,028 [er for task 686] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1028.0 (TID 686). 810 bytes result sent to driver
   [druid] 2019-06-11 09:35:50,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1028.0 (TID 686) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:35:50,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1028.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1028 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:35:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:35:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:35:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1029)
   [druid] 2019-06-11 09:35:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:35:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1029 (MapPartitionsRDD[1205] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:50,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_515 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:50,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_515_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:50,030 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_515_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:50,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 515 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:50,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1029 (MapPartitionsRDD[1205] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:35:50,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1029.0 with 1 tasks
   [druid] 2019-06-11 09:35:50,030 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1029.0 (TID 687, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:50,030 [er for task 687] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1029.0 (TID 687)
   [druid] 2019-06-11 09:35:50,031 [er for task 687] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:50,031 [er for task 687] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:50,031 [er for task 687] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:50,031 [er for task 687] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:50,032 [er for task 687] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1029.0 (TID 687). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:35:50,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1029.0 (TID 687) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:50,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1029.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:50,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1029 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:50,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 343 finished: print at Transform.scala:36, took 0.024301 s
   [druid] 2019-06-11 09:35:50,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:50,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 342 is 83 bytes
   [druid] 2019-06-11 09:35:50,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 343 is 160 bytes
   [druid] 2019-06-11 09:35:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 344 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1032 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1030, ShuffleMapStage 1031)
   [druid] 2019-06-11 09:35:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:35:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1032 (MapPartitionsRDD[1205] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:50,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_516 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:50,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_516_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:50,036 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_516_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:50,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 516 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1032 (MapPartitionsRDD[1205] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:35:50,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1032.0 with 1 tasks
   [druid] 2019-06-11 09:35:50,037 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1032.0 (TID 688, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:50,037 [er for task 688] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1032.0 (TID 688)
   [druid] 2019-06-11 09:35:50,038 [er for task 688] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:50,038 [er for task 688] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:50,038 [er for task 688] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:50,038 [er for task 688] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:50,039 [er for task 688] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1032.0 (TID 688). 966 bytes result sent to driver
   [druid] 2019-06-11 09:35:50,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1032.0 (TID 688) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:50,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1032.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:50,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1032 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:35:50,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 344 finished: print at Transform.scala:36, took 0.005954 s
   [druid] 2019-06-11 09:35:50,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216950000 ms.0 from job set of time 1560216950000 ms
   [druid] 2019-06-11 09:35:50,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560216950000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:35:50,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1198 from persistence list
   [druid] 2019-06-11 09:35:50,041 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1198
   [druid] 2019-06-11 09:35:50,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1193 from persistence list
   [druid] 2019-06-11 09:35:50,041 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1193
   [druid] 2019-06-11 09:35:50,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1192 from persistence list
   [druid] 2019-06-11 09:35:50,041 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1192
   [druid] 2019-06-11 09:35:50,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1192] at socketTextStream at Transform.scala:18 of time 1560216950000 ms
   [druid] 2019-06-11 09:35:50,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216940000 ms
   [druid] 2019-06-11 09:35:50,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216940000 ms
   [druid] 2019-06-11 09:35:55,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216955000 ms
   [druid] 2019-06-11 09:35:55,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216955000 ms.0 from job set of time 1560216955000 ms
   [druid] 2019-06-11 09:35:55,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1207 (map at Transform.scala:20)
   [druid] 2019-06-11 09:35:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:35:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 345 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1035 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1033, ShuffleMapStage 1034)
   [druid] 2019-06-11 09:35:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1034)
   [druid] 2019-06-11 09:35:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1034 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:35:55,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_517 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:55,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_517_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:35:55,012 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_517_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:55,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 517 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:55,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1034 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:35:55,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1034.0 with 2 tasks
   [druid] 2019-06-11 09:35:55,012 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1034.0 (TID 689, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:55,012 [er for task 689] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1034.0 (TID 689)
   [druid] 2019-06-11 09:35:55,022 [er for task 689] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1034.0 (TID 689). 810 bytes result sent to driver
   [druid] 2019-06-11 09:35:55,023 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1034.0 (TID 690, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:35:55,023 [er for task 690] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1034.0 (TID 690)
   [druid] 2019-06-11 09:35:55,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1034.0 (TID 689) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:35:55,032 [er for task 690] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1034.0 (TID 690). 853 bytes result sent to driver
   [druid] 2019-06-11 09:35:55,032 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1034.0 (TID 690) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:35:55,032 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1034.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:55,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1034 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:35:55,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:35:55,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:35:55,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1035)
   [druid] 2019-06-11 09:35:55,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:35:55,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1035 (MapPartitionsRDD[1212] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:55,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_518 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:55,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_518_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:55,035 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_518_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:55,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 518 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1035 (MapPartitionsRDD[1212] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:35:55,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1035.0 with 1 tasks
   [druid] 2019-06-11 09:35:55,036 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1035.0 (TID 691, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:55,036 [er for task 691] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1035.0 (TID 691)
   [druid] 2019-06-11 09:35:55,036 [er for task 691] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:55,036 [er for task 691] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:55,036 [er for task 691] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:55,036 [er for task 691] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:55,037 [er for task 691] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1035.0 (TID 691). 966 bytes result sent to driver
   [druid] 2019-06-11 09:35:55,038 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1035.0 (TID 691) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:55,038 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1035.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:55,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1035 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:35:55,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 345 finished: print at Transform.scala:36, took 0.028632 s
   [druid] 2019-06-11 09:35:55,040 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:35:55,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 344 is 83 bytes
   [druid] 2019-06-11 09:35:55,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 345 is 160 bytes
   [druid] 2019-06-11 09:35:55,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 346 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:35:55,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1038 (print at Transform.scala:36)
   [druid] 2019-06-11 09:35:55,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1036, ShuffleMapStage 1037)
   [druid] 2019-06-11 09:35:55,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:35:55,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1038 (MapPartitionsRDD[1212] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:35:55,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_519 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:55,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_519_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:35:55,043 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_519_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:35:55,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 519 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:35:55,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1038 (MapPartitionsRDD[1212] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:35:55,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1038.0 with 1 tasks
   [druid] 2019-06-11 09:35:55,043 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1038.0 (TID 692, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:35:55,043 [er for task 692] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1038.0 (TID 692)
   [druid] 2019-06-11 09:35:55,044 [er for task 692] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:35:55,044 [er for task 692] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:55,044 [er for task 692] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:35:55,044 [er for task 692] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:35:55,045 [er for task 692] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1038.0 (TID 692). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:35:55,045 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1038.0 (TID 692) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:35:55,045 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1038.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:35:55,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1038 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:35:55,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 346 finished: print at Transform.scala:36, took 0.005051 s
   [druid] 2019-06-11 09:35:55,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216955000 ms.0 from job set of time 1560216955000 ms
   [druid] 2019-06-11 09:35:55,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.046 s for time 1560216955000 ms (execution: 0.039 s)
   [druid] 2019-06-11 09:35:55,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1205 from persistence list
   [druid] 2019-06-11 09:35:55,046 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1205
   [druid] 2019-06-11 09:35:55,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1200 from persistence list
   [druid] 2019-06-11 09:35:55,046 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1200
   [druid] 2019-06-11 09:35:55,046 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1199 from persistence list
   [druid] 2019-06-11 09:35:55,046 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1199
   [druid] 2019-06-11 09:35:55,046 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1199] at socketTextStream at Transform.scala:18 of time 1560216955000 ms
   [druid] 2019-06-11 09:35:55,046 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216945000 ms
   [druid] 2019-06-11 09:35:55,047 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216945000 ms
   [druid] 2019-06-11 09:36:00,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216960000 ms
   [druid] 2019-06-11 09:36:00,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216960000 ms.0 from job set of time 1560216960000 ms
   [druid] 2019-06-11 09:36:00,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:36:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1214 (map at Transform.scala:20)
   [druid] 2019-06-11 09:36:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 347 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1041 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1039, ShuffleMapStage 1040)
   [druid] 2019-06-11 09:36:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1039)
   [druid] 2019-06-11 09:36:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1039 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:36:00,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_520 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:00,018 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_520_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:36:00,019 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_520_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,019 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_512_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,019 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 520 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:00,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1039 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:36:00,019 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1039.0 with 2 tasks
   [druid] 2019-06-11 09:36:00,020 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1039.0 (TID 693, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:00,020 [er for task 693] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1039.0 (TID 693)
   [druid] 2019-06-11 09:36:00,020 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_511_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,021 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 337
   [druid] 2019-06-11 09:36:00,021 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 341
   [druid] 2019-06-11 09:36:00,021 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_505_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,022 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_508_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,023 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_510_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,024 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_507_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 343
   [druid] 2019-06-11 09:36:00,025 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_516_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,025 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_506_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,026 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_514_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,027 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 339
   [druid] 2019-06-11 09:36:00,027 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 340
   [druid] 2019-06-11 09:36:00,028 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_518_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,029 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_509_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,029 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 338
   [druid] 2019-06-11 09:36:00,030 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 335
   [druid] 2019-06-11 09:36:00,030 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 334
   [druid] 2019-06-11 09:36:00,030 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 336
   [druid] 2019-06-11 09:36:00,030 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_515_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,031 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 342
   [druid] 2019-06-11 09:36:00,032 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_519_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,033 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_513_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,034 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_517_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,037 [er for task 693] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1039.0 (TID 693). 853 bytes result sent to driver
   [druid] 2019-06-11 09:36:00,038 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1039.0 (TID 694, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:00,038 [er for task 694] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1039.0 (TID 694)
   [druid] 2019-06-11 09:36:00,038 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1039.0 (TID 693) in 18 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:36:00,046 [er for task 694] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1039.0 (TID 694). 810 bytes result sent to driver
   [druid] 2019-06-11 09:36:00,047 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1039.0 (TID 694) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:36:00,047 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1039.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:00,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1039 (parallelize at Transform.scala:16) finished in 0.027 s
   [druid] 2019-06-11 09:36:00,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:36:00,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:36:00,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1041)
   [druid] 2019-06-11 09:36:00,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:36:00,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1041 (MapPartitionsRDD[1219] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:00,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_521 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:00,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_521_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:00,050 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_521_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,050 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 521 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:00,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1041 (MapPartitionsRDD[1219] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:36:00,051 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1041.0 with 1 tasks
   [druid] 2019-06-11 09:36:00,051 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1041.0 (TID 695, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:00,051 [er for task 695] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1041.0 (TID 695)
   [druid] 2019-06-11 09:36:00,052 [er for task 695] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:00,052 [er for task 695] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:00,052 [er for task 695] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:00,052 [er for task 695] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:00,053 [er for task 695] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1041.0 (TID 695). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:00,053 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1041.0 (TID 695) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:00,053 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1041.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:00,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1041 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:00,053 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 347 finished: print at Transform.scala:36, took 0.042692 s
   [druid] 2019-06-11 09:36:00,055 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:00,055 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 347 is 160 bytes
   [druid] 2019-06-11 09:36:00,055 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 346 is 83 bytes
   [druid] 2019-06-11 09:36:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 348 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1044 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1042, ShuffleMapStage 1043)
   [druid] 2019-06-11 09:36:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:36:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1044 (MapPartitionsRDD[1219] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:00,056 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_522 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:00,057 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_522_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:00,057 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_522_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:00,057 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 522 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:00,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1044 (MapPartitionsRDD[1219] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:36:00,058 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1044.0 with 1 tasks
   [druid] 2019-06-11 09:36:00,058 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1044.0 (TID 696, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:00,058 [er for task 696] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1044.0 (TID 696)
   [druid] 2019-06-11 09:36:00,059 [er for task 696] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:00,059 [er for task 696] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:00,059 [er for task 696] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:00,059 [er for task 696] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:00,059 [er for task 696] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1044.0 (TID 696). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:00,060 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1044.0 (TID 696) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:00,060 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1044.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:00,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1044 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:00,060 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 348 finished: print at Transform.scala:36, took 0.004767 s
   [druid] 2019-06-11 09:36:00,060 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216960000 ms.0 from job set of time 1560216960000 ms
   [druid] 2019-06-11 09:36:00,060 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.060 s for time 1560216960000 ms (execution: 0.051 s)
   [druid] 2019-06-11 09:36:00,060 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1212 from persistence list
   [druid] 2019-06-11 09:36:00,060 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1212
   [druid] 2019-06-11 09:36:00,061 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1207 from persistence list
   [druid] 2019-06-11 09:36:00,061 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1207
   [druid] 2019-06-11 09:36:00,061 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1206 from persistence list
   [druid] 2019-06-11 09:36:00,061 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1206
   [druid] 2019-06-11 09:36:00,061 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1206] at socketTextStream at Transform.scala:18 of time 1560216960000 ms
   [druid] 2019-06-11 09:36:00,061 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216950000 ms
   [druid] 2019-06-11 09:36:00,061 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216950000 ms
   [druid] 2019-06-11 09:36:05,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216965000 ms
   [druid] 2019-06-11 09:36:05,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216965000 ms.0 from job set of time 1560216965000 ms
   [druid] 2019-06-11 09:36:05,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1221 (map at Transform.scala:20)
   [druid] 2019-06-11 09:36:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:36:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 349 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1047 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1045, ShuffleMapStage 1046)
   [druid] 2019-06-11 09:36:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1046)
   [druid] 2019-06-11 09:36:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1046 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:36:05,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_523 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:05,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_523_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:36:05,011 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_523_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:05,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 523 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:05,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1046 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:36:05,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1046.0 with 2 tasks
   [druid] 2019-06-11 09:36:05,012 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1046.0 (TID 697, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:05,012 [er for task 697] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1046.0 (TID 697)
   [druid] 2019-06-11 09:36:05,028 [er for task 697] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1046.0 (TID 697). 853 bytes result sent to driver
   [druid] 2019-06-11 09:36:05,029 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1046.0 (TID 698, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:05,029 [er for task 698] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1046.0 (TID 698)
   [druid] 2019-06-11 09:36:05,029 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1046.0 (TID 697) in 17 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:36:05,038 [er for task 698] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1046.0 (TID 698). 810 bytes result sent to driver
   [druid] 2019-06-11 09:36:05,039 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1046.0 (TID 698) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:36:05,039 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1046.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1046 (parallelize at Transform.scala:16) finished in 0.027 s
   [druid] 2019-06-11 09:36:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:36:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:36:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1047)
   [druid] 2019-06-11 09:36:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:36:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1047 (MapPartitionsRDD[1226] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:05,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_524 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:05,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_524_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:05,040 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_524_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:05,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 524 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:05,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1047 (MapPartitionsRDD[1226] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:36:05,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1047.0 with 1 tasks
   [druid] 2019-06-11 09:36:05,041 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1047.0 (TID 699, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:05,041 [er for task 699] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1047.0 (TID 699)
   [druid] 2019-06-11 09:36:05,042 [er for task 699] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:05,042 [er for task 699] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:05,042 [er for task 699] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:05,042 [er for task 699] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:05,042 [er for task 699] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1047.0 (TID 699). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:05,043 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1047.0 (TID 699) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:05,043 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1047.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:05,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1047 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:05,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 349 finished: print at Transform.scala:36, took 0.034265 s
   [druid] 2019-06-11 09:36:05,045 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:05,045 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 348 is 83 bytes
   [druid] 2019-06-11 09:36:05,045 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 349 is 160 bytes
   [druid] 2019-06-11 09:36:05,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 350 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:05,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1050 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:05,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1048, ShuffleMapStage 1049)
   [druid] 2019-06-11 09:36:05,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:36:05,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1050 (MapPartitionsRDD[1226] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:05,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_525 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:05,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_525_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:05,047 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_525_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:05,047 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 525 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:05,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1050 (MapPartitionsRDD[1226] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:36:05,047 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1050.0 with 1 tasks
   [druid] 2019-06-11 09:36:05,047 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1050.0 (TID 700, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:05,047 [er for task 700] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1050.0 (TID 700)
   [druid] 2019-06-11 09:36:05,048 [er for task 700] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:05,049 [er for task 700] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:36:05,049 [er for task 700] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:05,049 [er for task 700] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:05,049 [er for task 700] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1050.0 (TID 700). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:36:05,050 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1050.0 (TID 700) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:05,050 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1050.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:05,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1050 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:36:05,050 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 350 finished: print at Transform.scala:36, took 0.005565 s
   [druid] 2019-06-11 09:36:05,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216965000 ms.0 from job set of time 1560216965000 ms
   [druid] 2019-06-11 09:36:05,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.050 s for time 1560216965000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:36:05,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1219 from persistence list
   [druid] 2019-06-11 09:36:05,050 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1219
   [druid] 2019-06-11 09:36:05,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1214 from persistence list
   [druid] 2019-06-11 09:36:05,051 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1214
   [druid] 2019-06-11 09:36:05,051 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1213 from persistence list
   [druid] 2019-06-11 09:36:05,051 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1213
   [druid] 2019-06-11 09:36:05,051 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1213] at socketTextStream at Transform.scala:18 of time 1560216965000 ms
   [druid] 2019-06-11 09:36:05,051 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216955000 ms
   [druid] 2019-06-11 09:36:05,051 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216955000 ms
   [druid] 2019-06-11 09:36:10,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216970000 ms
   [druid] 2019-06-11 09:36:10,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216970000 ms.0 from job set of time 1560216970000 ms
   [druid] 2019-06-11 09:36:10,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:36:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1228 (map at Transform.scala:20)
   [druid] 2019-06-11 09:36:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 351 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1053 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1051, ShuffleMapStage 1052)
   [druid] 2019-06-11 09:36:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1051)
   [druid] 2019-06-11 09:36:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1051 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:36:10,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_526 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:10,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_526_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:36:10,011 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_526_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:10,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 526 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:10,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1051 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:36:10,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1051.0 with 2 tasks
   [druid] 2019-06-11 09:36:10,012 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1051.0 (TID 701, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:10,012 [er for task 701] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1051.0 (TID 701)
   [druid] 2019-06-11 09:36:10,021 [er for task 701] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1051.0 (TID 701). 810 bytes result sent to driver
   [druid] 2019-06-11 09:36:10,021 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1051.0 (TID 702, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:10,021 [er for task 702] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1051.0 (TID 702)
   [druid] 2019-06-11 09:36:10,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1051.0 (TID 701) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:36:10,029 [er for task 702] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1051.0 (TID 702). 767 bytes result sent to driver
   [druid] 2019-06-11 09:36:10,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1051.0 (TID 702) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:36:10,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1051.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1051 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:36:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:36:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:36:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1053)
   [druid] 2019-06-11 09:36:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:36:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1053 (MapPartitionsRDD[1233] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:10,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_527 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:10,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_527_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:10,032 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_527_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:10,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 527 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1053 (MapPartitionsRDD[1233] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:36:10,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1053.0 with 1 tasks
   [druid] 2019-06-11 09:36:10,033 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1053.0 (TID 703, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:10,033 [er for task 703] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1053.0 (TID 703)
   [druid] 2019-06-11 09:36:10,033 [er for task 703] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:10,033 [er for task 703] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:10,034 [er for task 703] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:10,034 [er for task 703] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:10,035 [er for task 703] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1053.0 (TID 703). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:10,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1053.0 (TID 703) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:10,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1053.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:10,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1053 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:10,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 351 finished: print at Transform.scala:36, took 0.026662 s
   [druid] 2019-06-11 09:36:10,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:10,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 351 is 160 bytes
   [druid] 2019-06-11 09:36:10,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 350 is 83 bytes
   [druid] 2019-06-11 09:36:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 352 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1056 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1054, ShuffleMapStage 1055)
   [druid] 2019-06-11 09:36:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:36:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1056 (MapPartitionsRDD[1233] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:10,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_528 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:10,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_528_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:10,039 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_528_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:10,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 528 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1056 (MapPartitionsRDD[1233] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:36:10,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1056.0 with 1 tasks
   [druid] 2019-06-11 09:36:10,040 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1056.0 (TID 704, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:10,040 [er for task 704] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1056.0 (TID 704)
   [druid] 2019-06-11 09:36:10,041 [er for task 704] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:10,041 [er for task 704] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:10,041 [er for task 704] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:10,041 [er for task 704] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:10,042 [er for task 704] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1056.0 (TID 704). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:10,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1056.0 (TID 704) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:10,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1056.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1056 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:10,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 352 finished: print at Transform.scala:36, took 0.004904 s
   [druid] 2019-06-11 09:36:10,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216970000 ms.0 from job set of time 1560216970000 ms
   [druid] 2019-06-11 09:36:10,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560216970000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:36:10,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1226 from persistence list
   [druid] 2019-06-11 09:36:10,043 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1226
   [druid] 2019-06-11 09:36:10,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1221 from persistence list
   [druid] 2019-06-11 09:36:10,043 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1221
   [druid] 2019-06-11 09:36:10,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1220 from persistence list
   [druid] 2019-06-11 09:36:10,043 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1220
   [druid] 2019-06-11 09:36:10,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1220] at socketTextStream at Transform.scala:18 of time 1560216970000 ms
   [druid] 2019-06-11 09:36:10,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216960000 ms
   [druid] 2019-06-11 09:36:10,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216960000 ms
   [druid] 2019-06-11 09:36:15,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216975000 ms
   [druid] 2019-06-11 09:36:15,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216975000 ms.0 from job set of time 1560216975000 ms
   [druid] 2019-06-11 09:36:15,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1235 (map at Transform.scala:20)
   [druid] 2019-06-11 09:36:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:36:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 353 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1059 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1058, ShuffleMapStage 1057)
   [druid] 2019-06-11 09:36:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1058)
   [druid] 2019-06-11 09:36:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1058 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:36:15,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_529 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:15,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_529_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:36:15,010 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_529_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:15,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 529 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1058 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:36:15,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1058.0 with 2 tasks
   [druid] 2019-06-11 09:36:15,011 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1058.0 (TID 705, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:15,011 [er for task 705] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1058.0 (TID 705)
   [druid] 2019-06-11 09:36:15,021 [er for task 705] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1058.0 (TID 705). 810 bytes result sent to driver
   [druid] 2019-06-11 09:36:15,021 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1058.0 (TID 706, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:15,021 [er for task 706] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1058.0 (TID 706)
   [druid] 2019-06-11 09:36:15,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1058.0 (TID 705) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:36:15,029 [er for task 706] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1058.0 (TID 706). 810 bytes result sent to driver
   [druid] 2019-06-11 09:36:15,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1058.0 (TID 706) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:36:15,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1058.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1058 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:36:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:36:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:36:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1059)
   [druid] 2019-06-11 09:36:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:36:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1059 (MapPartitionsRDD[1240] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:15,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_530 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:15,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_530_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:15,031 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_530_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:15,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 530 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:15,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1059 (MapPartitionsRDD[1240] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:36:15,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1059.0 with 1 tasks
   [druid] 2019-06-11 09:36:15,032 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1059.0 (TID 707, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:15,032 [er for task 707] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1059.0 (TID 707)
   [druid] 2019-06-11 09:36:15,033 [er for task 707] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:15,033 [er for task 707] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:15,033 [er for task 707] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:15,033 [er for task 707] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:15,034 [er for task 707] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1059.0 (TID 707). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:15,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1059.0 (TID 707) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:15,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1059.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1059 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:15,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 353 finished: print at Transform.scala:36, took 0.026496 s
   [druid] 2019-06-11 09:36:15,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:15,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 352 is 83 bytes
   [druid] 2019-06-11 09:36:15,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 353 is 160 bytes
   [druid] 2019-06-11 09:36:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 354 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1062 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1060, ShuffleMapStage 1061)
   [druid] 2019-06-11 09:36:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:36:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1062 (MapPartitionsRDD[1240] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:15,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_531 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:15,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_531_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:15,040 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_531_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:15,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 531 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1062 (MapPartitionsRDD[1240] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:36:15,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1062.0 with 1 tasks
   [druid] 2019-06-11 09:36:15,041 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1062.0 (TID 708, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:15,041 [er for task 708] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1062.0 (TID 708)
   [druid] 2019-06-11 09:36:15,041 [er for task 708] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:15,041 [er for task 708] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:15,041 [er for task 708] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:15,041 [er for task 708] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:15,042 [er for task 708] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1062.0 (TID 708). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:15,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1062.0 (TID 708) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:15,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1062.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:15,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1062 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:36:15,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 354 finished: print at Transform.scala:36, took 0.005939 s
   [druid] 2019-06-11 09:36:15,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216975000 ms.0 from job set of time 1560216975000 ms
   [druid] 2019-06-11 09:36:15,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560216975000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:36:15,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1233 from persistence list
   [druid] 2019-06-11 09:36:15,043 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1233
   [druid] 2019-06-11 09:36:15,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1228 from persistence list
   [druid] 2019-06-11 09:36:15,043 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1228
   [druid] 2019-06-11 09:36:15,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1227 from persistence list
   [druid] 2019-06-11 09:36:15,043 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1227
   [druid] 2019-06-11 09:36:15,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1227] at socketTextStream at Transform.scala:18 of time 1560216975000 ms
   [druid] 2019-06-11 09:36:15,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216965000 ms
   [druid] 2019-06-11 09:36:15,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216965000 ms
   [druid] 2019-06-11 09:36:20,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216980000 ms
   [druid] 2019-06-11 09:36:20,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216980000 ms.0 from job set of time 1560216980000 ms
   [druid] 2019-06-11 09:36:20,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1242 (map at Transform.scala:20)
   [druid] 2019-06-11 09:36:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:36:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 355 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1065 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1063, ShuffleMapStage 1064)
   [druid] 2019-06-11 09:36:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1064)
   [druid] 2019-06-11 09:36:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1064 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:36:20,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_532 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:20,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_532_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:36:20,011 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_532_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:20,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 532 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:20,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1064 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:36:20,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1064.0 with 2 tasks
   [druid] 2019-06-11 09:36:20,012 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1064.0 (TID 709, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:20,012 [er for task 709] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1064.0 (TID 709)
   [druid] 2019-06-11 09:36:20,023 [er for task 709] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1064.0 (TID 709). 853 bytes result sent to driver
   [druid] 2019-06-11 09:36:20,023 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1064.0 (TID 710, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:20,023 [er for task 710] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1064.0 (TID 710)
   [druid] 2019-06-11 09:36:20,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1064.0 (TID 709) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:36:20,031 [er for task 710] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1064.0 (TID 710). 810 bytes result sent to driver
   [druid] 2019-06-11 09:36:20,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1064.0 (TID 710) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:36:20,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1064.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1064 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:36:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:36:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:36:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1065)
   [druid] 2019-06-11 09:36:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:36:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1065 (MapPartitionsRDD[1247] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:20,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_533 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:20,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_533_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:20,033 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_533_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:20,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 533 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:20,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1065 (MapPartitionsRDD[1247] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:36:20,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1065.0 with 1 tasks
   [druid] 2019-06-11 09:36:20,034 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1065.0 (TID 711, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:20,034 [er for task 711] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1065.0 (TID 711)
   [druid] 2019-06-11 09:36:20,034 [er for task 711] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:20,034 [er for task 711] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:20,034 [er for task 711] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:20,034 [er for task 711] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:20,035 [er for task 711] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1065.0 (TID 711). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:20,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1065.0 (TID 711) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:20,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1065.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1065 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:36:20,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 355 finished: print at Transform.scala:36, took 0.026865 s
   [druid] 2019-06-11 09:36:20,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:20,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 354 is 83 bytes
   [druid] 2019-06-11 09:36:20,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 355 is 160 bytes
   [druid] 2019-06-11 09:36:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 356 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1068 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1066, ShuffleMapStage 1067)
   [druid] 2019-06-11 09:36:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:36:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1068 (MapPartitionsRDD[1247] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:20,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_534 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:20,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_534_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:20,041 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_534_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:20,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 534 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:20,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1068 (MapPartitionsRDD[1247] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:36:20,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1068.0 with 1 tasks
   [druid] 2019-06-11 09:36:20,042 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1068.0 (TID 712, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:20,042 [er for task 712] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1068.0 (TID 712)
   [druid] 2019-06-11 09:36:20,042 [er for task 712] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:20,042 [er for task 712] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:20,042 [er for task 712] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:20,042 [er for task 712] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:20,043 [er for task 712] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1068.0 (TID 712). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:20,043 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1068.0 (TID 712) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:20,043 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1068.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:20,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1068 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:36:20,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 356 finished: print at Transform.scala:36, took 0.004827 s
   [druid] 2019-06-11 09:36:20,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216980000 ms.0 from job set of time 1560216980000 ms
   [druid] 2019-06-11 09:36:20,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.044 s for time 1560216980000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:36:20,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1240 from persistence list
   [druid] 2019-06-11 09:36:20,044 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1240
   [druid] 2019-06-11 09:36:20,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1235 from persistence list
   [druid] 2019-06-11 09:36:20,044 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1235
   [druid] 2019-06-11 09:36:20,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1234 from persistence list
   [druid] 2019-06-11 09:36:20,045 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1234
   [druid] 2019-06-11 09:36:20,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1234] at socketTextStream at Transform.scala:18 of time 1560216980000 ms
   [druid] 2019-06-11 09:36:20,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216970000 ms
   [druid] 2019-06-11 09:36:20,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216970000 ms
   [druid] 2019-06-11 09:36:25,021 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216985000 ms
   [druid] 2019-06-11 09:36:25,021 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216985000 ms.0 from job set of time 1560216985000 ms
   [druid] 2019-06-11 09:36:25,024 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:25,024 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:36:25,024 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1249 (map at Transform.scala:20)
   [druid] 2019-06-11 09:36:25,024 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 357 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:25,024 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1071 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:25,024 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1069, ShuffleMapStage 1070)
   [druid] 2019-06-11 09:36:25,024 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1069)
   [druid] 2019-06-11 09:36:25,025 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1069 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:36:25,025 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_535 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:25,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_535_piece0 stored as bytes in memory (estimated size 1310.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:36:25,032 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_525_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,032 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_535_piece0 in memory on 192.168.91.1:63518 (size: 1310.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 535 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:25,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1069 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:36:25,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1069.0 with 2 tasks
   [druid] 2019-06-11 09:36:25,033 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1069.0 (TID 713, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:25,033 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_528_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,033 [er for task 713] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1069.0 (TID 713)
   [druid] 2019-06-11 09:36:25,034 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 351
   [druid] 2019-06-11 09:36:25,035 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 353
   [druid] 2019-06-11 09:36:25,036 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_520_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,036 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 350
   [druid] 2019-06-11 09:36:25,037 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_529_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,038 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_522_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,039 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_527_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,039 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_534_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,040 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 349
   [druid] 2019-06-11 09:36:25,041 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_533_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,041 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 352
   [druid] 2019-06-11 09:36:25,042 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_521_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,043 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 348
   [druid] 2019-06-11 09:36:25,043 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 347
   [druid] 2019-06-11 09:36:25,044 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_531_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,044 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_532_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,045 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 345
   [druid] 2019-06-11 09:36:25,046 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_523_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,048 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_526_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,048 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 344
   [druid] 2019-06-11 09:36:25,049 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_530_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,049 [er for task 713] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1069.0 (TID 713). 853 bytes result sent to driver
   [druid] 2019-06-11 09:36:25,050 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1069.0 (TID 714, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:25,050 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1069.0 (TID 713) in 17 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:36:25,050 [er for task 714] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1069.0 (TID 714)
   [druid] 2019-06-11 09:36:25,051 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_524_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 346
   [druid] 2019-06-11 09:36:25,059 [er for task 714] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1069.0 (TID 714). 767 bytes result sent to driver
   [druid] 2019-06-11 09:36:25,059 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1069.0 (TID 714) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:36:25,059 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1069.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:25,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1069 (parallelize at Transform.scala:16) finished in 0.026 s
   [druid] 2019-06-11 09:36:25,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:36:25,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:36:25,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1071)
   [druid] 2019-06-11 09:36:25,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:36:25,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1071 (MapPartitionsRDD[1254] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:25,060 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_536 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:25,061 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_536_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:25,061 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_536_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,061 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 536 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:25,061 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1071 (MapPartitionsRDD[1254] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:36:25,061 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1071.0 with 1 tasks
   [druid] 2019-06-11 09:36:25,062 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1071.0 (TID 715, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:25,062 [er for task 715] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1071.0 (TID 715)
   [druid] 2019-06-11 09:36:25,062 [er for task 715] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:25,063 [er for task 715] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:36:25,063 [er for task 715] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:25,063 [er for task 715] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:25,063 [er for task 715] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1071.0 (TID 715). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:25,064 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1071.0 (TID 715) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:25,064 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1071.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:25,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1071 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:25,064 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 357 finished: print at Transform.scala:36, took 0.040225 s
   [druid] 2019-06-11 09:36:25,066 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:25,066 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 357 is 160 bytes
   [druid] 2019-06-11 09:36:25,067 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 356 is 83 bytes
   [druid] 2019-06-11 09:36:25,067 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 358 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:25,067 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1074 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:25,067 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1072, ShuffleMapStage 1073)
   [druid] 2019-06-11 09:36:25,067 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:36:25,067 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1074 (MapPartitionsRDD[1254] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:25,067 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_537 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:25,069 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_537_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:25,069 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_537_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:25,069 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 537 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:25,070 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1074 (MapPartitionsRDD[1254] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:36:25,070 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1074.0 with 1 tasks
   [druid] 2019-06-11 09:36:25,070 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1074.0 (TID 716, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:25,070 [er for task 716] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1074.0 (TID 716)
   [druid] 2019-06-11 09:36:25,071 [er for task 716] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:25,071 [er for task 716] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:25,071 [er for task 716] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:25,071 [er for task 716] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:25,072 [er for task 716] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1074.0 (TID 716). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:25,072 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1074.0 (TID 716) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:25,072 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1074.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:25,073 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1074 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:25,073 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 358 finished: print at Transform.scala:36, took 0.006578 s
   [druid] 2019-06-11 09:36:25,073 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216985000 ms.0 from job set of time 1560216985000 ms
   [druid] 2019-06-11 09:36:25,073 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.073 s for time 1560216985000 ms (execution: 0.052 s)
   [druid] 2019-06-11 09:36:25,073 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1247 from persistence list
   [druid] 2019-06-11 09:36:25,073 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1247
   [druid] 2019-06-11 09:36:25,073 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1242 from persistence list
   [druid] 2019-06-11 09:36:25,073 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1242
   [druid] 2019-06-11 09:36:25,073 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1241 from persistence list
   [druid] 2019-06-11 09:36:25,074 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1241
   [druid] 2019-06-11 09:36:25,074 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1241] at socketTextStream at Transform.scala:18 of time 1560216985000 ms
   [druid] 2019-06-11 09:36:25,074 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216975000 ms
   [druid] 2019-06-11 09:36:25,074 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216975000 ms
   [druid] 2019-06-11 09:36:30,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216990000 ms
   [druid] 2019-06-11 09:36:30,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216990000 ms.0 from job set of time 1560216990000 ms
   [druid] 2019-06-11 09:36:30,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:36:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1256 (map at Transform.scala:20)
   [druid] 2019-06-11 09:36:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 359 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1077 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1076, ShuffleMapStage 1075)
   [druid] 2019-06-11 09:36:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1075)
   [druid] 2019-06-11 09:36:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1075 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:36:30,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_538 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:30,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_538_piece0 stored as bytes in memory (estimated size 1310.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:36:30,011 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_538_piece0 in memory on 192.168.91.1:63518 (size: 1310.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:30,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 538 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1075 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:36:30,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1075.0 with 2 tasks
   [druid] 2019-06-11 09:36:30,012 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1075.0 (TID 717, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:30,012 [er for task 717] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1075.0 (TID 717)
   [druid] 2019-06-11 09:36:30,022 [er for task 717] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1075.0 (TID 717). 810 bytes result sent to driver
   [druid] 2019-06-11 09:36:30,022 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1075.0 (TID 718, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:30,022 [er for task 718] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1075.0 (TID 718)
   [druid] 2019-06-11 09:36:30,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1075.0 (TID 717) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:36:30,031 [er for task 718] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1075.0 (TID 718). 853 bytes result sent to driver
   [druid] 2019-06-11 09:36:30,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1075.0 (TID 718) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:36:30,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1075.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1075 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:36:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:36:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:36:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1077)
   [druid] 2019-06-11 09:36:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:36:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1077 (MapPartitionsRDD[1261] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:30,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_539 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:30,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_539_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:30,034 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_539_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:30,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 539 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:30,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1077 (MapPartitionsRDD[1261] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:36:30,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1077.0 with 1 tasks
   [druid] 2019-06-11 09:36:30,034 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1077.0 (TID 719, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:30,034 [er for task 719] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1077.0 (TID 719)
   [druid] 2019-06-11 09:36:30,035 [er for task 719] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:30,035 [er for task 719] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:30,035 [er for task 719] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:30,035 [er for task 719] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:30,036 [er for task 719] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1077.0 (TID 719). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:30,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1077.0 (TID 719) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:30,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1077.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1077 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:30,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 359 finished: print at Transform.scala:36, took 0.027774 s
   [druid] 2019-06-11 09:36:30,038 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:30,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 359 is 160 bytes
   [druid] 2019-06-11 09:36:30,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 358 is 83 bytes
   [druid] 2019-06-11 09:36:30,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 360 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:30,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1080 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:30,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1078, ShuffleMapStage 1079)
   [druid] 2019-06-11 09:36:30,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:36:30,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1080 (MapPartitionsRDD[1261] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:30,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_540 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:30,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_540_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:30,041 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_540_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:30,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 540 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:30,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1080 (MapPartitionsRDD[1261] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:36:30,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1080.0 with 1 tasks
   [druid] 2019-06-11 09:36:30,042 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1080.0 (TID 720, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:30,042 [er for task 720] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1080.0 (TID 720)
   [druid] 2019-06-11 09:36:30,043 [er for task 720] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:30,043 [er for task 720] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:30,043 [er for task 720] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:30,043 [er for task 720] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:30,043 [er for task 720] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1080.0 (TID 720). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:30,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1080.0 (TID 720) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:30,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1080.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:30,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1080 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:30,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 360 finished: print at Transform.scala:36, took 0.005404 s
   [druid] 2019-06-11 09:36:30,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216990000 ms.0 from job set of time 1560216990000 ms
   [druid] 2019-06-11 09:36:30,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.044 s for time 1560216990000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:36:30,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1254 from persistence list
   [druid] 2019-06-11 09:36:30,045 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1254
   [druid] 2019-06-11 09:36:30,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1249 from persistence list
   [druid] 2019-06-11 09:36:30,045 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1249
   [druid] 2019-06-11 09:36:30,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1248 from persistence list
   [druid] 2019-06-11 09:36:30,045 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1248
   [druid] 2019-06-11 09:36:30,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1248] at socketTextStream at Transform.scala:18 of time 1560216990000 ms
   [druid] 2019-06-11 09:36:30,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216980000 ms
   [druid] 2019-06-11 09:36:30,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216980000 ms
   [druid] 2019-06-11 09:36:35,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560216995000 ms
   [druid] 2019-06-11 09:36:35,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560216995000 ms.0 from job set of time 1560216995000 ms
   [druid] 2019-06-11 09:36:35,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1263 (map at Transform.scala:20)
   [druid] 2019-06-11 09:36:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:36:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 361 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1083 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1081, ShuffleMapStage 1082)
   [druid] 2019-06-11 09:36:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1082)
   [druid] 2019-06-11 09:36:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1082 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:36:35,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_541 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_541_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:36:35,009 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_541_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:35,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 541 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:35,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1082 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:36:35,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1082.0 with 2 tasks
   [druid] 2019-06-11 09:36:35,010 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1082.0 (TID 721, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:35,010 [er for task 721] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1082.0 (TID 721)
   [druid] 2019-06-11 09:36:35,020 [er for task 721] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1082.0 (TID 721). 810 bytes result sent to driver
   [druid] 2019-06-11 09:36:35,020 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1082.0 (TID 722, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:35,021 [er for task 722] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1082.0 (TID 722)
   [druid] 2019-06-11 09:36:35,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1082.0 (TID 721) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:36:35,030 [er for task 722] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1082.0 (TID 722). 767 bytes result sent to driver
   [druid] 2019-06-11 09:36:35,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1082.0 (TID 722) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:36:35,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1082.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:35,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1082 (parallelize at Transform.scala:16) finished in 0.021 s
   [druid] 2019-06-11 09:36:35,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:36:35,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:36:35,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1083)
   [druid] 2019-06-11 09:36:35,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:36:35,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1083 (MapPartitionsRDD[1268] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:35,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_542 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:35,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_542_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:35,033 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_542_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:35,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 542 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:35,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1083 (MapPartitionsRDD[1268] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:36:35,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1083.0 with 1 tasks
   [druid] 2019-06-11 09:36:35,034 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1083.0 (TID 723, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:35,034 [er for task 723] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1083.0 (TID 723)
   [druid] 2019-06-11 09:36:35,035 [er for task 723] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:35,035 [er for task 723] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:35,035 [er for task 723] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:35,035 [er for task 723] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:35,036 [er for task 723] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1083.0 (TID 723). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:36:35,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1083.0 (TID 723) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:35,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1083.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1083 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:35,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 361 finished: print at Transform.scala:36, took 0.028818 s
   [druid] 2019-06-11 09:36:35,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:35,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 360 is 83 bytes
   [druid] 2019-06-11 09:36:35,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 361 is 160 bytes
   [druid] 2019-06-11 09:36:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 362 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1086 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1084, ShuffleMapStage 1085)
   [druid] 2019-06-11 09:36:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:36:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1086 (MapPartitionsRDD[1268] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:35,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_543 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:35,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_543_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:35,041 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_543_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:35,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 543 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:35,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1086 (MapPartitionsRDD[1268] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:36:35,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1086.0 with 1 tasks
   [druid] 2019-06-11 09:36:35,042 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1086.0 (TID 724, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:35,042 [er for task 724] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1086.0 (TID 724)
   [druid] 2019-06-11 09:36:35,042 [er for task 724] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:35,042 [er for task 724] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:35,042 [er for task 724] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:35,042 [er for task 724] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:35,043 [er for task 724] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1086.0 (TID 724). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:35,043 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1086.0 (TID 724) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:35,043 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1086.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:35,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1086 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:35,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 362 finished: print at Transform.scala:36, took 0.004644 s
   [druid] 2019-06-11 09:36:35,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560216995000 ms.0 from job set of time 1560216995000 ms
   [druid] 2019-06-11 09:36:35,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.044 s for time 1560216995000 ms (execution: 0.039 s)
   [druid] 2019-06-11 09:36:35,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1261 from persistence list
   [druid] 2019-06-11 09:36:35,044 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1261
   [druid] 2019-06-11 09:36:35,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1256 from persistence list
   [druid] 2019-06-11 09:36:35,044 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1256
   [druid] 2019-06-11 09:36:35,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1255 from persistence list
   [druid] 2019-06-11 09:36:35,044 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1255
   [druid] 2019-06-11 09:36:35,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1255] at socketTextStream at Transform.scala:18 of time 1560216995000 ms
   [druid] 2019-06-11 09:36:35,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216985000 ms
   [druid] 2019-06-11 09:36:35,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216985000 ms
   [druid] 2019-06-11 09:36:40,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217000000 ms
   [druid] 2019-06-11 09:36:40,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217000000 ms.0 from job set of time 1560217000000 ms
   [druid] 2019-06-11 09:36:40,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1270 (map at Transform.scala:20)
   [druid] 2019-06-11 09:36:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:36:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 363 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1089 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1087, ShuffleMapStage 1088)
   [druid] 2019-06-11 09:36:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1088)
   [druid] 2019-06-11 09:36:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1088 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:36:40,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_544 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:40,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_544_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:36:40,009 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_544_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:40,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 544 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1088 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:36:40,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1088.0 with 2 tasks
   [druid] 2019-06-11 09:36:40,010 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1088.0 (TID 725, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:40,010 [er for task 725] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1088.0 (TID 725)
   [druid] 2019-06-11 09:36:40,018 [er for task 725] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1088.0 (TID 725). 810 bytes result sent to driver
   [druid] 2019-06-11 09:36:40,019 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1088.0 (TID 726, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:40,019 [er for task 726] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1088.0 (TID 726)
   [druid] 2019-06-11 09:36:40,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1088.0 (TID 725) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:36:40,026 [er for task 726] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1088.0 (TID 726). 767 bytes result sent to driver
   [druid] 2019-06-11 09:36:40,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1088.0 (TID 726) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:36:40,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1088.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:40,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1088 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:36:40,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:36:40,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:36:40,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1089)
   [druid] 2019-06-11 09:36:40,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:36:40,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1089 (MapPartitionsRDD[1275] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:40,027 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_545 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:40,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_545_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:40,028 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_545_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:40,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 545 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1089 (MapPartitionsRDD[1275] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:36:40,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1089.0 with 1 tasks
   [druid] 2019-06-11 09:36:40,029 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1089.0 (TID 727, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:40,029 [er for task 727] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1089.0 (TID 727)
   [druid] 2019-06-11 09:36:40,030 [er for task 727] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:40,030 [er for task 727] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:40,030 [er for task 727] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:40,030 [er for task 727] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:40,030 [er for task 727] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1089.0 (TID 727). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:40,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1089.0 (TID 727) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:40,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1089.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:40,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1089 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:40,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 363 finished: print at Transform.scala:36, took 0.023256 s
   [druid] 2019-06-11 09:36:40,033 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:40,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 362 is 83 bytes
   [druid] 2019-06-11 09:36:40,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 363 is 160 bytes
   [druid] 2019-06-11 09:36:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 364 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1092 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1090, ShuffleMapStage 1091)
   [druid] 2019-06-11 09:36:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:36:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1092 (MapPartitionsRDD[1275] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:40,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_546 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:40,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_546_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:40,035 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_546_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:40,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 546 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1092 (MapPartitionsRDD[1275] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:36:40,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1092.0 with 1 tasks
   [druid] 2019-06-11 09:36:40,035 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1092.0 (TID 728, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:40,036 [er for task 728] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1092.0 (TID 728)
   [druid] 2019-06-11 09:36:40,036 [er for task 728] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:40,036 [er for task 728] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:40,036 [er for task 728] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:40,036 [er for task 728] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:40,037 [er for task 728] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1092.0 (TID 728). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:40,037 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1092.0 (TID 728) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:40,037 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1092.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:40,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1092 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:40,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 364 finished: print at Transform.scala:36, took 0.005209 s
   [druid] 2019-06-11 09:36:40,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217000000 ms.0 from job set of time 1560217000000 ms
   [druid] 2019-06-11 09:36:40,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.038 s for time 1560217000000 ms (execution: 0.033 s)
   [druid] 2019-06-11 09:36:40,038 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1268 from persistence list
   [druid] 2019-06-11 09:36:40,038 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1268
   [druid] 2019-06-11 09:36:40,038 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1263 from persistence list
   [druid] 2019-06-11 09:36:40,038 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1263
   [druid] 2019-06-11 09:36:40,038 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1262 from persistence list
   [druid] 2019-06-11 09:36:40,038 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1262
   [druid] 2019-06-11 09:36:40,038 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1262] at socketTextStream at Transform.scala:18 of time 1560217000000 ms
   [druid] 2019-06-11 09:36:40,039 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216990000 ms
   [druid] 2019-06-11 09:36:40,039 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216990000 ms
   [druid] 2019-06-11 09:36:45,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217005000 ms
   [druid] 2019-06-11 09:36:45,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217005000 ms.0 from job set of time 1560217005000 ms
   [druid] 2019-06-11 09:36:45,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:36:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1277 (map at Transform.scala:20)
   [druid] 2019-06-11 09:36:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 365 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1095 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1094, ShuffleMapStage 1093)
   [druid] 2019-06-11 09:36:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1093)
   [druid] 2019-06-11 09:36:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1093 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:36:45,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_547 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:45,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_547_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:36:45,009 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_547_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:45,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 547 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1093 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:36:45,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1093.0 with 2 tasks
   [druid] 2019-06-11 09:36:45,009 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1093.0 (TID 729, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:45,010 [er for task 729] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1093.0 (TID 729)
   [druid] 2019-06-11 09:36:45,019 [er for task 729] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1093.0 (TID 729). 810 bytes result sent to driver
   [druid] 2019-06-11 09:36:45,020 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1093.0 (TID 730, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:45,020 [er for task 730] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1093.0 (TID 730)
   [druid] 2019-06-11 09:36:45,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1093.0 (TID 729) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:36:45,027 [er for task 730] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1093.0 (TID 730). 767 bytes result sent to driver
   [druid] 2019-06-11 09:36:45,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1093.0 (TID 730) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:36:45,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1093.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1093 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:36:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:36:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:36:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1095)
   [druid] 2019-06-11 09:36:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:36:45,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1095 (MapPartitionsRDD[1282] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:45,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_548 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:45,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_548_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:45,029 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_548_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:45,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 548 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1095 (MapPartitionsRDD[1282] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:36:45,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1095.0 with 1 tasks
   [druid] 2019-06-11 09:36:45,030 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1095.0 (TID 731, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:45,031 [er for task 731] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1095.0 (TID 731)
   [druid] 2019-06-11 09:36:45,031 [er for task 731] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:45,031 [er for task 731] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:45,031 [er for task 731] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:45,031 [er for task 731] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:45,032 [er for task 731] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1095.0 (TID 731). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:45,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1095.0 (TID 731) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:45,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1095.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:45,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1095 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:45,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 365 finished: print at Transform.scala:36, took 0.025342 s
   [druid] 2019-06-11 09:36:45,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:45,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 365 is 160 bytes
   [druid] 2019-06-11 09:36:45,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 364 is 83 bytes
   [druid] 2019-06-11 09:36:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 366 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1098 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1096, ShuffleMapStage 1097)
   [druid] 2019-06-11 09:36:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:36:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1098 (MapPartitionsRDD[1282] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:45,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_549 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:45,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_549_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:36:45,036 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_549_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:45,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 549 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1098 (MapPartitionsRDD[1282] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:36:45,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1098.0 with 1 tasks
   [druid] 2019-06-11 09:36:45,037 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1098.0 (TID 732, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:45,037 [er for task 732] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1098.0 (TID 732)
   [druid] 2019-06-11 09:36:45,038 [er for task 732] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:45,038 [er for task 732] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:45,039 [er for task 732] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:45,039 [er for task 732] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:36:45,040 [er for task 732] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1098.0 (TID 732). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:45,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1098.0 (TID 732) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:45,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1098.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1098 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:36:45,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 366 finished: print at Transform.scala:36, took 0.006083 s
   [druid] 2019-06-11 09:36:45,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217005000 ms.0 from job set of time 1560217005000 ms
   [druid] 2019-06-11 09:36:45,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560217005000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:36:45,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1275 from persistence list
   [druid] 2019-06-11 09:36:45,041 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1275
   [druid] 2019-06-11 09:36:45,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1270 from persistence list
   [druid] 2019-06-11 09:36:45,041 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1270
   [druid] 2019-06-11 09:36:45,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1269 from persistence list
   [druid] 2019-06-11 09:36:45,041 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1269
   [druid] 2019-06-11 09:36:45,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1269] at socketTextStream at Transform.scala:18 of time 1560217005000 ms
   [druid] 2019-06-11 09:36:45,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560216995000 ms
   [druid] 2019-06-11 09:36:45,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560216995000 ms
   [druid] 2019-06-11 09:36:50,010 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_541_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,011 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217010000 ms
   [druid] 2019-06-11 09:36:50,011 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_548_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,011 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217010000 ms.0 from job set of time 1560217010000 ms
   [druid] 2019-06-11 09:36:50,012 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_543_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,013 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 363
   [druid] 2019-06-11 09:36:50,013 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_540_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,014 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 355
   [druid] 2019-06-11 09:36:50,014 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 361
   [druid] 2019-06-11 09:36:50,014 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 354
   [druid] 2019-06-11 09:36:50,015 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_549_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,016 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_538_piece0 on 192.168.91.1:63518 in memory (size: 1310.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,016 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:50,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:36:50,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1284 (map at Transform.scala:20)
   [druid] 2019-06-11 09:36:50,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 367 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:50,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1101 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:50,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1099, ShuffleMapStage 1100)
   [druid] 2019-06-11 09:36:50,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1099)
   [druid] 2019-06-11 09:36:50,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1099 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:36:50,017 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_535_piece0 on 192.168.91.1:63518 in memory (size: 1310.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,017 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_550 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:50,019 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_550_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:36:50,019 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_550_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,019 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 550 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:50,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1099 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:36:50,019 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_536_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,019 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1099.0 with 2 tasks
   [druid] 2019-06-11 09:36:50,020 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1099.0 (TID 733, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:50,020 [er for task 733] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1099.0 (TID 733)
   [druid] 2019-06-11 09:36:50,020 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_537_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,021 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 356
   [druid] 2019-06-11 09:36:50,021 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_545_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,022 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_542_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,022 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 359
   [druid] 2019-06-11 09:36:50,022 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 357
   [druid] 2019-06-11 09:36:50,023 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 360
   [druid] 2019-06-11 09:36:50,023 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_544_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 362
   [druid] 2019-06-11 09:36:50,024 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_547_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 358
   [druid] 2019-06-11 09:36:50,025 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_539_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,026 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_546_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,032 [er for task 733] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1099.0 (TID 733). 853 bytes result sent to driver
   [druid] 2019-06-11 09:36:50,032 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1099.0 (TID 734, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:50,032 [er for task 734] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1099.0 (TID 734)
   [druid] 2019-06-11 09:36:50,032 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1099.0 (TID 733) in 12 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:36:50,041 [er for task 734] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1099.0 (TID 734). 767 bytes result sent to driver
   [druid] 2019-06-11 09:36:50,042 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1099.0 (TID 734) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:36:50,042 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1099.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:50,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1099 (parallelize at Transform.scala:16) finished in 0.022 s
   [druid] 2019-06-11 09:36:50,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:36:50,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:36:50,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1101)
   [druid] 2019-06-11 09:36:50,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:36:50,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1101 (MapPartitionsRDD[1289] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:50,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_551 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:50,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_551_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:50,044 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_551_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 551 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:50,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1101 (MapPartitionsRDD[1289] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:36:50,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1101.0 with 1 tasks
   [druid] 2019-06-11 09:36:50,044 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1101.0 (TID 735, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:50,044 [er for task 735] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1101.0 (TID 735)
   [druid] 2019-06-11 09:36:50,045 [er for task 735] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:50,045 [er for task 735] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:50,045 [er for task 735] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:50,045 [er for task 735] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:50,046 [er for task 735] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1101.0 (TID 735). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:36:50,046 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1101.0 (TID 735) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:50,046 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1101.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:50,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1101 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:50,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 367 finished: print at Transform.scala:36, took 0.029948 s
   [druid] 2019-06-11 09:36:50,050 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:50,050 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 367 is 160 bytes
   [druid] 2019-06-11 09:36:50,050 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 366 is 83 bytes
   [druid] 2019-06-11 09:36:50,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 368 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:50,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1104 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:50,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1102, ShuffleMapStage 1103)
   [druid] 2019-06-11 09:36:50,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:36:50,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1104 (MapPartitionsRDD[1289] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:50,051 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_552 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:50,051 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_552_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:50,052 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_552_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:50,052 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 552 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:50,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1104 (MapPartitionsRDD[1289] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:36:50,052 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1104.0 with 1 tasks
   [druid] 2019-06-11 09:36:50,052 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1104.0 (TID 736, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:50,052 [er for task 736] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1104.0 (TID 736)
   [druid] 2019-06-11 09:36:50,053 [er for task 736] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:50,053 [er for task 736] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:50,053 [er for task 736] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:50,053 [er for task 736] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:50,053 [er for task 736] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1104.0 (TID 736). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:50,054 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1104.0 (TID 736) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:50,054 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1104.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:50,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1104 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:50,054 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 368 finished: print at Transform.scala:36, took 0.004255 s
   [druid] 2019-06-11 09:36:50,054 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217010000 ms.0 from job set of time 1560217010000 ms
   [druid] 2019-06-11 09:36:50,054 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.054 s for time 1560217010000 ms (execution: 0.043 s)
   [druid] 2019-06-11 09:36:50,054 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1282 from persistence list
   [druid] 2019-06-11 09:36:50,054 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1282
   [druid] 2019-06-11 09:36:50,054 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1277 from persistence list
   [druid] 2019-06-11 09:36:50,055 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1277
   [druid] 2019-06-11 09:36:50,055 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1276 from persistence list
   [druid] 2019-06-11 09:36:50,055 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1276
   [druid] 2019-06-11 09:36:50,055 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1276] at socketTextStream at Transform.scala:18 of time 1560217010000 ms
   [druid] 2019-06-11 09:36:50,055 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217000000 ms
   [druid] 2019-06-11 09:36:50,055 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217000000 ms
   [druid] 2019-06-11 09:36:55,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217015000 ms
   [druid] 2019-06-11 09:36:55,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217015000 ms.0 from job set of time 1560217015000 ms
   [druid] 2019-06-11 09:36:55,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:36:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1291 (map at Transform.scala:20)
   [druid] 2019-06-11 09:36:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 369 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1107 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1105, ShuffleMapStage 1106)
   [druid] 2019-06-11 09:36:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1105)
   [druid] 2019-06-11 09:36:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1105 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:36:55,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_553 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:55,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_553_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:36:55,011 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_553_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:55,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 553 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:55,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1105 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:36:55,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1105.0 with 2 tasks
   [druid] 2019-06-11 09:36:55,012 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1105.0 (TID 737, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:55,012 [er for task 737] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1105.0 (TID 737)
   [druid] 2019-06-11 09:36:55,023 [er for task 737] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1105.0 (TID 737). 810 bytes result sent to driver
   [druid] 2019-06-11 09:36:55,023 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1105.0 (TID 738, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:36:55,023 [er for task 738] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1105.0 (TID 738)
   [druid] 2019-06-11 09:36:55,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1105.0 (TID 737) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:36:55,030 [er for task 738] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1105.0 (TID 738). 767 bytes result sent to driver
   [druid] 2019-06-11 09:36:55,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1105.0 (TID 738) in 7 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:36:55,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1105.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:55,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1105 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:36:55,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:36:55,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:36:55,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1107)
   [druid] 2019-06-11 09:36:55,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:36:55,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1107 (MapPartitionsRDD[1296] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:55,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_554 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:55,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_554_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:55,033 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_554_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:55,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 554 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1107 (MapPartitionsRDD[1296] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:36:55,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1107.0 with 1 tasks
   [druid] 2019-06-11 09:36:55,034 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1107.0 (TID 739, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:55,034 [er for task 739] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1107.0 (TID 739)
   [druid] 2019-06-11 09:36:55,035 [er for task 739] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:55,035 [er for task 739] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:55,035 [er for task 739] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:55,035 [er for task 739] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:55,036 [er for task 739] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1107.0 (TID 739). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:55,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1107.0 (TID 739) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:55,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1107.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1107 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:55,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 369 finished: print at Transform.scala:36, took 0.027466 s
   [druid] 2019-06-11 09:36:55,038 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:36:55,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 369 is 160 bytes
   [druid] 2019-06-11 09:36:55,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 368 is 83 bytes
   [druid] 2019-06-11 09:36:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 370 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:36:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1110 (print at Transform.scala:36)
   [druid] 2019-06-11 09:36:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1108, ShuffleMapStage 1109)
   [druid] 2019-06-11 09:36:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:36:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1110 (MapPartitionsRDD[1296] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:36:55,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_555 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:55,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_555_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:36:55,040 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_555_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:36:55,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 555 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:36:55,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1110 (MapPartitionsRDD[1296] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:36:55,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1110.0 with 1 tasks
   [druid] 2019-06-11 09:36:55,041 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1110.0 (TID 740, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:36:55,041 [er for task 740] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1110.0 (TID 740)
   [druid] 2019-06-11 09:36:55,041 [er for task 740] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:36:55,041 [er for task 740] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:55,041 [er for task 740] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:36:55,041 [er for task 740] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:36:55,042 [er for task 740] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1110.0 (TID 740). 966 bytes result sent to driver
   [druid] 2019-06-11 09:36:55,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1110.0 (TID 740) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:36:55,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1110.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:36:55,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1110 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:36:55,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 370 finished: print at Transform.scala:36, took 0.004226 s
   [druid] 2019-06-11 09:36:55,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217015000 ms.0 from job set of time 1560217015000 ms
   [druid] 2019-06-11 09:36:55,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560217015000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:36:55,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1289 from persistence list
   [druid] 2019-06-11 09:36:55,043 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1289
   [druid] 2019-06-11 09:36:55,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1284 from persistence list
   [druid] 2019-06-11 09:36:55,043 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1284
   [druid] 2019-06-11 09:36:55,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1283 from persistence list
   [druid] 2019-06-11 09:36:55,044 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1283
   [druid] 2019-06-11 09:36:55,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1283] at socketTextStream at Transform.scala:18 of time 1560217015000 ms
   [druid] 2019-06-11 09:36:55,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217005000 ms
   [druid] 2019-06-11 09:36:55,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217005000 ms
   [druid] 2019-06-11 09:37:00,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217020000 ms
   [druid] 2019-06-11 09:37:00,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217020000 ms.0 from job set of time 1560217020000 ms
   [druid] 2019-06-11 09:37:00,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:37:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1298 (map at Transform.scala:20)
   [druid] 2019-06-11 09:37:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 371 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1113 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1112, ShuffleMapStage 1111)
   [druid] 2019-06-11 09:37:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1111)
   [druid] 2019-06-11 09:37:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1111 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:37:00,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_556 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:00,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_556_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:37:00,014 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_556_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:00,015 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 556 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:00,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1111 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:37:00,015 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1111.0 with 2 tasks
   [druid] 2019-06-11 09:37:00,015 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1111.0 (TID 741, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:00,016 [er for task 741] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1111.0 (TID 741)
   [druid] 2019-06-11 09:37:00,025 [er for task 741] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1111.0 (TID 741). 767 bytes result sent to driver
   [druid] 2019-06-11 09:37:00,026 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1111.0 (TID 742, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:00,026 [er for task 742] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1111.0 (TID 742)
   [druid] 2019-06-11 09:37:00,026 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1111.0 (TID 741) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:37:00,035 [er for task 742] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1111.0 (TID 742). 810 bytes result sent to driver
   [druid] 2019-06-11 09:37:00,035 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1111.0 (TID 742) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:37:00,035 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1111.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:00,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1111 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:37:00,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:37:00,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:37:00,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1113)
   [druid] 2019-06-11 09:37:00,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:37:00,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1113 (MapPartitionsRDD[1303] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:00,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_557 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:00,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_557_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:00,038 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_557_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:00,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 557 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1113 (MapPartitionsRDD[1303] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:37:00,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1113.0 with 1 tasks
   [druid] 2019-06-11 09:37:00,039 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1113.0 (TID 743, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:00,039 [er for task 743] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1113.0 (TID 743)
   [druid] 2019-06-11 09:37:00,040 [er for task 743] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:00,040 [er for task 743] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:00,040 [er for task 743] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:00,040 [er for task 743] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:00,040 [er for task 743] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1113.0 (TID 743). 966 bytes result sent to driver
   [druid] 2019-06-11 09:37:00,041 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1113.0 (TID 743) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:00,041 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1113.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:00,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1113 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:00,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 371 finished: print at Transform.scala:36, took 0.029949 s
   [druid] 2019-06-11 09:37:00,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:00,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 371 is 160 bytes
   [druid] 2019-06-11 09:37:00,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 370 is 83 bytes
   [druid] 2019-06-11 09:37:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 372 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1116 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1114, ShuffleMapStage 1115)
   [druid] 2019-06-11 09:37:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:37:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1116 (MapPartitionsRDD[1303] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:00,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_558 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:00,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_558_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:00,045 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_558_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:00,045 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 558 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:00,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1116 (MapPartitionsRDD[1303] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:37:00,045 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1116.0 with 1 tasks
   [druid] 2019-06-11 09:37:00,045 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1116.0 (TID 744, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:00,045 [er for task 744] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1116.0 (TID 744)
   [druid] 2019-06-11 09:37:00,046 [er for task 744] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:00,046 [er for task 744] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:00,046 [er for task 744] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:00,046 [er for task 744] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:00,047 [er for task 744] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1116.0 (TID 744). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:37:00,047 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1116.0 (TID 744) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:00,047 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1116.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:00,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1116 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:37:00,048 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 372 finished: print at Transform.scala:36, took 0.004739 s
   [druid] 2019-06-11 09:37:00,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217020000 ms.0 from job set of time 1560217020000 ms
   [druid] 2019-06-11 09:37:00,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.048 s for time 1560217020000 ms (execution: 0.040 s)
   [druid] 2019-06-11 09:37:00,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1296 from persistence list
   [druid] 2019-06-11 09:37:00,048 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1296
   [druid] 2019-06-11 09:37:00,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1291 from persistence list
   [druid] 2019-06-11 09:37:00,049 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1291
   [druid] 2019-06-11 09:37:00,049 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1290 from persistence list
   [druid] 2019-06-11 09:37:00,049 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1290
   [druid] 2019-06-11 09:37:00,049 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1290] at socketTextStream at Transform.scala:18 of time 1560217020000 ms
   [druid] 2019-06-11 09:37:00,049 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217010000 ms
   [druid] 2019-06-11 09:37:00,049 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217010000 ms
   [druid] 2019-06-11 09:37:05,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217025000 ms
   [druid] 2019-06-11 09:37:05,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217025000 ms.0 from job set of time 1560217025000 ms
   [druid] 2019-06-11 09:37:05,006 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:05,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:37:05,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1305 (map at Transform.scala:20)
   [druid] 2019-06-11 09:37:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 373 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1119 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1117, ShuffleMapStage 1118)
   [druid] 2019-06-11 09:37:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1117)
   [druid] 2019-06-11 09:37:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1117 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:37:05,007 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_559 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:05,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_559_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:37:05,009 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_559_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:05,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 559 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1117 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:37:05,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1117.0 with 2 tasks
   [druid] 2019-06-11 09:37:05,010 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1117.0 (TID 745, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:05,010 [er for task 745] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1117.0 (TID 745)
   [druid] 2019-06-11 09:37:05,023 [er for task 745] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1117.0 (TID 745). 767 bytes result sent to driver
   [druid] 2019-06-11 09:37:05,023 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1117.0 (TID 746, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:05,023 [er for task 746] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1117.0 (TID 746)
   [druid] 2019-06-11 09:37:05,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1117.0 (TID 745) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:37:05,031 [er for task 746] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1117.0 (TID 746). 853 bytes result sent to driver
   [druid] 2019-06-11 09:37:05,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1117.0 (TID 746) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:37:05,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1117.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1117 (parallelize at Transform.scala:16) finished in 0.022 s
   [druid] 2019-06-11 09:37:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:37:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:37:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1119)
   [druid] 2019-06-11 09:37:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:37:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1119 (MapPartitionsRDD[1310] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:05,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_560 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:05,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_560_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:05,033 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_560_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:05,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 560 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:05,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1119 (MapPartitionsRDD[1310] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:37:05,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1119.0 with 1 tasks
   [druid] 2019-06-11 09:37:05,033 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1119.0 (TID 747, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:05,033 [er for task 747] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1119.0 (TID 747)
   [druid] 2019-06-11 09:37:05,035 [er for task 747] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:05,035 [er for task 747] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:05,035 [er for task 747] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:05,035 [er for task 747] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:05,035 [er for task 747] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1119.0 (TID 747). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:37:05,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1119.0 (TID 747) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:05,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1119.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1119 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:05,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 373 finished: print at Transform.scala:36, took 0.029411 s
   [druid] 2019-06-11 09:37:05,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:05,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 373 is 160 bytes
   [druid] 2019-06-11 09:37:05,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 372 is 83 bytes
   [druid] 2019-06-11 09:37:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 374 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1122 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1120, ShuffleMapStage 1121)
   [druid] 2019-06-11 09:37:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:37:05,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1122 (MapPartitionsRDD[1310] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:05,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_561 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:05,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_561_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:05,041 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_561_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:05,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 561 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:05,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1122 (MapPartitionsRDD[1310] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:37:05,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1122.0 with 1 tasks
   [druid] 2019-06-11 09:37:05,041 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1122.0 (TID 748, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:05,041 [er for task 748] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1122.0 (TID 748)
   [druid] 2019-06-11 09:37:05,042 [er for task 748] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:05,042 [er for task 748] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:05,042 [er for task 748] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:05,042 [er for task 748] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:05,043 [er for task 748] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1122.0 (TID 748). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:37:05,043 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1122.0 (TID 748) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:05,043 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1122.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:05,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1122 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:05,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 374 finished: print at Transform.scala:36, took 0.004325 s
   [druid] 2019-06-11 09:37:05,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217025000 ms.0 from job set of time 1560217025000 ms
   [druid] 2019-06-11 09:37:05,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560217025000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:37:05,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1303 from persistence list
   [druid] 2019-06-11 09:37:05,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1298 from persistence list
   [druid] 2019-06-11 09:37:05,044 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1303
   [druid] 2019-06-11 09:37:05,044 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1298
   [druid] 2019-06-11 09:37:05,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1297 from persistence list
   [druid] 2019-06-11 09:37:05,044 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1297
   [druid] 2019-06-11 09:37:05,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1297] at socketTextStream at Transform.scala:18 of time 1560217025000 ms
   [druid] 2019-06-11 09:37:05,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217015000 ms
   [druid] 2019-06-11 09:37:05,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217015000 ms
   [druid] 2019-06-11 09:37:10,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217030000 ms
   [druid] 2019-06-11 09:37:10,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217030000 ms.0 from job set of time 1560217030000 ms
   [druid] 2019-06-11 09:37:10,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1312 (map at Transform.scala:20)
   [druid] 2019-06-11 09:37:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:37:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 375 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1125 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1123, ShuffleMapStage 1124)
   [druid] 2019-06-11 09:37:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1124)
   [druid] 2019-06-11 09:37:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1124 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:37:10,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_562 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:10,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_562_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:37:10,009 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_562_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 562 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1124 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:37:10,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1124.0 with 2 tasks
   [druid] 2019-06-11 09:37:10,010 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1124.0 (TID 749, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:10,010 [er for task 749] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1124.0 (TID 749)
   [druid] 2019-06-11 09:37:10,019 [er for task 749] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1124.0 (TID 749). 853 bytes result sent to driver
   [druid] 2019-06-11 09:37:10,019 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1124.0 (TID 750, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:10,019 [er for task 750] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1124.0 (TID 750)
   [druid] 2019-06-11 09:37:10,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1124.0 (TID 749) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:37:10,028 [er for task 750] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1124.0 (TID 750). 810 bytes result sent to driver
   [druid] 2019-06-11 09:37:10,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1124.0 (TID 750) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:37:10,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1124.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:10,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1124 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:37:10,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:37:10,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:37:10,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1125)
   [druid] 2019-06-11 09:37:10,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:37:10,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1125 (MapPartitionsRDD[1317] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:10,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_563 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:10,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_563_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:10,029 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_563_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 563 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:10,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1125 (MapPartitionsRDD[1317] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:37:10,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1125.0 with 1 tasks
   [druid] 2019-06-11 09:37:10,030 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1125.0 (TID 751, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:10,030 [er for task 751] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1125.0 (TID 751)
   [druid] 2019-06-11 09:37:10,030 [er for task 751] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:10,030 [er for task 751] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:10,030 [er for task 751] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:10,030 [er for task 751] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:10,031 [er for task 751] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1125.0 (TID 751). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:37:10,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1125.0 (TID 751) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:10,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1125.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:10,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1125 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:10,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 375 finished: print at Transform.scala:36, took 0.024581 s
   [druid] 2019-06-11 09:37:10,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:10,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 374 is 83 bytes
   [druid] 2019-06-11 09:37:10,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 375 is 160 bytes
   [druid] 2019-06-11 09:37:10,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 376 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:10,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1128 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:10,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1127, ShuffleMapStage 1126)
   [druid] 2019-06-11 09:37:10,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:37:10,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1128 (MapPartitionsRDD[1317] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:10,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_564 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:10,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_564_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:10,042 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_564_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,042 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_554_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 564 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:10,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1128 (MapPartitionsRDD[1317] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:37:10,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1128.0 with 1 tasks
   [druid] 2019-06-11 09:37:10,043 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1128.0 (TID 752, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:10,043 [er for task 752] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1128.0 (TID 752)
   [druid] 2019-06-11 09:37:10,043 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_555_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,044 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_551_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,044 [er for task 752] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:10,044 [er for task 752] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:10,044 [er for task 752] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:10,044 [er for task 752] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:10,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 366
   [druid] 2019-06-11 09:37:10,045 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 369
   [druid] 2019-06-11 09:37:10,045 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_563_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,045 [er for task 752] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1128.0 (TID 752). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:37:10,046 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 367
   [druid] 2019-06-11 09:37:10,046 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1128.0 (TID 752) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:10,046 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1128.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1128 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:37:10,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 376 finished: print at Transform.scala:36, took 0.011579 s
   [druid] 2019-06-11 09:37:10,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217030000 ms.0 from job set of time 1560217030000 ms
   [druid] 2019-06-11 09:37:10,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.046 s for time 1560217030000 ms (execution: 0.041 s)
   [druid] 2019-06-11 09:37:10,046 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_557_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1310 from persistence list
   [druid] 2019-06-11 09:37:10,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1305 from persistence list
   [druid] 2019-06-11 09:37:10,047 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1310
   [druid] 2019-06-11 09:37:10,047 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1305
   [druid] 2019-06-11 09:37:10,047 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1304 from persistence list
   [druid] 2019-06-11 09:37:10,047 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1304
   [druid] 2019-06-11 09:37:10,047 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1304] at socketTextStream at Transform.scala:18 of time 1560217030000 ms
   [druid] 2019-06-11 09:37:10,047 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_550_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,047 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217020000 ms
   [druid] 2019-06-11 09:37:10,047 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217020000 ms
   [druid] 2019-06-11 09:37:10,048 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_559_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,049 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_561_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,050 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_562_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,050 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_560_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,051 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_558_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,052 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_556_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 370
   [druid] 2019-06-11 09:37:10,053 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_552_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 364
   [druid] 2019-06-11 09:37:10,054 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_553_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:10,054 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 368
   [druid] 2019-06-11 09:37:10,054 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 371
   [druid] 2019-06-11 09:37:10,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 365
   [druid] 2019-06-11 09:37:15,012 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217035000 ms
   [druid] 2019-06-11 09:37:15,012 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217035000 ms.0 from job set of time 1560217035000 ms
   [druid] 2019-06-11 09:37:15,013 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1319 (map at Transform.scala:20)
   [druid] 2019-06-11 09:37:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:37:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 377 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1131 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1130, ShuffleMapStage 1129)
   [druid] 2019-06-11 09:37:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1130)
   [druid] 2019-06-11 09:37:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1130 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:37:15,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_565 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:15,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_565_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:37:15,015 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_565_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:15,015 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 565 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:15,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1130 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:37:15,015 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1130.0 with 2 tasks
   [druid] 2019-06-11 09:37:15,015 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1130.0 (TID 753, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:15,015 [er for task 753] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1130.0 (TID 753)
   [druid] 2019-06-11 09:37:15,024 [er for task 753] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1130.0 (TID 753). 810 bytes result sent to driver
   [druid] 2019-06-11 09:37:15,024 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1130.0 (TID 754, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:15,025 [er for task 754] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1130.0 (TID 754)
   [druid] 2019-06-11 09:37:15,025 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1130.0 (TID 753) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:37:15,033 [er for task 754] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1130.0 (TID 754). 810 bytes result sent to driver
   [druid] 2019-06-11 09:37:15,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1130.0 (TID 754) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:37:15,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1130.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1130 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:37:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:37:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:37:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1131)
   [druid] 2019-06-11 09:37:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:37:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1131 (MapPartitionsRDD[1324] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:15,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_566 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:15,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_566_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:15,035 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_566_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:15,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 566 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:15,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1131 (MapPartitionsRDD[1324] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:37:15,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1131.0 with 1 tasks
   [druid] 2019-06-11 09:37:15,036 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1131.0 (TID 755, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:15,036 [er for task 755] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1131.0 (TID 755)
   [druid] 2019-06-11 09:37:15,037 [er for task 755] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:15,037 [er for task 755] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:15,037 [er for task 755] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:15,037 [er for task 755] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:15,037 [er for task 755] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1131.0 (TID 755). 966 bytes result sent to driver
   [druid] 2019-06-11 09:37:15,037 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1131.0 (TID 755) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:15,038 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1131.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1131 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:15,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 377 finished: print at Transform.scala:36, took 0.024471 s
   [druid] 2019-06-11 09:37:15,040 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:15,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 376 is 83 bytes
   [druid] 2019-06-11 09:37:15,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 377 is 160 bytes
   [druid] 2019-06-11 09:37:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 378 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1134 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1132, ShuffleMapStage 1133)
   [druid] 2019-06-11 09:37:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:37:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1134 (MapPartitionsRDD[1324] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:15,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_567 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:15,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_567_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:15,042 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_567_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:15,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 567 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1134 (MapPartitionsRDD[1324] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:37:15,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1134.0 with 1 tasks
   [druid] 2019-06-11 09:37:15,042 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1134.0 (TID 756, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:15,042 [er for task 756] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1134.0 (TID 756)
   [druid] 2019-06-11 09:37:15,043 [er for task 756] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:15,043 [er for task 756] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:15,043 [er for task 756] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:15,043 [er for task 756] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:15,044 [er for task 756] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1134.0 (TID 756). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:37:15,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1134.0 (TID 756) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:15,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1134.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:15,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1134 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:15,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 378 finished: print at Transform.scala:36, took 0.004501 s
   [druid] 2019-06-11 09:37:15,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217035000 ms.0 from job set of time 1560217035000 ms
   [druid] 2019-06-11 09:37:15,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.044 s for time 1560217035000 ms (execution: 0.032 s)
   [druid] 2019-06-11 09:37:15,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1317 from persistence list
   [druid] 2019-06-11 09:37:15,044 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1317
   [druid] 2019-06-11 09:37:15,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1312 from persistence list
   [druid] 2019-06-11 09:37:15,044 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1312
   [druid] 2019-06-11 09:37:15,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1311 from persistence list
   [druid] 2019-06-11 09:37:15,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1311] at socketTextStream at Transform.scala:18 of time 1560217035000 ms
   [druid] 2019-06-11 09:37:15,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217025000 ms
   [druid] 2019-06-11 09:37:15,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217025000 ms
   [druid] 2019-06-11 09:37:15,045 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1311
   [druid] 2019-06-11 09:37:20,004 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217040000 ms
   [druid] 2019-06-11 09:37:20,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217040000 ms.0 from job set of time 1560217040000 ms
   [druid] 2019-06-11 09:37:20,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:37:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1326 (map at Transform.scala:20)
   [druid] 2019-06-11 09:37:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 379 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1137 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1135, ShuffleMapStage 1136)
   [druid] 2019-06-11 09:37:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1135)
   [druid] 2019-06-11 09:37:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1135 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:37:20,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_568 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:20,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_568_piece0 stored as bytes in memory (estimated size 1314.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:37:20,009 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_568_piece0 in memory on 192.168.91.1:63518 (size: 1314.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:20,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 568 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1135 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:37:20,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1135.0 with 2 tasks
   [druid] 2019-06-11 09:37:20,010 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1135.0 (TID 757, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:20,010 [er for task 757] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1135.0 (TID 757)
   [druid] 2019-06-11 09:37:20,020 [er for task 757] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1135.0 (TID 757). 853 bytes result sent to driver
   [druid] 2019-06-11 09:37:20,020 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1135.0 (TID 758, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:20,020 [er for task 758] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1135.0 (TID 758)
   [druid] 2019-06-11 09:37:20,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1135.0 (TID 757) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:37:20,028 [er for task 758] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1135.0 (TID 758). 767 bytes result sent to driver
   [druid] 2019-06-11 09:37:20,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1135.0 (TID 758) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:37:20,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1135.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1135 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:37:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:37:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:37:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1137)
   [druid] 2019-06-11 09:37:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:37:20,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1137 (MapPartitionsRDD[1331] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:20,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_569 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:20,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_569_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:20,030 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_569_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:20,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 569 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1137 (MapPartitionsRDD[1331] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:37:20,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1137.0 with 1 tasks
   [druid] 2019-06-11 09:37:20,031 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1137.0 (TID 759, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:20,031 [er for task 759] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1137.0 (TID 759)
   [druid] 2019-06-11 09:37:20,031 [er for task 759] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:20,031 [er for task 759] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:20,031 [er for task 759] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:20,031 [er for task 759] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:20,032 [er for task 759] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1137.0 (TID 759). 966 bytes result sent to driver
   [druid] 2019-06-11 09:37:20,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1137.0 (TID 759) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:20,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1137.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1137 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:37:20,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 379 finished: print at Transform.scala:36, took 0.025389 s
   [druid] 2019-06-11 09:37:20,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:20,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 379 is 160 bytes
   [druid] 2019-06-11 09:37:20,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 378 is 83 bytes
   [druid] 2019-06-11 09:37:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 380 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1140 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1138, ShuffleMapStage 1139)
   [druid] 2019-06-11 09:37:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:37:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1140 (MapPartitionsRDD[1331] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:20,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_570 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:20,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_570_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:20,036 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_570_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:20,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 570 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1140 (MapPartitionsRDD[1331] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:37:20,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1140.0 with 1 tasks
   [druid] 2019-06-11 09:37:20,037 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1140.0 (TID 760, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:20,037 [er for task 760] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1140.0 (TID 760)
   [druid] 2019-06-11 09:37:20,038 [er for task 760] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:20,038 [er for task 760] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:20,038 [er for task 760] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:20,039 [er for task 760] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:37:20,039 [er for task 760] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1140.0 (TID 760). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:37:20,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1140.0 (TID 760) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:20,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1140.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:20,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1140 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:37:20,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 380 finished: print at Transform.scala:36, took 0.005383 s
   [druid] 2019-06-11 09:37:20,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217040000 ms.0 from job set of time 1560217040000 ms
   [druid] 2019-06-11 09:37:20,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560217040000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:37:20,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1324 from persistence list
   [druid] 2019-06-11 09:37:20,040 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1324
   [druid] 2019-06-11 09:37:20,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1319 from persistence list
   [druid] 2019-06-11 09:37:20,040 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1319
   [druid] 2019-06-11 09:37:20,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1318 from persistence list
   [druid] 2019-06-11 09:37:20,040 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1318
   [druid] 2019-06-11 09:37:20,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1318] at socketTextStream at Transform.scala:18 of time 1560217040000 ms
   [druid] 2019-06-11 09:37:20,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217030000 ms
   [druid] 2019-06-11 09:37:20,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217030000 ms
   [druid] 2019-06-11 09:37:25,004 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217045000 ms
   [druid] 2019-06-11 09:37:25,004 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217045000 ms.0 from job set of time 1560217045000 ms
   [druid] 2019-06-11 09:37:25,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:37:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1333 (map at Transform.scala:20)
   [druid] 2019-06-11 09:37:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 381 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1143 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1141, ShuffleMapStage 1142)
   [druid] 2019-06-11 09:37:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1141)
   [druid] 2019-06-11 09:37:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1141 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:37:25,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_571 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_571_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:37:25,009 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_571_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:25,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 571 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1141 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:37:25,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1141.0 with 2 tasks
   [druid] 2019-06-11 09:37:25,010 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1141.0 (TID 761, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:25,010 [er for task 761] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1141.0 (TID 761)
   [druid] 2019-06-11 09:37:25,019 [er for task 761] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1141.0 (TID 761). 810 bytes result sent to driver
   [druid] 2019-06-11 09:37:25,019 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1141.0 (TID 762, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:25,019 [er for task 762] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1141.0 (TID 762)
   [druid] 2019-06-11 09:37:25,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1141.0 (TID 761) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:37:25,028 [er for task 762] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1141.0 (TID 762). 810 bytes result sent to driver
   [druid] 2019-06-11 09:37:25,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1141.0 (TID 762) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:37:25,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1141.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1141 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:37:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:37:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:37:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1143)
   [druid] 2019-06-11 09:37:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:37:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1143 (MapPartitionsRDD[1338] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:25,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_572 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:25,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_572_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:25,031 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_572_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:25,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 572 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:25,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1143 (MapPartitionsRDD[1338] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:37:25,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1143.0 with 1 tasks
   [druid] 2019-06-11 09:37:25,031 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1143.0 (TID 763, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:25,031 [er for task 763] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1143.0 (TID 763)
   [druid] 2019-06-11 09:37:25,032 [er for task 763] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:25,032 [er for task 763] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:25,032 [er for task 763] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:25,032 [er for task 763] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:25,034 [er for task 763] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1143.0 (TID 763). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:37:25,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1143.0 (TID 763) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:25,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1143.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1143 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:37:25,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 381 finished: print at Transform.scala:36, took 0.027030 s
   [druid] 2019-06-11 09:37:25,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:25,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 381 is 160 bytes
   [druid] 2019-06-11 09:37:25,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 380 is 83 bytes
   [druid] 2019-06-11 09:37:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 382 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1146 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1145, ShuffleMapStage 1144)
   [druid] 2019-06-11 09:37:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:37:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1146 (MapPartitionsRDD[1338] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:25,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_573 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:25,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_573_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:25,039 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_573_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:25,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 573 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1146 (MapPartitionsRDD[1338] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:37:25,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1146.0 with 1 tasks
   [druid] 2019-06-11 09:37:25,040 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1146.0 (TID 764, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:25,040 [er for task 764] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1146.0 (TID 764)
   [druid] 2019-06-11 09:37:25,041 [er for task 764] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:25,041 [er for task 764] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:25,041 [er for task 764] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:25,041 [er for task 764] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:25,041 [er for task 764] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1146.0 (TID 764). 966 bytes result sent to driver
   [druid] 2019-06-11 09:37:25,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1146.0 (TID 764) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:25,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1146.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:25,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1146 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:25,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 382 finished: print at Transform.scala:36, took 0.004970 s
   [druid] 2019-06-11 09:37:25,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217045000 ms.0 from job set of time 1560217045000 ms
   [druid] 2019-06-11 09:37:25,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560217045000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:37:25,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1331 from persistence list
   [druid] 2019-06-11 09:37:25,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1326 from persistence list
   [druid] 2019-06-11 09:37:25,043 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1331
   [druid] 2019-06-11 09:37:25,043 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1326
   [druid] 2019-06-11 09:37:25,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1325 from persistence list
   [druid] 2019-06-11 09:37:25,043 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1325
   [druid] 2019-06-11 09:37:25,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1325] at socketTextStream at Transform.scala:18 of time 1560217045000 ms
   [druid] 2019-06-11 09:37:25,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217035000 ms
   [druid] 2019-06-11 09:37:25,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217035000 ms
   [druid] 2019-06-11 09:37:30,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217050000 ms
   [druid] 2019-06-11 09:37:30,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217050000 ms.0 from job set of time 1560217050000 ms
   [druid] 2019-06-11 09:37:30,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1340 (map at Transform.scala:20)
   [druid] 2019-06-11 09:37:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:37:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 383 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1149 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1148, ShuffleMapStage 1147)
   [druid] 2019-06-11 09:37:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1148)
   [druid] 2019-06-11 09:37:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1148 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:37:30,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_574 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:30,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_574_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:37:30,010 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_574_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:30,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 574 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1148 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:37:30,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1148.0 with 2 tasks
   [druid] 2019-06-11 09:37:30,011 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1148.0 (TID 765, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:30,012 [er for task 765] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1148.0 (TID 765)
   [druid] 2019-06-11 09:37:30,021 [er for task 765] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1148.0 (TID 765). 767 bytes result sent to driver
   [druid] 2019-06-11 09:37:30,021 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1148.0 (TID 766, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:30,021 [er for task 766] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1148.0 (TID 766)
   [druid] 2019-06-11 09:37:30,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1148.0 (TID 765) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:37:30,030 [er for task 766] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1148.0 (TID 766). 810 bytes result sent to driver
   [druid] 2019-06-11 09:37:30,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1148.0 (TID 766) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:37:30,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1148.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1148 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:37:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:37:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:37:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1149)
   [druid] 2019-06-11 09:37:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:37:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1149 (MapPartitionsRDD[1345] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:30,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_575 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:30,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_575_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:30,032 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_575_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:30,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 575 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1149 (MapPartitionsRDD[1345] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:37:30,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1149.0 with 1 tasks
   [druid] 2019-06-11 09:37:30,033 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1149.0 (TID 767, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:30,033 [er for task 767] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1149.0 (TID 767)
   [druid] 2019-06-11 09:37:30,034 [er for task 767] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:30,034 [er for task 767] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:30,034 [er for task 767] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:30,034 [er for task 767] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:30,035 [er for task 767] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1149.0 (TID 767). 966 bytes result sent to driver
   [druid] 2019-06-11 09:37:30,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1149.0 (TID 767) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:30,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1149.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:30,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1149 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:30,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 383 finished: print at Transform.scala:36, took 0.026878 s
   [druid] 2019-06-11 09:37:30,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:30,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 382 is 83 bytes
   [druid] 2019-06-11 09:37:30,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 383 is 160 bytes
   [druid] 2019-06-11 09:37:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 384 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1152 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1150, ShuffleMapStage 1151)
   [druid] 2019-06-11 09:37:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:37:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1152 (MapPartitionsRDD[1345] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:30,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_576 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:30,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_576_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:30,040 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_576_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:30,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 576 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:30,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1152 (MapPartitionsRDD[1345] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:37:30,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1152.0 with 1 tasks
   [druid] 2019-06-11 09:37:30,040 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1152.0 (TID 768, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:30,041 [er for task 768] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1152.0 (TID 768)
   [druid] 2019-06-11 09:37:30,041 [er for task 768] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:30,041 [er for task 768] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:30,041 [er for task 768] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:30,041 [er for task 768] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:30,042 [er for task 768] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1152.0 (TID 768). 966 bytes result sent to driver
   [druid] 2019-06-11 09:37:30,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1152.0 (TID 768) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:30,043 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1152.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1152 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:37:30,043 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 384 finished: print at Transform.scala:36, took 0.006133 s
   [druid] 2019-06-11 09:37:30,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217050000 ms.0 from job set of time 1560217050000 ms
   [druid] 2019-06-11 09:37:30,043 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.043 s for time 1560217050000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:37:30,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1338 from persistence list
   [druid] 2019-06-11 09:37:30,043 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1338
   [druid] 2019-06-11 09:37:30,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1333 from persistence list
   [druid] 2019-06-11 09:37:30,043 [-thread-pool-21] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1333
   [druid] 2019-06-11 09:37:30,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1332 from persistence list
   [druid] 2019-06-11 09:37:30,044 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1332
   [druid] 2019-06-11 09:37:30,044 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1332] at socketTextStream at Transform.scala:18 of time 1560217050000 ms
   [druid] 2019-06-11 09:37:30,044 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217040000 ms
   [druid] 2019-06-11 09:37:30,044 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217040000 ms
   [druid] 2019-06-11 09:37:35,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217055000 ms
   [druid] 2019-06-11 09:37:35,009 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217055000 ms.0 from job set of time 1560217055000 ms
   [druid] 2019-06-11 09:37:35,013 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:35,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1347 (map at Transform.scala:20)
   [druid] 2019-06-11 09:37:35,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:37:35,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 385 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:35,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1155 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:35,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1153, ShuffleMapStage 1154)
   [druid] 2019-06-11 09:37:35,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1154)
   [druid] 2019-06-11 09:37:35,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1154 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:37:35,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_577 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:35,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_577_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:37:35,015 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_577_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,015 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 577 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:35,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1154 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:37:35,016 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1154.0 with 2 tasks
   [druid] 2019-06-11 09:37:35,016 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1154.0 (TID 769, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:35,016 [er for task 769] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1154.0 (TID 769)
   [druid] 2019-06-11 09:37:35,028 [er for task 769] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1154.0 (TID 769). 767 bytes result sent to driver
   [druid] 2019-06-11 09:37:35,029 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1154.0 (TID 770, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:35,029 [er for task 770] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1154.0 (TID 770)
   [druid] 2019-06-11 09:37:35,029 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1154.0 (TID 769) in 13 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:37:35,039 [er for task 770] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1154.0 (TID 770). 810 bytes result sent to driver
   [druid] 2019-06-11 09:37:35,039 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1154.0 (TID 770) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:37:35,039 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1154.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1154 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:37:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:37:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:37:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1155)
   [druid] 2019-06-11 09:37:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:37:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1155 (MapPartitionsRDD[1352] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:35,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_578 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:35,050 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 376
   [druid] 2019-06-11 09:37:35,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_578_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:35,050 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 380
   [druid] 2019-06-11 09:37:35,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 374
   [druid] 2019-06-11 09:37:35,051 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_578_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,051 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 578 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:35,051 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_564_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1155 (MapPartitionsRDD[1352] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:37:35,052 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1155.0 with 1 tasks
   [druid] 2019-06-11 09:37:35,052 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1155.0 (TID 771, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:35,052 [er for task 771] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1155.0 (TID 771)
   [druid] 2019-06-11 09:37:35,053 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_570_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,053 [er for task 771] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:35,053 [er for task 771] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:35,053 [er for task 771] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:35,053 [er for task 771] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:35,054 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_567_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,055 [er for task 771] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1155.0 (TID 771). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:37:35,055 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_569_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,055 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1155.0 (TID 771) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:35,055 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1155.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:35,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1155 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:37:35,055 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_576_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,055 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 385 finished: print at Transform.scala:36, took 0.042052 s
   [druid] 2019-06-11 09:37:35,057 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 381
   [druid] 2019-06-11 09:37:35,057 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 377
   [druid] 2019-06-11 09:37:35,059 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_575_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,060 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 375
   [druid] 2019-06-11 09:37:35,060 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_565_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,061 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 372
   [druid] 2019-06-11 09:37:35,061 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 378
   [druid] 2019-06-11 09:37:35,061 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_568_piece0 on 192.168.91.1:63518 in memory (size: 1314.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,063 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_566_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,065 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:35,066 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_574_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,066 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 384 is 83 bytes
   [druid] 2019-06-11 09:37:35,067 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_571_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,067 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 385 is 160 bytes
   [druid] 2019-06-11 09:37:35,067 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 386 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:35,067 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1158 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:35,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1156, ShuffleMapStage 1157)
   [druid] 2019-06-11 09:37:35,068 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 379
   [druid] 2019-06-11 09:37:35,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:37:35,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1158 (MapPartitionsRDD[1352] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:35,068 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_572_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,068 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_579 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:35,069 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 373
   [druid] 2019-06-11 09:37:35,070 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_573_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,070 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_579_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:35,070 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_579_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:35,070 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 579 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:35,070 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1158 (MapPartitionsRDD[1352] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:37:35,071 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1158.0 with 1 tasks
   [druid] 2019-06-11 09:37:35,071 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1158.0 (TID 772, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:35,071 [er for task 772] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1158.0 (TID 772)
   [druid] 2019-06-11 09:37:35,072 [er for task 772] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:35,072 [er for task 772] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:35,073 [er for task 772] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:35,073 [er for task 772] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:35,073 [er for task 772] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1158.0 (TID 772). 966 bytes result sent to driver
   [druid] 2019-06-11 09:37:35,074 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1158.0 (TID 772) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:35,074 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1158.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:35,074 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1158 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:37:35,074 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 386 finished: print at Transform.scala:36, took 0.009130 s
   [druid] 2019-06-11 09:37:35,075 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217055000 ms.0 from job set of time 1560217055000 ms
   [druid] 2019-06-11 09:37:35,075 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.075 s for time 1560217055000 ms (execution: 0.066 s)
   [druid] 2019-06-11 09:37:35,075 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1345 from persistence list
   [druid] 2019-06-11 09:37:35,076 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1345
   [druid] 2019-06-11 09:37:35,076 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1340 from persistence list
   [druid] 2019-06-11 09:37:35,076 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1340
   [druid] 2019-06-11 09:37:35,076 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1339 from persistence list
   [druid] 2019-06-11 09:37:35,076 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1339
   [druid] 2019-06-11 09:37:35,076 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1339] at socketTextStream at Transform.scala:18 of time 1560217055000 ms
   [druid] 2019-06-11 09:37:35,076 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217045000 ms
   [druid] 2019-06-11 09:37:35,076 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217045000 ms
   [druid] 2019-06-11 09:37:40,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217060000 ms
   [druid] 2019-06-11 09:37:40,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217060000 ms.0 from job set of time 1560217060000 ms
   [druid] 2019-06-11 09:37:40,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:37:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1354 (map at Transform.scala:20)
   [druid] 2019-06-11 09:37:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 387 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1161 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1159, ShuffleMapStage 1160)
   [druid] 2019-06-11 09:37:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1159)
   [druid] 2019-06-11 09:37:40,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1159 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:37:40,007 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_580 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:40,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_580_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:37:40,009 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_580_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:40,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 580 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1159 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:37:40,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1159.0 with 2 tasks
   [druid] 2019-06-11 09:37:40,009 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1159.0 (TID 773, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:40,009 [er for task 773] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1159.0 (TID 773)
   [druid] 2019-06-11 09:37:40,019 [er for task 773] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1159.0 (TID 773). 810 bytes result sent to driver
   [druid] 2019-06-11 09:37:40,019 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1159.0 (TID 774, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:40,019 [er for task 774] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1159.0 (TID 774)
   [druid] 2019-06-11 09:37:40,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1159.0 (TID 773) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:37:40,027 [er for task 774] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1159.0 (TID 774). 767 bytes result sent to driver
   [druid] 2019-06-11 09:37:40,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1159.0 (TID 774) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:37:40,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1159.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1159 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:37:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:37:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:37:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1161)
   [druid] 2019-06-11 09:37:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:37:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1161 (MapPartitionsRDD[1359] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:40,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_581 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:40,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_581_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:40,029 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_581_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:40,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 581 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1161 (MapPartitionsRDD[1359] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:37:40,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1161.0 with 1 tasks
   [druid] 2019-06-11 09:37:40,030 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1161.0 (TID 775, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:40,030 [er for task 775] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1161.0 (TID 775)
   [druid] 2019-06-11 09:37:40,030 [er for task 775] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:40,031 [er for task 775] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:37:40,031 [er for task 775] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:40,031 [er for task 775] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:40,031 [er for task 775] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1161.0 (TID 775). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:37:40,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1161.0 (TID 775) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:40,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1161.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1161 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:40,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 387 finished: print at Transform.scala:36, took 0.025316 s
   [druid] 2019-06-11 09:37:40,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:40,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 387 is 160 bytes
   [druid] 2019-06-11 09:37:40,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 386 is 83 bytes
   [druid] 2019-06-11 09:37:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 388 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1164 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1163, ShuffleMapStage 1162)
   [druid] 2019-06-11 09:37:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:37:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1164 (MapPartitionsRDD[1359] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:40,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_582 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:40,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_582_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:40,036 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_582_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:40,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 582 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:40,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1164 (MapPartitionsRDD[1359] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:37:40,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1164.0 with 1 tasks
   [druid] 2019-06-11 09:37:40,037 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1164.0 (TID 776, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:40,037 [er for task 776] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1164.0 (TID 776)
   [druid] 2019-06-11 09:37:40,038 [er for task 776] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:40,038 [er for task 776] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:40,038 [er for task 776] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:40,038 [er for task 776] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:40,039 [er for task 776] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1164.0 (TID 776). 966 bytes result sent to driver
   [druid] 2019-06-11 09:37:40,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1164.0 (TID 776) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:40,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1164.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:40,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1164 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:40,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 388 finished: print at Transform.scala:36, took 0.005307 s
   [druid] 2019-06-11 09:37:40,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217060000 ms.0 from job set of time 1560217060000 ms
   [druid] 2019-06-11 09:37:40,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.039 s for time 1560217060000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:37:40,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1352 from persistence list
   [druid] 2019-06-11 09:37:40,040 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1352
   [druid] 2019-06-11 09:37:40,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1347 from persistence list
   [druid] 2019-06-11 09:37:40,040 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1347
   [druid] 2019-06-11 09:37:40,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1346 from persistence list
   [druid] 2019-06-11 09:37:40,041 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1346
   [druid] 2019-06-11 09:37:40,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1346] at socketTextStream at Transform.scala:18 of time 1560217060000 ms
   [druid] 2019-06-11 09:37:40,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217050000 ms
   [druid] 2019-06-11 09:37:40,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217050000 ms
   [druid] 2019-06-11 09:37:45,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217065000 ms
   [druid] 2019-06-11 09:37:45,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217065000 ms.0 from job set of time 1560217065000 ms
   [druid] 2019-06-11 09:37:45,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:37:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1361 (map at Transform.scala:20)
   [druid] 2019-06-11 09:37:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 389 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1167 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1166, ShuffleMapStage 1165)
   [druid] 2019-06-11 09:37:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1165)
   [druid] 2019-06-11 09:37:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1165 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:37:45,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_583 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:45,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_583_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:37:45,010 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_583_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:45,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 583 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:45,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1165 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:37:45,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1165.0 with 2 tasks
   [druid] 2019-06-11 09:37:45,011 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1165.0 (TID 777, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:45,011 [er for task 777] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1165.0 (TID 777)
   [druid] 2019-06-11 09:37:45,020 [er for task 777] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1165.0 (TID 777). 853 bytes result sent to driver
   [druid] 2019-06-11 09:37:45,020 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1165.0 (TID 778, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:45,020 [er for task 778] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1165.0 (TID 778)
   [druid] 2019-06-11 09:37:45,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1165.0 (TID 777) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:37:45,029 [er for task 778] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1165.0 (TID 778). 810 bytes result sent to driver
   [druid] 2019-06-11 09:37:45,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1165.0 (TID 778) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:37:45,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1165.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1165 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:37:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:37:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:37:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1167)
   [druid] 2019-06-11 09:37:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:37:45,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1167 (MapPartitionsRDD[1366] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:45,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_584 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:45,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_584_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:45,031 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_584_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:45,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 584 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:45,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1167 (MapPartitionsRDD[1366] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:37:45,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1167.0 with 1 tasks
   [druid] 2019-06-11 09:37:45,032 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1167.0 (TID 779, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:45,032 [er for task 779] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1167.0 (TID 779)
   [druid] 2019-06-11 09:37:45,033 [er for task 779] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:45,033 [er for task 779] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:45,033 [er for task 779] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:45,033 [er for task 779] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:45,033 [er for task 779] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1167.0 (TID 779). 966 bytes result sent to driver
   [druid] 2019-06-11 09:37:45,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1167.0 (TID 779) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:45,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1167.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1167 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:45,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 389 finished: print at Transform.scala:36, took 0.025265 s
   [druid] 2019-06-11 09:37:45,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:45,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 389 is 160 bytes
   [druid] 2019-06-11 09:37:45,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 388 is 83 bytes
   [druid] 2019-06-11 09:37:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 390 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1170 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1168, ShuffleMapStage 1169)
   [druid] 2019-06-11 09:37:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:37:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1170 (MapPartitionsRDD[1366] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:45,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_585 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:45,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_585_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:37:45,038 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_585_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:45,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 585 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1170 (MapPartitionsRDD[1366] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:37:45,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1170.0 with 1 tasks
   [druid] 2019-06-11 09:37:45,039 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1170.0 (TID 780, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:45,039 [er for task 780] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1170.0 (TID 780)
   [druid] 2019-06-11 09:37:45,040 [er for task 780] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:45,040 [er for task 780] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:45,040 [er for task 780] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:45,040 [er for task 780] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:45,041 [er for task 780] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1170.0 (TID 780). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:37:45,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1170.0 (TID 780) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:45,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1170.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:45,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1170 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:37:45,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 390 finished: print at Transform.scala:36, took 0.005633 s
   [druid] 2019-06-11 09:37:45,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217065000 ms.0 from job set of time 1560217065000 ms
   [druid] 2019-06-11 09:37:45,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560217065000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:37:45,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1359 from persistence list
   [druid] 2019-06-11 09:37:45,042 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1359
   [druid] 2019-06-11 09:37:45,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1354 from persistence list
   [druid] 2019-06-11 09:37:45,042 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1354
   [druid] 2019-06-11 09:37:45,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1353 from persistence list
   [druid] 2019-06-11 09:37:45,042 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1353
   [druid] 2019-06-11 09:37:45,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1353] at socketTextStream at Transform.scala:18 of time 1560217065000 ms
   [druid] 2019-06-11 09:37:45,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217055000 ms
   [druid] 2019-06-11 09:37:45,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217055000 ms
   [druid] 2019-06-11 09:37:50,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217070000 ms
   [druid] 2019-06-11 09:37:50,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217070000 ms.0 from job set of time 1560217070000 ms
   [druid] 2019-06-11 09:37:50,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1368 (map at Transform.scala:20)
   [druid] 2019-06-11 09:37:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:37:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 391 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1173 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1171, ShuffleMapStage 1172)
   [druid] 2019-06-11 09:37:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1172)
   [druid] 2019-06-11 09:37:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1172 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:37:50,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_586 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:50,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_586_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:37:50,010 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_586_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:50,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 586 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:50,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1172 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:37:50,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1172.0 with 2 tasks
   [druid] 2019-06-11 09:37:50,011 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1172.0 (TID 781, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:50,011 [er for task 781] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1172.0 (TID 781)
   [druid] 2019-06-11 09:37:50,019 [er for task 781] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1172.0 (TID 781). 810 bytes result sent to driver
   [druid] 2019-06-11 09:37:50,019 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1172.0 (TID 782, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:50,019 [er for task 782] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1172.0 (TID 782)
   [druid] 2019-06-11 09:37:50,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1172.0 (TID 781) in 8 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:37:50,027 [er for task 782] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1172.0 (TID 782). 767 bytes result sent to driver
   [druid] 2019-06-11 09:37:50,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1172.0 (TID 782) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:37:50,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1172.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:50,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1172 (parallelize at Transform.scala:16) finished in 0.016 s
   [druid] 2019-06-11 09:37:50,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:37:50,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:37:50,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1173)
   [druid] 2019-06-11 09:37:50,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:37:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1173 (MapPartitionsRDD[1373] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:50,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_587 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:50,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_587_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:50,029 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_587_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:50,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 587 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1173 (MapPartitionsRDD[1373] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:37:50,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1173.0 with 1 tasks
   [druid] 2019-06-11 09:37:50,029 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1173.0 (TID 783, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:50,029 [er for task 783] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1173.0 (TID 783)
   [druid] 2019-06-11 09:37:50,030 [er for task 783] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:50,030 [er for task 783] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:50,030 [er for task 783] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:50,030 [er for task 783] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:50,031 [er for task 783] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1173.0 (TID 783). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:37:50,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1173.0 (TID 783) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:50,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1173.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1173 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:50,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 391 finished: print at Transform.scala:36, took 0.024140 s
   [druid] 2019-06-11 09:37:50,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:50,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 390 is 83 bytes
   [druid] 2019-06-11 09:37:50,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 391 is 160 bytes
   [druid] 2019-06-11 09:37:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 392 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1176 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1174, ShuffleMapStage 1175)
   [druid] 2019-06-11 09:37:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:37:50,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1176 (MapPartitionsRDD[1373] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:50,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_588 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:50,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_588_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:50,036 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_588_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:50,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 588 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:50,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1176 (MapPartitionsRDD[1373] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:37:50,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1176.0 with 1 tasks
   [druid] 2019-06-11 09:37:50,037 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1176.0 (TID 784, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:50,037 [er for task 784] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1176.0 (TID 784)
   [druid] 2019-06-11 09:37:50,038 [er for task 784] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:50,038 [er for task 784] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:50,038 [er for task 784] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:50,038 [er for task 784] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:50,039 [er for task 784] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1176.0 (TID 784). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:37:50,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1176.0 (TID 784) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:50,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1176.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1176 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:50,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 392 finished: print at Transform.scala:36, took 0.005339 s
   [druid] 2019-06-11 09:37:50,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217070000 ms.0 from job set of time 1560217070000 ms
   [druid] 2019-06-11 09:37:50,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560217070000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:37:50,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1366 from persistence list
   [druid] 2019-06-11 09:37:50,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1361 from persistence list
   [druid] 2019-06-11 09:37:50,040 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1366
   [druid] 2019-06-11 09:37:50,040 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1361
   [druid] 2019-06-11 09:37:50,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1360 from persistence list
   [druid] 2019-06-11 09:37:50,041 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1360
   [druid] 2019-06-11 09:37:50,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1360] at socketTextStream at Transform.scala:18 of time 1560217070000 ms
   [druid] 2019-06-11 09:37:50,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217060000 ms
   [druid] 2019-06-11 09:37:50,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217060000 ms
   [druid] 2019-06-11 09:37:55,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217075000 ms
   [druid] 2019-06-11 09:37:55,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217075000 ms.0 from job set of time 1560217075000 ms
   [druid] 2019-06-11 09:37:55,006 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1375 (map at Transform.scala:20)
   [druid] 2019-06-11 09:37:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:37:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 393 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1179 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1178, ShuffleMapStage 1177)
   [druid] 2019-06-11 09:37:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1178)
   [druid] 2019-06-11 09:37:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1178 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:37:55,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_589 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:55,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_589_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:37:55,009 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_589_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:55,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 589 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1178 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:37:55,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1178.0 with 2 tasks
   [druid] 2019-06-11 09:37:55,010 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1178.0 (TID 785, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:55,010 [er for task 785] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1178.0 (TID 785)
   [druid] 2019-06-11 09:37:55,018 [er for task 785] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1178.0 (TID 785). 810 bytes result sent to driver
   [druid] 2019-06-11 09:37:55,019 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1178.0 (TID 786, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:37:55,019 [er for task 786] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1178.0 (TID 786)
   [druid] 2019-06-11 09:37:55,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1178.0 (TID 785) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:37:55,026 [er for task 786] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1178.0 (TID 786). 767 bytes result sent to driver
   [druid] 2019-06-11 09:37:55,026 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1178.0 (TID 786) in 7 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:37:55,026 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1178.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:55,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1178 (parallelize at Transform.scala:16) finished in 0.016 s
   [druid] 2019-06-11 09:37:55,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:37:55,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:37:55,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1179)
   [druid] 2019-06-11 09:37:55,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:37:55,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1179 (MapPartitionsRDD[1380] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:55,027 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_590 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:55,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_590_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:55,028 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_590_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:55,028 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 590 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1179 (MapPartitionsRDD[1380] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:37:55,028 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1179.0 with 1 tasks
   [druid] 2019-06-11 09:37:55,028 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1179.0 (TID 787, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:55,029 [er for task 787] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1179.0 (TID 787)
   [druid] 2019-06-11 09:37:55,029 [er for task 787] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:55,029 [er for task 787] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:55,029 [er for task 787] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:55,029 [er for task 787] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:55,030 [er for task 787] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1179.0 (TID 787). 966 bytes result sent to driver
   [druid] 2019-06-11 09:37:55,030 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1179.0 (TID 787) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:55,030 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1179.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:55,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1179 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:55,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 393 finished: print at Transform.scala:36, took 0.024334 s
   [druid] 2019-06-11 09:37:55,033 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:37:55,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 392 is 83 bytes
   [druid] 2019-06-11 09:37:55,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 393 is 160 bytes
   [druid] 2019-06-11 09:37:55,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 394 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:37:55,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1182 (print at Transform.scala:36)
   [druid] 2019-06-11 09:37:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1181, ShuffleMapStage 1180)
   [druid] 2019-06-11 09:37:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:37:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1182 (MapPartitionsRDD[1380] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:37:55,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_591 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:55,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_591_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:37:55,036 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_591_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:37:55,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 591 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:37:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1182 (MapPartitionsRDD[1380] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:37:55,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1182.0 with 1 tasks
   [druid] 2019-06-11 09:37:55,036 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1182.0 (TID 788, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:37:55,036 [er for task 788] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1182.0 (TID 788)
   [druid] 2019-06-11 09:37:55,037 [er for task 788] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:37:55,037 [er for task 788] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:55,037 [er for task 788] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:37:55,037 [er for task 788] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:37:55,038 [er for task 788] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1182.0 (TID 788). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:37:55,038 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1182.0 (TID 788) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:37:55,038 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1182.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:37:55,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1182 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:37:55,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 394 finished: print at Transform.scala:36, took 0.005396 s
   [druid] 2019-06-11 09:37:55,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217075000 ms.0 from job set of time 1560217075000 ms
   [druid] 2019-06-11 09:37:55,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.038 s for time 1560217075000 ms (execution: 0.033 s)
   [druid] 2019-06-11 09:37:55,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1373 from persistence list
   [druid] 2019-06-11 09:37:55,039 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1373
   [druid] 2019-06-11 09:37:55,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1368 from persistence list
   [druid] 2019-06-11 09:37:55,039 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1368
   [druid] 2019-06-11 09:37:55,039 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1367 from persistence list
   [druid] 2019-06-11 09:37:55,039 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1367
   [druid] 2019-06-11 09:37:55,039 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1367] at socketTextStream at Transform.scala:18 of time 1560217075000 ms
   [druid] 2019-06-11 09:37:55,039 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217065000 ms
   [druid] 2019-06-11 09:37:55,039 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217065000 ms
   [druid] 2019-06-11 09:38:00,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217080000 ms
   [druid] 2019-06-11 09:38:00,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217080000 ms.0 from job set of time 1560217080000 ms
   [druid] 2019-06-11 09:38:00,014 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_579_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,016 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_581_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,016 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:00,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:38:00,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1382 (map at Transform.scala:20)
   [druid] 2019-06-11 09:38:00,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 395 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:00,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1185 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:00,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1184, ShuffleMapStage 1183)
   [druid] 2019-06-11 09:38:00,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1183)
   [druid] 2019-06-11 09:38:00,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1183 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:38:00,017 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_588_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,017 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_592 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:00,018 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_583_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,018 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_592_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:38:00,019 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_587_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,019 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_592_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,019 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 592 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:00,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1183 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:38:00,019 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_590_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,019 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1183.0 with 2 tasks
   [druid] 2019-06-11 09:38:00,020 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1183.0 (TID 789, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:00,020 [er for task 789] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1183.0 (TID 789)
   [druid] 2019-06-11 09:38:00,020 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_589_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,021 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_582_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,021 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 383
   [druid] 2019-06-11 09:38:00,021 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_591_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,022 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 388
   [druid] 2019-06-11 09:38:00,022 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 389
   [druid] 2019-06-11 09:38:00,023 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_577_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,023 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_578_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 387
   [druid] 2019-06-11 09:38:00,024 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_585_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 390
   [druid] 2019-06-11 09:38:00,025 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_580_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,025 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 385
   [druid] 2019-06-11 09:38:00,026 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 391
   [druid] 2019-06-11 09:38:00,027 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_586_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,031 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 382
   [druid] 2019-06-11 09:38:00,031 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 384
   [druid] 2019-06-11 09:38:00,033 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_584_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,033 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 386
   [druid] 2019-06-11 09:38:00,034 [er for task 789] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1183.0 (TID 789). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:00,035 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1183.0 (TID 790, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:00,035 [er for task 790] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1183.0 (TID 790)
   [druid] 2019-06-11 09:38:00,035 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1183.0 (TID 789) in 16 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:38:00,045 [er for task 790] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1183.0 (TID 790). 767 bytes result sent to driver
   [druid] 2019-06-11 09:38:00,046 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1183.0 (TID 790) in 12 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:38:00,046 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1183.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:00,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1183 (parallelize at Transform.scala:16) finished in 0.027 s
   [druid] 2019-06-11 09:38:00,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:38:00,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:38:00,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1185)
   [druid] 2019-06-11 09:38:00,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:38:00,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1185 (MapPartitionsRDD[1387] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:00,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_593 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:00,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_593_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:00,048 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_593_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,049 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 593 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:00,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1185 (MapPartitionsRDD[1387] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:38:00,049 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1185.0 with 1 tasks
   [druid] 2019-06-11 09:38:00,049 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1185.0 (TID 791, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:00,049 [er for task 791] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1185.0 (TID 791)
   [druid] 2019-06-11 09:38:00,050 [er for task 791] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:00,050 [er for task 791] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:00,050 [er for task 791] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:00,050 [er for task 791] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:00,051 [er for task 791] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1185.0 (TID 791). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:38:00,051 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1185.0 (TID 791) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:00,051 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1185.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:00,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1185 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:38:00,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 395 finished: print at Transform.scala:36, took 0.035372 s
   [druid] 2019-06-11 09:38:00,054 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:00,054 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 395 is 160 bytes
   [druid] 2019-06-11 09:38:00,054 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 394 is 83 bytes
   [druid] 2019-06-11 09:38:00,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 396 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:00,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1188 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:00,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1186, ShuffleMapStage 1187)
   [druid] 2019-06-11 09:38:00,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:38:00,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1188 (MapPartitionsRDD[1387] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:00,055 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_594 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:00,055 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_594_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:00,056 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_594_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:00,056 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 594 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:00,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1188 (MapPartitionsRDD[1387] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:38:00,056 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1188.0 with 1 tasks
   [druid] 2019-06-11 09:38:00,057 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1188.0 (TID 792, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:00,057 [er for task 792] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1188.0 (TID 792)
   [druid] 2019-06-11 09:38:00,057 [er for task 792] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:00,057 [er for task 792] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:00,057 [er for task 792] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:00,057 [er for task 792] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:00,058 [er for task 792] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1188.0 (TID 792). 966 bytes result sent to driver
   [druid] 2019-06-11 09:38:00,058 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1188.0 (TID 792) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:00,058 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1188.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:00,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1188 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:38:00,059 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 396 finished: print at Transform.scala:36, took 0.004859 s
   [druid] 2019-06-11 09:38:00,059 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217080000 ms.0 from job set of time 1560217080000 ms
   [druid] 2019-06-11 09:38:00,059 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.059 s for time 1560217080000 ms (execution: 0.051 s)
   [druid] 2019-06-11 09:38:00,059 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1380 from persistence list
   [druid] 2019-06-11 09:38:00,059 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1375 from persistence list
   [druid] 2019-06-11 09:38:00,059 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1380
   [druid] 2019-06-11 09:38:00,059 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1375
   [druid] 2019-06-11 09:38:00,059 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1374 from persistence list
   [druid] 2019-06-11 09:38:00,060 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1374
   [druid] 2019-06-11 09:38:00,060 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1374] at socketTextStream at Transform.scala:18 of time 1560217080000 ms
   [druid] 2019-06-11 09:38:00,060 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217070000 ms
   [druid] 2019-06-11 09:38:00,060 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217070000 ms
   [druid] 2019-06-11 09:38:05,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217085000 ms
   [druid] 2019-06-11 09:38:05,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217085000 ms.0 from job set of time 1560217085000 ms
   [druid] 2019-06-11 09:38:05,006 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1389 (map at Transform.scala:20)
   [druid] 2019-06-11 09:38:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:38:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 397 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1191 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1189, ShuffleMapStage 1190)
   [druid] 2019-06-11 09:38:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1190)
   [druid] 2019-06-11 09:38:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1190 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:38:05,007 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_595 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:05,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_595_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:38:05,008 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_595_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:05,008 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 595 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1190 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:38:05,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1190.0 with 2 tasks
   [druid] 2019-06-11 09:38:05,010 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1190.0 (TID 793, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:05,010 [er for task 793] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1190.0 (TID 793)
   [druid] 2019-06-11 09:38:05,019 [er for task 793] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1190.0 (TID 793). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:05,019 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1190.0 (TID 794, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:05,019 [er for task 794] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1190.0 (TID 794)
   [druid] 2019-06-11 09:38:05,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1190.0 (TID 793) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:38:05,027 [er for task 794] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1190.0 (TID 794). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:05,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1190.0 (TID 794) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:38:05,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1190.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1190 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:38:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:38:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:38:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1191)
   [druid] 2019-06-11 09:38:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:38:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1191 (MapPartitionsRDD[1394] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:05,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_596 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:05,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_596_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:05,029 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_596_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:05,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 596 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1191 (MapPartitionsRDD[1394] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:38:05,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1191.0 with 1 tasks
   [druid] 2019-06-11 09:38:05,030 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1191.0 (TID 795, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:05,030 [er for task 795] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1191.0 (TID 795)
   [druid] 2019-06-11 09:38:05,031 [er for task 795] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:05,031 [er for task 795] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:05,031 [er for task 795] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:05,031 [er for task 795] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:05,032 [er for task 795] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1191.0 (TID 795). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:38:05,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1191.0 (TID 795) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:05,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1191.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:05,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1191 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:38:05,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 397 finished: print at Transform.scala:36, took 0.026313 s
   [druid] 2019-06-11 09:38:05,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:05,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 396 is 83 bytes
   [druid] 2019-06-11 09:38:05,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 397 is 160 bytes
   [druid] 2019-06-11 09:38:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 398 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1194 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1192, ShuffleMapStage 1193)
   [druid] 2019-06-11 09:38:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:38:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1194 (MapPartitionsRDD[1394] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:05,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_597 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:05,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_597_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:05,037 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_597_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:05,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 597 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:05,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1194 (MapPartitionsRDD[1394] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:38:05,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1194.0 with 1 tasks
   [druid] 2019-06-11 09:38:05,038 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1194.0 (TID 796, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:05,038 [er for task 796] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1194.0 (TID 796)
   [druid] 2019-06-11 09:38:05,040 [er for task 796] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:05,040 [er for task 796] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:05,040 [er for task 796] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:05,040 [er for task 796] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:05,041 [er for task 796] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1194.0 (TID 796). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:38:05,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1194.0 (TID 796) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:05,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1194.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:05,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1194 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:38:05,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 398 finished: print at Transform.scala:36, took 0.006350 s
   [druid] 2019-06-11 09:38:05,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217085000 ms.0 from job set of time 1560217085000 ms
   [druid] 2019-06-11 09:38:05,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560217085000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:38:05,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1387 from persistence list
   [druid] 2019-06-11 09:38:05,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1382 from persistence list
   [druid] 2019-06-11 09:38:05,042 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1387
   [druid] 2019-06-11 09:38:05,042 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1382
   [druid] 2019-06-11 09:38:05,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1381 from persistence list
   [druid] 2019-06-11 09:38:05,042 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1381
   [druid] 2019-06-11 09:38:05,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1381] at socketTextStream at Transform.scala:18 of time 1560217085000 ms
   [druid] 2019-06-11 09:38:05,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217075000 ms
   [druid] 2019-06-11 09:38:05,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217075000 ms
   [druid] 2019-06-11 09:38:10,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217090000 ms
   [druid] 2019-06-11 09:38:10,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217090000 ms.0 from job set of time 1560217090000 ms
   [druid] 2019-06-11 09:38:10,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:38:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1396 (map at Transform.scala:20)
   [druid] 2019-06-11 09:38:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 399 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1197 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1196, ShuffleMapStage 1195)
   [druid] 2019-06-11 09:38:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1195)
   [druid] 2019-06-11 09:38:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1195 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:38:10,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_598 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:10,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_598_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:38:10,010 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_598_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:10,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 598 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:10,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1195 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:38:10,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1195.0 with 2 tasks
   [druid] 2019-06-11 09:38:10,011 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1195.0 (TID 797, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:10,011 [er for task 797] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1195.0 (TID 797)
   [druid] 2019-06-11 09:38:10,020 [er for task 797] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1195.0 (TID 797). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:10,020 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1195.0 (TID 798, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:10,020 [er for task 798] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1195.0 (TID 798)
   [druid] 2019-06-11 09:38:10,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1195.0 (TID 797) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:38:10,029 [er for task 798] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1195.0 (TID 798). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:10,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1195.0 (TID 798) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:38:10,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1195.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:10,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1195 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:38:10,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:38:10,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:38:10,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1197)
   [druid] 2019-06-11 09:38:10,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:38:10,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1197 (MapPartitionsRDD[1401] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:10,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_599 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:10,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_599_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:10,031 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_599_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:10,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 599 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:10,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1197 (MapPartitionsRDD[1401] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:38:10,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1197.0 with 1 tasks
   [druid] 2019-06-11 09:38:10,031 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1197.0 (TID 799, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:10,032 [er for task 799] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1197.0 (TID 799)
   [druid] 2019-06-11 09:38:10,032 [er for task 799] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:10,032 [er for task 799] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:10,032 [er for task 799] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:10,032 [er for task 799] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:10,033 [er for task 799] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1197.0 (TID 799). 966 bytes result sent to driver
   [druid] 2019-06-11 09:38:10,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1197.0 (TID 799) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:10,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1197.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1197 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:38:10,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 399 finished: print at Transform.scala:36, took 0.025657 s
   [druid] 2019-06-11 09:38:10,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:10,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 399 is 160 bytes
   [druid] 2019-06-11 09:38:10,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 398 is 83 bytes
   [druid] 2019-06-11 09:38:10,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 400 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:10,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1200 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:10,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1199, ShuffleMapStage 1198)
   [druid] 2019-06-11 09:38:10,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:38:10,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1200 (MapPartitionsRDD[1401] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:10,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_600 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:10,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_600_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:10,039 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_600_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:10,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 600 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:10,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1200 (MapPartitionsRDD[1401] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:38:10,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1200.0 with 1 tasks
   [druid] 2019-06-11 09:38:10,039 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1200.0 (TID 800, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:10,039 [er for task 800] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1200.0 (TID 800)
   [druid] 2019-06-11 09:38:10,040 [er for task 800] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:10,040 [er for task 800] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:10,040 [er for task 800] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:10,040 [er for task 800] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:10,041 [er for task 800] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1200.0 (TID 800). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:38:10,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1200.0 (TID 800) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:10,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1200.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1200 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:38:10,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 400 finished: print at Transform.scala:36, took 0.004904 s
   [druid] 2019-06-11 09:38:10,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217090000 ms.0 from job set of time 1560217090000 ms
   [druid] 2019-06-11 09:38:10,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560217090000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:38:10,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1394 from persistence list
   [druid] 2019-06-11 09:38:10,042 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1394
   [druid] 2019-06-11 09:38:10,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1389 from persistence list
   [druid] 2019-06-11 09:38:10,042 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1389
   [druid] 2019-06-11 09:38:10,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1388 from persistence list
   [druid] 2019-06-11 09:38:10,042 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1388
   [druid] 2019-06-11 09:38:10,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1388] at socketTextStream at Transform.scala:18 of time 1560217090000 ms
   [druid] 2019-06-11 09:38:10,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217080000 ms
   [druid] 2019-06-11 09:38:10,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217080000 ms
   [druid] 2019-06-11 09:38:15,011 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217095000 ms
   [druid] 2019-06-11 09:38:15,011 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217095000 ms.0 from job set of time 1560217095000 ms
   [druid] 2019-06-11 09:38:15,012 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:38:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1403 (map at Transform.scala:20)
   [druid] 2019-06-11 09:38:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 401 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1203 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1202, ShuffleMapStage 1201)
   [druid] 2019-06-11 09:38:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1201)
   [druid] 2019-06-11 09:38:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1201 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:38:15,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_601 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:15,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_601_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:38:15,015 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_601_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:15,016 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 601 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:15,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1201 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:38:15,016 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1201.0 with 2 tasks
   [druid] 2019-06-11 09:38:15,016 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1201.0 (TID 801, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:15,016 [er for task 801] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1201.0 (TID 801)
   [druid] 2019-06-11 09:38:15,024 [er for task 801] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1201.0 (TID 801). 767 bytes result sent to driver
   [druid] 2019-06-11 09:38:15,025 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1201.0 (TID 802, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:15,025 [er for task 802] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1201.0 (TID 802)
   [druid] 2019-06-11 09:38:15,025 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1201.0 (TID 801) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:38:15,033 [er for task 802] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1201.0 (TID 802). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:15,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1201.0 (TID 802) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:38:15,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1201.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1201 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:38:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:38:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:38:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1203)
   [druid] 2019-06-11 09:38:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:38:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1203 (MapPartitionsRDD[1408] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:15,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_602 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:15,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_602_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:15,034 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_602_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:15,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 602 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:15,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1203 (MapPartitionsRDD[1408] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:38:15,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1203.0 with 1 tasks
   [druid] 2019-06-11 09:38:15,035 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1203.0 (TID 803, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:15,035 [er for task 803] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1203.0 (TID 803)
   [druid] 2019-06-11 09:38:15,035 [er for task 803] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:15,035 [er for task 803] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:15,036 [er for task 803] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:15,036 [er for task 803] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:38:15,037 [er for task 803] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1203.0 (TID 803). 966 bytes result sent to driver
   [druid] 2019-06-11 09:38:15,037 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1203.0 (TID 803) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:15,037 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1203.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1203 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:38:15,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 401 finished: print at Transform.scala:36, took 0.024677 s
   [druid] 2019-06-11 09:38:15,040 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:15,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 401 is 160 bytes
   [druid] 2019-06-11 09:38:15,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 400 is 83 bytes
   [druid] 2019-06-11 09:38:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 402 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1206 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1204, ShuffleMapStage 1205)
   [druid] 2019-06-11 09:38:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:38:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1206 (MapPartitionsRDD[1408] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:15,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_603 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:15,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_603_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:15,042 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_603_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:15,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 603 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1206 (MapPartitionsRDD[1408] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:38:15,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1206.0 with 1 tasks
   [druid] 2019-06-11 09:38:15,043 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1206.0 (TID 804, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:15,043 [er for task 804] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1206.0 (TID 804)
   [druid] 2019-06-11 09:38:15,043 [er for task 804] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:15,043 [er for task 804] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:15,044 [er for task 804] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:15,044 [er for task 804] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:15,044 [er for task 804] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1206.0 (TID 804). 966 bytes result sent to driver
   [druid] 2019-06-11 09:38:15,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1206.0 (TID 804) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:15,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1206.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:15,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1206 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:38:15,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 402 finished: print at Transform.scala:36, took 0.004916 s
   [druid] 2019-06-11 09:38:15,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217095000 ms.0 from job set of time 1560217095000 ms
   [druid] 2019-06-11 09:38:15,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560217095000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:38:15,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1401 from persistence list
   [druid] 2019-06-11 09:38:15,045 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1401
   [druid] 2019-06-11 09:38:15,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1396 from persistence list
   [druid] 2019-06-11 09:38:15,045 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1396
   [druid] 2019-06-11 09:38:15,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1395 from persistence list
   [druid] 2019-06-11 09:38:15,046 [-thread-pool-24] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1395
   [druid] 2019-06-11 09:38:15,046 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1395] at socketTextStream at Transform.scala:18 of time 1560217095000 ms
   [druid] 2019-06-11 09:38:15,046 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217085000 ms
   [druid] 2019-06-11 09:38:15,046 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217085000 ms
   [druid] 2019-06-11 09:38:20,004 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217100000 ms
   [druid] 2019-06-11 09:38:20,004 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217100000 ms.0 from job set of time 1560217100000 ms
   [druid] 2019-06-11 09:38:20,006 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1410 (map at Transform.scala:20)
   [druid] 2019-06-11 09:38:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:38:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 403 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1209 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1207, ShuffleMapStage 1208)
   [druid] 2019-06-11 09:38:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1208)
   [druid] 2019-06-11 09:38:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1208 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:38:20,007 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_604 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:20,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_604_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:38:20,008 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_604_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 604 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1208 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:38:20,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1208.0 with 2 tasks
   [druid] 2019-06-11 09:38:20,009 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1208.0 (TID 805, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:20,009 [er for task 805] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1208.0 (TID 805)
   [druid] 2019-06-11 09:38:20,018 [er for task 805] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1208.0 (TID 805). 853 bytes result sent to driver
   [druid] 2019-06-11 09:38:20,018 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1208.0 (TID 806, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:20,019 [er for task 806] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1208.0 (TID 806)
   [druid] 2019-06-11 09:38:20,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1208.0 (TID 805) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:38:20,027 [er for task 806] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1208.0 (TID 806). 767 bytes result sent to driver
   [druid] 2019-06-11 09:38:20,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1208.0 (TID 806) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:38:20,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1208.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1208 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:38:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:38:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:38:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1209)
   [druid] 2019-06-11 09:38:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:38:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1209 (MapPartitionsRDD[1415] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:20,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_605 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:20,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_605_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:20,030 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_605_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 605 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1209 (MapPartitionsRDD[1415] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:38:20,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1209.0 with 1 tasks
   [druid] 2019-06-11 09:38:20,030 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1209.0 (TID 807, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:20,030 [er for task 807] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1209.0 (TID 807)
   [druid] 2019-06-11 09:38:20,031 [er for task 807] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:20,031 [er for task 807] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:20,031 [er for task 807] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:20,031 [er for task 807] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:20,032 [er for task 807] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1209.0 (TID 807). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:38:20,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1209.0 (TID 807) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:20,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1209.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1209 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:38:20,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 403 finished: print at Transform.scala:36, took 0.025520 s
   [druid] 2019-06-11 09:38:20,041 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_593_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,041 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:20,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 402 is 83 bytes
   [druid] 2019-06-11 09:38:20,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 403 is 160 bytes
   [druid] 2019-06-11 09:38:20,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 404 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:20,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1212 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:20,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1210, ShuffleMapStage 1211)
   [druid] 2019-06-11 09:38:20,042 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_595_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:38:20,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1212 (MapPartitionsRDD[1415] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:20,043 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_598_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_606 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:20,044 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_599_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_606_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:20,044 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_606_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,045 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_600_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,045 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 606 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:20,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1212 (MapPartitionsRDD[1415] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:38:20,045 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1212.0 with 1 tasks
   [druid] 2019-06-11 09:38:20,045 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1212.0 (TID 808, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:20,045 [er for task 808] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1212.0 (TID 808)
   [druid] 2019-06-11 09:38:20,045 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_602_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,046 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 396
   [druid] 2019-06-11 09:38:20,046 [er for task 808] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:20,046 [er for task 808] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:20,046 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_601_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,046 [er for task 808] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:20,046 [er for task 808] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:20,046 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_592_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,047 [er for task 808] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1212.0 (TID 808). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:38:20,047 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1212.0 (TID 808) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:20,047 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1212.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:20,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1212 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:38:20,047 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 404 finished: print at Transform.scala:36, took 0.006027 s
   [druid] 2019-06-11 09:38:20,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217100000 ms.0 from job set of time 1560217100000 ms
   [druid] 2019-06-11 09:38:20,047 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_605_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1408 from persistence list
   [druid] 2019-06-11 09:38:20,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.047 s for time 1560217100000 ms (execution: 0.043 s)
   [druid] 2019-06-11 09:38:20,048 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1408
   [druid] 2019-06-11 09:38:20,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1403 from persistence list
   [druid] 2019-06-11 09:38:20,048 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1403
   [druid] 2019-06-11 09:38:20,048 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1402 from persistence list
   [druid] 2019-06-11 09:38:20,048 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1402
   [druid] 2019-06-11 09:38:20,048 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1402] at socketTextStream at Transform.scala:18 of time 1560217100000 ms
   [druid] 2019-06-11 09:38:20,048 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_604_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,048 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217090000 ms
   [druid] 2019-06-11 09:38:20,048 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217090000 ms
   [druid] 2019-06-11 09:38:20,048 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 395
   [druid] 2019-06-11 09:38:20,049 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 397
   [druid] 2019-06-11 09:38:20,049 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_603_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,050 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_597_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,051 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_596_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 393
   [druid] 2019-06-11 09:38:20,052 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_594_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:20,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 399
   [druid] 2019-06-11 09:38:20,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 394
   [druid] 2019-06-11 09:38:20,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 398
   [druid] 2019-06-11 09:38:20,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 392
   [druid] 2019-06-11 09:38:25,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217105000 ms
   [druid] 2019-06-11 09:38:25,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217105000 ms.0 from job set of time 1560217105000 ms
   [druid] 2019-06-11 09:38:25,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1417 (map at Transform.scala:20)
   [druid] 2019-06-11 09:38:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:38:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 405 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1215 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1214, ShuffleMapStage 1213)
   [druid] 2019-06-11 09:38:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1214)
   [druid] 2019-06-11 09:38:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1214 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:38:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_607 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:25,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_607_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:38:25,010 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_607_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:25,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 607 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:25,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1214 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:38:25,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1214.0 with 2 tasks
   [druid] 2019-06-11 09:38:25,011 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1214.0 (TID 809, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:25,011 [er for task 809] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1214.0 (TID 809)
   [druid] 2019-06-11 09:38:25,021 [er for task 809] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1214.0 (TID 809). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:25,021 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1214.0 (TID 810, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:25,022 [er for task 810] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1214.0 (TID 810)
   [druid] 2019-06-11 09:38:25,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1214.0 (TID 809) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:38:25,029 [er for task 810] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1214.0 (TID 810). 767 bytes result sent to driver
   [druid] 2019-06-11 09:38:25,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1214.0 (TID 810) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:38:25,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1214.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1214 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:38:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:38:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:38:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1215)
   [druid] 2019-06-11 09:38:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:38:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1215 (MapPartitionsRDD[1422] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:25,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_608 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:25,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_608_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:25,032 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_608_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:25,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 608 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:25,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1215 (MapPartitionsRDD[1422] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:38:25,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1215.0 with 1 tasks
   [druid] 2019-06-11 09:38:25,033 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1215.0 (TID 811, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:25,033 [er for task 811] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1215.0 (TID 811)
   [druid] 2019-06-11 09:38:25,034 [er for task 811] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:25,034 [er for task 811] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:25,034 [er for task 811] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:25,034 [er for task 811] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:25,035 [er for task 811] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1215.0 (TID 811). 966 bytes result sent to driver
   [druid] 2019-06-11 09:38:25,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1215.0 (TID 811) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:25,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1215.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1215 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:38:25,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 405 finished: print at Transform.scala:36, took 0.026752 s
   [druid] 2019-06-11 09:38:25,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:25,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 404 is 83 bytes
   [druid] 2019-06-11 09:38:25,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 405 is 160 bytes
   [druid] 2019-06-11 09:38:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 406 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1218 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1217, ShuffleMapStage 1216)
   [druid] 2019-06-11 09:38:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:38:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1218 (MapPartitionsRDD[1422] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:25,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_609 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:25,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_609_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:25,039 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_609_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:25,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 609 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1218 (MapPartitionsRDD[1422] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:38:25,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1218.0 with 1 tasks
   [druid] 2019-06-11 09:38:25,040 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1218.0 (TID 812, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:25,040 [er for task 812] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1218.0 (TID 812)
   [druid] 2019-06-11 09:38:25,041 [er for task 812] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:25,041 [er for task 812] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:25,041 [er for task 812] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:25,041 [er for task 812] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:25,042 [er for task 812] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1218.0 (TID 812). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:38:25,042 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1218.0 (TID 812) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:25,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1218.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:25,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1218 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:38:25,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 406 finished: print at Transform.scala:36, took 0.004800 s
   [druid] 2019-06-11 09:38:25,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217105000 ms.0 from job set of time 1560217105000 ms
   [druid] 2019-06-11 09:38:25,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560217105000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:38:25,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1415 from persistence list
   [druid] 2019-06-11 09:38:25,043 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1415
   [druid] 2019-06-11 09:38:25,043 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1410 from persistence list
   [druid] 2019-06-11 09:38:25,043 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1410
   [druid] 2019-06-11 09:38:25,043 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1409 from persistence list
   [druid] 2019-06-11 09:38:25,043 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1409
   [druid] 2019-06-11 09:38:25,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1409] at socketTextStream at Transform.scala:18 of time 1560217105000 ms
   [druid] 2019-06-11 09:38:25,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217095000 ms
   [druid] 2019-06-11 09:38:25,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217095000 ms
   [druid] 2019-06-11 09:38:30,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217110000 ms
   [druid] 2019-06-11 09:38:30,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217110000 ms.0 from job set of time 1560217110000 ms
   [druid] 2019-06-11 09:38:30,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1424 (map at Transform.scala:20)
   [druid] 2019-06-11 09:38:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:38:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 407 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1221 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1220, ShuffleMapStage 1219)
   [druid] 2019-06-11 09:38:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1220)
   [druid] 2019-06-11 09:38:30,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1220 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:38:30,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_610 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:30,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_610_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:38:30,010 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_610_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:30,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 610 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1220 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:38:30,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1220.0 with 2 tasks
   [druid] 2019-06-11 09:38:30,011 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1220.0 (TID 813, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:30,011 [er for task 813] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1220.0 (TID 813)
   [druid] 2019-06-11 09:38:30,019 [er for task 813] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1220.0 (TID 813). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:30,019 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1220.0 (TID 814, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:30,019 [er for task 814] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1220.0 (TID 814)
   [druid] 2019-06-11 09:38:30,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1220.0 (TID 813) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:38:30,027 [er for task 814] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1220.0 (TID 814). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:30,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1220.0 (TID 814) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:38:30,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1220.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1220 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:38:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:38:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:38:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1221)
   [druid] 2019-06-11 09:38:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:38:30,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1221 (MapPartitionsRDD[1429] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:30,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_611 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:30,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_611_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:30,030 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_611_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:30,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 611 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1221 (MapPartitionsRDD[1429] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:38:30,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1221.0 with 1 tasks
   [druid] 2019-06-11 09:38:30,030 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1221.0 (TID 815, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:30,030 [er for task 815] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1221.0 (TID 815)
   [druid] 2019-06-11 09:38:30,031 [er for task 815] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:30,031 [er for task 815] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:30,031 [er for task 815] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:30,031 [er for task 815] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:30,032 [er for task 815] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1221.0 (TID 815). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:38:30,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1221.0 (TID 815) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:30,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1221.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1221 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:38:30,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 407 finished: print at Transform.scala:36, took 0.025108 s
   [druid] 2019-06-11 09:38:30,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:30,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 406 is 83 bytes
   [druid] 2019-06-11 09:38:30,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 407 is 160 bytes
   [druid] 2019-06-11 09:38:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 408 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1224 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1222, ShuffleMapStage 1223)
   [druid] 2019-06-11 09:38:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:38:30,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1224 (MapPartitionsRDD[1429] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:30,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_612 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:30,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_612_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:30,038 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_612_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:30,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 612 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1224 (MapPartitionsRDD[1429] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:38:30,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1224.0 with 1 tasks
   [druid] 2019-06-11 09:38:30,039 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1224.0 (TID 816, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:30,039 [er for task 816] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1224.0 (TID 816)
   [druid] 2019-06-11 09:38:30,040 [er for task 816] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:30,040 [er for task 816] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:30,040 [er for task 816] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:30,040 [er for task 816] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:30,040 [er for task 816] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1224.0 (TID 816). 966 bytes result sent to driver
   [druid] 2019-06-11 09:38:30,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1224.0 (TID 816) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:30,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1224.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:30,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1224 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:38:30,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 408 finished: print at Transform.scala:36, took 0.005067 s
   [druid] 2019-06-11 09:38:30,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217110000 ms.0 from job set of time 1560217110000 ms
   [druid] 2019-06-11 09:38:30,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560217110000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:38:30,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1422 from persistence list
   [druid] 2019-06-11 09:38:30,042 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1422
   [druid] 2019-06-11 09:38:30,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1417 from persistence list
   [druid] 2019-06-11 09:38:30,042 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1417
   [druid] 2019-06-11 09:38:30,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1416 from persistence list
   [druid] 2019-06-11 09:38:30,042 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1416
   [druid] 2019-06-11 09:38:30,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1416] at socketTextStream at Transform.scala:18 of time 1560217110000 ms
   [druid] 2019-06-11 09:38:30,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217100000 ms
   [druid] 2019-06-11 09:38:30,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217100000 ms
   [druid] 2019-06-11 09:38:35,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217115000 ms
   [druid] 2019-06-11 09:38:35,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217115000 ms.0 from job set of time 1560217115000 ms
   [druid] 2019-06-11 09:38:35,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:38:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1431 (map at Transform.scala:20)
   [druid] 2019-06-11 09:38:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 409 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1227 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1225, ShuffleMapStage 1226)
   [druid] 2019-06-11 09:38:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1225)
   [druid] 2019-06-11 09:38:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1225 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:38:35,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_613 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:35,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_613_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:38:35,011 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_613_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:35,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 613 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:35,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1225 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:38:35,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1225.0 with 2 tasks
   [druid] 2019-06-11 09:38:35,012 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1225.0 (TID 817, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:35,012 [er for task 817] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1225.0 (TID 817)
   [druid] 2019-06-11 09:38:35,023 [er for task 817] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1225.0 (TID 817). 853 bytes result sent to driver
   [druid] 2019-06-11 09:38:35,023 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1225.0 (TID 818, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:35,023 [er for task 818] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1225.0 (TID 818)
   [druid] 2019-06-11 09:38:35,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1225.0 (TID 817) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:38:35,031 [er for task 818] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1225.0 (TID 818). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:35,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1225.0 (TID 818) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:38:35,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1225.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1225 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:38:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:38:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:38:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1227)
   [druid] 2019-06-11 09:38:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:38:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1227 (MapPartitionsRDD[1436] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:35,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_614 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:35,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_614_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:35,035 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_614_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:35,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 614 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:35,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1227 (MapPartitionsRDD[1436] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:38:35,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1227.0 with 1 tasks
   [druid] 2019-06-11 09:38:35,036 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1227.0 (TID 819, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:35,036 [er for task 819] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1227.0 (TID 819)
   [druid] 2019-06-11 09:38:35,036 [er for task 819] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:35,036 [er for task 819] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:35,036 [er for task 819] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:35,036 [er for task 819] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:35,037 [er for task 819] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1227.0 (TID 819). 966 bytes result sent to driver
   [druid] 2019-06-11 09:38:35,037 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1227.0 (TID 819) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:35,037 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1227.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1227 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:38:35,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 409 finished: print at Transform.scala:36, took 0.028451 s
   [druid] 2019-06-11 09:38:35,040 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:35,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 409 is 160 bytes
   [druid] 2019-06-11 09:38:35,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 408 is 83 bytes
   [druid] 2019-06-11 09:38:35,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 410 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:35,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1230 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:35,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1228, ShuffleMapStage 1229)
   [druid] 2019-06-11 09:38:35,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:38:35,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1230 (MapPartitionsRDD[1436] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:35,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_615 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:35,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_615_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:35,042 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_615_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:35,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 615 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:35,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1230 (MapPartitionsRDD[1436] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:38:35,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1230.0 with 1 tasks
   [druid] 2019-06-11 09:38:35,043 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1230.0 (TID 820, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:35,043 [er for task 820] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1230.0 (TID 820)
   [druid] 2019-06-11 09:38:35,044 [er for task 820] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:35,044 [er for task 820] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:35,044 [er for task 820] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:35,044 [er for task 820] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:35,045 [er for task 820] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1230.0 (TID 820). 966 bytes result sent to driver
   [druid] 2019-06-11 09:38:35,045 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1230.0 (TID 820) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:35,045 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1230.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:35,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1230 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:38:35,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 410 finished: print at Transform.scala:36, took 0.005434 s
   [druid] 2019-06-11 09:38:35,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217115000 ms.0 from job set of time 1560217115000 ms
   [druid] 2019-06-11 09:38:35,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560217115000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:38:35,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1429 from persistence list
   [druid] 2019-06-11 09:38:35,046 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1429
   [druid] 2019-06-11 09:38:35,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1424 from persistence list
   [druid] 2019-06-11 09:38:35,046 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1423 from persistence list
   [druid] 2019-06-11 09:38:35,046 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1424
   [druid] 2019-06-11 09:38:35,046 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1423
   [druid] 2019-06-11 09:38:35,046 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1423] at socketTextStream at Transform.scala:18 of time 1560217115000 ms
   [druid] 2019-06-11 09:38:35,046 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217105000 ms
   [druid] 2019-06-11 09:38:35,046 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217105000 ms
   [druid] 2019-06-11 09:38:40,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217120000 ms
   [druid] 2019-06-11 09:38:40,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217120000 ms.0 from job set of time 1560217120000 ms
   [druid] 2019-06-11 09:38:40,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1438 (map at Transform.scala:20)
   [druid] 2019-06-11 09:38:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:38:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 411 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1233 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1232, ShuffleMapStage 1231)
   [druid] 2019-06-11 09:38:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1232)
   [druid] 2019-06-11 09:38:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1232 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:38:40,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_616 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:40,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_616_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:38:40,011 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_616_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:40,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 616 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:40,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1232 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:38:40,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1232.0 with 2 tasks
   [druid] 2019-06-11 09:38:40,012 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1232.0 (TID 821, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:40,012 [er for task 821] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1232.0 (TID 821)
   [druid] 2019-06-11 09:38:40,022 [er for task 821] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1232.0 (TID 821). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:40,022 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1232.0 (TID 822, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:40,022 [er for task 822] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1232.0 (TID 822)
   [druid] 2019-06-11 09:38:40,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1232.0 (TID 821) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:38:40,032 [er for task 822] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1232.0 (TID 822). 853 bytes result sent to driver
   [druid] 2019-06-11 09:38:40,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1232.0 (TID 822) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:38:40,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1232.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1232 (parallelize at Transform.scala:16) finished in 0.021 s
   [druid] 2019-06-11 09:38:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:38:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:38:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1233)
   [druid] 2019-06-11 09:38:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:38:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1233 (MapPartitionsRDD[1443] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:40,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_617 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:40,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_617_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:40,036 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_617_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:40,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 617 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1233 (MapPartitionsRDD[1443] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:38:40,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1233.0 with 1 tasks
   [druid] 2019-06-11 09:38:40,037 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1233.0 (TID 823, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:40,037 [er for task 823] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1233.0 (TID 823)
   [druid] 2019-06-11 09:38:40,038 [er for task 823] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:40,038 [er for task 823] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:40,038 [er for task 823] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:40,038 [er for task 823] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:40,039 [er for task 823] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1233.0 (TID 823). 966 bytes result sent to driver
   [druid] 2019-06-11 09:38:40,039 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1233.0 (TID 823) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:40,039 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1233.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:40,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1233 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:38:40,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 411 finished: print at Transform.scala:36, took 0.030277 s
   [druid] 2019-06-11 09:38:40,041 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:40,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 410 is 83 bytes
   [druid] 2019-06-11 09:38:40,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 411 is 160 bytes
   [druid] 2019-06-11 09:38:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 412 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1236 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1235, ShuffleMapStage 1234)
   [druid] 2019-06-11 09:38:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:38:40,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1236 (MapPartitionsRDD[1443] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:40,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_618 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:40,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_618_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:40,044 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_618_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:40,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 618 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:40,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1236 (MapPartitionsRDD[1443] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:38:40,045 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1236.0 with 1 tasks
   [druid] 2019-06-11 09:38:40,045 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1236.0 (TID 824, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:40,045 [er for task 824] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1236.0 (TID 824)
   [druid] 2019-06-11 09:38:40,046 [er for task 824] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:40,046 [er for task 824] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:40,046 [er for task 824] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:40,046 [er for task 824] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:40,047 [er for task 824] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1236.0 (TID 824). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:38:40,047 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1236.0 (TID 824) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:40,047 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1236.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:40,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1236 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:38:40,048 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 412 finished: print at Transform.scala:36, took 0.006493 s
   [druid] 2019-06-11 09:38:40,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217120000 ms.0 from job set of time 1560217120000 ms
   [druid] 2019-06-11 09:38:40,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.048 s for time 1560217120000 ms (execution: 0.041 s)
   [druid] 2019-06-11 09:38:40,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1436 from persistence list
   [druid] 2019-06-11 09:38:40,048 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1436
   [druid] 2019-06-11 09:38:40,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1431 from persistence list
   [druid] 2019-06-11 09:38:40,049 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1431
   [druid] 2019-06-11 09:38:40,049 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1430 from persistence list
   [druid] 2019-06-11 09:38:40,049 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1430
   [druid] 2019-06-11 09:38:40,049 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1430] at socketTextStream at Transform.scala:18 of time 1560217120000 ms
   [druid] 2019-06-11 09:38:40,049 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217110000 ms
   [druid] 2019-06-11 09:38:40,049 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217110000 ms
   [druid] 2019-06-11 09:38:45,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217125000 ms
   [druid] 2019-06-11 09:38:45,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217125000 ms.0 from job set of time 1560217125000 ms
   [druid] 2019-06-11 09:38:45,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1445 (map at Transform.scala:20)
   [druid] 2019-06-11 09:38:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:38:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 413 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1239 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1238, ShuffleMapStage 1237)
   [druid] 2019-06-11 09:38:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1238)
   [druid] 2019-06-11 09:38:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1238 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:38:45,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_619 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:38:45,016 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_619_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:38:45,016 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_619_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,017 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_606_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,017 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 619 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:45,017 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 403
   [druid] 2019-06-11 09:38:45,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1238 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:38:45,017 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1238.0 with 2 tasks
   [druid] 2019-06-11 09:38:45,017 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 407
   [druid] 2019-06-11 09:38:45,018 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1238.0 (TID 825, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:45,018 [er for task 825] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1238.0 (TID 825)
   [druid] 2019-06-11 09:38:45,019 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_611_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,019 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 404
   [druid] 2019-06-11 09:38:45,020 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_618_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,021 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_617_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,022 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_612_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,023 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_609_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,023 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 400
   [druid] 2019-06-11 09:38:45,024 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_616_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,024 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 401
   [druid] 2019-06-11 09:38:45,025 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_610_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,026 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_614_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,026 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 406
   [druid] 2019-06-11 09:38:45,026 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 402
   [druid] 2019-06-11 09:38:45,027 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_607_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,027 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 405
   [druid] 2019-06-11 09:38:45,028 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 409
   [druid] 2019-06-11 09:38:45,028 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_615_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,029 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_613_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,030 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_608_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,031 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 408
   [druid] 2019-06-11 09:38:45,032 [er for task 825] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1238.0 (TID 825). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:45,032 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1238.0 (TID 826, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:45,032 [er for task 826] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1238.0 (TID 826)
   [druid] 2019-06-11 09:38:45,032 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1238.0 (TID 825) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:38:45,043 [er for task 826] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1238.0 (TID 826). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:45,043 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1238.0 (TID 826) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:38:45,043 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1238.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1238 (parallelize at Transform.scala:16) finished in 0.025 s
   [druid] 2019-06-11 09:38:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:38:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:38:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1239)
   [druid] 2019-06-11 09:38:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:38:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1239 (MapPartitionsRDD[1450] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:45,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_620 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:45,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_620_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:45,045 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_620_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,045 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 620 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:45,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1239 (MapPartitionsRDD[1450] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:38:45,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1239.0 with 1 tasks
   [druid] 2019-06-11 09:38:45,046 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1239.0 (TID 827, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:45,046 [er for task 827] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1239.0 (TID 827)
   [druid] 2019-06-11 09:38:45,047 [er for task 827] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:45,047 [er for task 827] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:45,048 [er for task 827] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:45,048 [er for task 827] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:38:45,049 [er for task 827] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1239.0 (TID 827). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:38:45,049 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1239.0 (TID 827) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:45,049 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1239.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:45,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1239 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:38:45,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 413 finished: print at Transform.scala:36, took 0.040049 s
   [druid] 2019-06-11 09:38:45,051 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:45,052 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 412 is 83 bytes
   [druid] 2019-06-11 09:38:45,052 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 413 is 160 bytes
   [druid] 2019-06-11 09:38:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 414 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1242 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1240, ShuffleMapStage 1241)
   [druid] 2019-06-11 09:38:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:38:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1242 (MapPartitionsRDD[1450] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:45,053 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_621 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:45,053 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_621_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:45,054 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_621_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:45,054 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 621 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:45,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1242 (MapPartitionsRDD[1450] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:38:45,054 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1242.0 with 1 tasks
   [druid] 2019-06-11 09:38:45,054 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1242.0 (TID 828, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:45,055 [er for task 828] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1242.0 (TID 828)
   [druid] 2019-06-11 09:38:45,055 [er for task 828] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:45,055 [er for task 828] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:45,055 [er for task 828] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:45,055 [er for task 828] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:45,056 [er for task 828] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1242.0 (TID 828). 966 bytes result sent to driver
   [druid] 2019-06-11 09:38:45,056 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1242.0 (TID 828) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:45,056 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1242.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:45,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1242 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:38:45,056 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 414 finished: print at Transform.scala:36, took 0.005030 s
   [druid] 2019-06-11 09:38:45,057 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217125000 ms.0 from job set of time 1560217125000 ms
   [druid] 2019-06-11 09:38:45,057 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.057 s for time 1560217125000 ms (execution: 0.050 s)
   [druid] 2019-06-11 09:38:45,057 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1443 from persistence list
   [druid] 2019-06-11 09:38:45,057 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1443
   [druid] 2019-06-11 09:38:45,057 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1438 from persistence list
   [druid] 2019-06-11 09:38:45,057 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1438
   [druid] 2019-06-11 09:38:45,057 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1437 from persistence list
   [druid] 2019-06-11 09:38:45,057 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1437
   [druid] 2019-06-11 09:38:45,057 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1437] at socketTextStream at Transform.scala:18 of time 1560217125000 ms
   [druid] 2019-06-11 09:38:45,057 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217115000 ms
   [druid] 2019-06-11 09:38:45,058 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217115000 ms
   [druid] 2019-06-11 09:38:50,004 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217130000 ms
   [druid] 2019-06-11 09:38:50,004 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217130000 ms.0 from job set of time 1560217130000 ms
   [druid] 2019-06-11 09:38:50,006 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:50,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:38:50,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1452 (map at Transform.scala:20)
   [druid] 2019-06-11 09:38:50,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 415 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:50,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1245 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:50,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1243, ShuffleMapStage 1244)
   [druid] 2019-06-11 09:38:50,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1243)
   [druid] 2019-06-11 09:38:50,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1243 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:38:50,007 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_622 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:50,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_622_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:38:50,009 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_622_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:50,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 622 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1243 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:38:50,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1243.0 with 2 tasks
   [druid] 2019-06-11 09:38:50,010 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1243.0 (TID 829, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:50,010 [er for task 829] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1243.0 (TID 829)
   [druid] 2019-06-11 09:38:50,018 [er for task 829] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1243.0 (TID 829). 810 bytes result sent to driver
   [druid] 2019-06-11 09:38:50,018 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1243.0 (TID 830, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:50,018 [er for task 830] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1243.0 (TID 830)
   [druid] 2019-06-11 09:38:50,018 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1243.0 (TID 829) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:38:50,026 [er for task 830] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1243.0 (TID 830). 767 bytes result sent to driver
   [druid] 2019-06-11 09:38:50,026 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1243.0 (TID 830) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:38:50,026 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1243.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:50,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1243 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:38:50,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:38:50,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:38:50,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1245)
   [druid] 2019-06-11 09:38:50,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:38:50,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1245 (MapPartitionsRDD[1457] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:50,027 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_623 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:50,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_623_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:50,028 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_623_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:50,028 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 623 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:50,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1245 (MapPartitionsRDD[1457] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:38:50,028 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1245.0 with 1 tasks
   [druid] 2019-06-11 09:38:50,029 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1245.0 (TID 831, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:50,029 [er for task 831] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1245.0 (TID 831)
   [druid] 2019-06-11 09:38:50,029 [er for task 831] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:50,029 [er for task 831] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:50,029 [er for task 831] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:50,029 [er for task 831] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:50,030 [er for task 831] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1245.0 (TID 831). 966 bytes result sent to driver
   [druid] 2019-06-11 09:38:50,030 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1245.0 (TID 831) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:50,030 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1245.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:50,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1245 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:38:50,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 415 finished: print at Transform.scala:36, took 0.025104 s
   [druid] 2019-06-11 09:38:50,033 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:50,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 415 is 160 bytes
   [druid] 2019-06-11 09:38:50,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 414 is 83 bytes
   [druid] 2019-06-11 09:38:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 416 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1248 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1247, ShuffleMapStage 1246)
   [druid] 2019-06-11 09:38:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:38:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1248 (MapPartitionsRDD[1457] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:50,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_624 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:50,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_624_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:50,036 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_624_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:50,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 624 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1248 (MapPartitionsRDD[1457] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:38:50,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1248.0 with 1 tasks
   [druid] 2019-06-11 09:38:50,036 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1248.0 (TID 832, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:50,036 [er for task 832] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1248.0 (TID 832)
   [druid] 2019-06-11 09:38:50,037 [er for task 832] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:50,037 [er for task 832] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:50,037 [er for task 832] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:50,037 [er for task 832] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:50,038 [er for task 832] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1248.0 (TID 832). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:38:50,038 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1248.0 (TID 832) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:50,038 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1248.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:50,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1248 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:38:50,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 416 finished: print at Transform.scala:36, took 0.005005 s
   [druid] 2019-06-11 09:38:50,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217130000 ms.0 from job set of time 1560217130000 ms
   [druid] 2019-06-11 09:38:50,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.039 s for time 1560217130000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:38:50,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1450 from persistence list
   [druid] 2019-06-11 09:38:50,039 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1450
   [druid] 2019-06-11 09:38:50,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1445 from persistence list
   [druid] 2019-06-11 09:38:50,039 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1445
   [druid] 2019-06-11 09:38:50,039 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1444 from persistence list
   [druid] 2019-06-11 09:38:50,040 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1444
   [druid] 2019-06-11 09:38:50,040 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1444] at socketTextStream at Transform.scala:18 of time 1560217130000 ms
   [druid] 2019-06-11 09:38:50,040 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217120000 ms
   [druid] 2019-06-11 09:38:50,040 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217120000 ms
   [druid] 2019-06-11 09:38:55,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217135000 ms
   [druid] 2019-06-11 09:38:55,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217135000 ms.0 from job set of time 1560217135000 ms
   [druid] 2019-06-11 09:38:55,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1459 (map at Transform.scala:20)
   [druid] 2019-06-11 09:38:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:38:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 417 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1251 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1250, ShuffleMapStage 1249)
   [druid] 2019-06-11 09:38:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1250)
   [druid] 2019-06-11 09:38:55,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1250 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:38:55,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_625 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:55,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_625_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:38:55,010 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_625_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:55,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 625 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1250 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:38:55,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1250.0 with 2 tasks
   [druid] 2019-06-11 09:38:55,011 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1250.0 (TID 833, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:55,011 [er for task 833] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1250.0 (TID 833)
   [druid] 2019-06-11 09:38:55,025 [er for task 833] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1250.0 (TID 833). 853 bytes result sent to driver
   [druid] 2019-06-11 09:38:55,025 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1250.0 (TID 834, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:38:55,025 [er for task 834] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1250.0 (TID 834)
   [druid] 2019-06-11 09:38:55,025 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1250.0 (TID 833) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:38:55,034 [er for task 834] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1250.0 (TID 834). 853 bytes result sent to driver
   [druid] 2019-06-11 09:38:55,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1250.0 (TID 834) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:38:55,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1250.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1250 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:38:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:38:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:38:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1251)
   [druid] 2019-06-11 09:38:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:38:55,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1251 (MapPartitionsRDD[1464] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:55,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_626 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:55,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_626_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:55,037 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_626_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:55,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 626 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1251 (MapPartitionsRDD[1464] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:38:55,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1251.0 with 1 tasks
   [druid] 2019-06-11 09:38:55,038 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1251.0 (TID 835, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:55,038 [er for task 835] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1251.0 (TID 835)
   [druid] 2019-06-11 09:38:55,039 [er for task 835] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:55,039 [er for task 835] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:55,039 [er for task 835] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:55,039 [er for task 835] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:55,040 [er for task 835] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1251.0 (TID 835). 966 bytes result sent to driver
   [druid] 2019-06-11 09:38:55,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1251.0 (TID 835) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:55,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1251.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:55,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1251 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:38:55,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 417 finished: print at Transform.scala:36, took 0.032254 s
   [druid] 2019-06-11 09:38:55,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:38:55,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 416 is 83 bytes
   [druid] 2019-06-11 09:38:55,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 417 is 160 bytes
   [druid] 2019-06-11 09:38:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 418 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:38:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1254 (print at Transform.scala:36)
   [druid] 2019-06-11 09:38:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1253, ShuffleMapStage 1252)
   [druid] 2019-06-11 09:38:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:38:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1254 (MapPartitionsRDD[1464] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:38:55,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_627 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:55,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_627_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:38:55,045 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_627_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:38:55,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 627 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:38:55,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1254 (MapPartitionsRDD[1464] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:38:55,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1254.0 with 1 tasks
   [druid] 2019-06-11 09:38:55,046 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1254.0 (TID 836, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:38:55,046 [er for task 836] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1254.0 (TID 836)
   [druid] 2019-06-11 09:38:55,047 [er for task 836] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:38:55,047 [er for task 836] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:55,047 [er for task 836] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:38:55,047 [er for task 836] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:38:55,049 [er for task 836] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1254.0 (TID 836). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:38:55,049 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1254.0 (TID 836) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:38:55,049 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1254.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:38:55,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1254 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:38:55,050 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 418 finished: print at Transform.scala:36, took 0.006509 s
   [druid] 2019-06-11 09:38:55,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217135000 ms.0 from job set of time 1560217135000 ms
   [druid] 2019-06-11 09:38:55,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.050 s for time 1560217135000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:38:55,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1457 from persistence list
   [druid] 2019-06-11 09:38:55,050 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1457
   [druid] 2019-06-11 09:38:55,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1452 from persistence list
   [druid] 2019-06-11 09:38:55,050 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1452
   [druid] 2019-06-11 09:38:55,050 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1451 from persistence list
   [druid] 2019-06-11 09:38:55,050 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1451
   [druid] 2019-06-11 09:38:55,050 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1451] at socketTextStream at Transform.scala:18 of time 1560217135000 ms
   [druid] 2019-06-11 09:38:55,051 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217125000 ms
   [druid] 2019-06-11 09:38:55,051 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217125000 ms
   [druid] 2019-06-11 09:39:00,010 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217140000 ms
   [druid] 2019-06-11 09:39:00,010 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217140000 ms.0 from job set of time 1560217140000 ms
   [druid] 2019-06-11 09:39:00,013 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:39:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1466 (map at Transform.scala:20)
   [druid] 2019-06-11 09:39:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 419 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1257 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1256, ShuffleMapStage 1255)
   [druid] 2019-06-11 09:39:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1255)
   [druid] 2019-06-11 09:39:00,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1255 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:39:00,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_628 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:00,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_628_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:39:00,015 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_628_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:00,015 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 628 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:00,016 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1255 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:39:00,016 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1255.0 with 2 tasks
   [druid] 2019-06-11 09:39:00,016 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1255.0 (TID 837, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:00,016 [er for task 837] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1255.0 (TID 837)
   [druid] 2019-06-11 09:39:00,026 [er for task 837] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1255.0 (TID 837). 767 bytes result sent to driver
   [druid] 2019-06-11 09:39:00,026 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1255.0 (TID 838, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:00,026 [er for task 838] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1255.0 (TID 838)
   [druid] 2019-06-11 09:39:00,026 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1255.0 (TID 837) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:39:00,037 [er for task 838] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1255.0 (TID 838). 810 bytes result sent to driver
   [druid] 2019-06-11 09:39:00,037 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1255.0 (TID 838) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:39:00,037 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1255.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1255 (parallelize at Transform.scala:16) finished in 0.021 s
   [druid] 2019-06-11 09:39:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:39:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:39:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1257)
   [druid] 2019-06-11 09:39:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:39:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1257 (MapPartitionsRDD[1471] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:00,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_629 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:00,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_629_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:00,039 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_629_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:00,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 629 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1257 (MapPartitionsRDD[1471] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:39:00,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1257.0 with 1 tasks
   [druid] 2019-06-11 09:39:00,039 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1257.0 (TID 839, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:00,040 [er for task 839] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1257.0 (TID 839)
   [druid] 2019-06-11 09:39:00,040 [er for task 839] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:00,040 [er for task 839] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:00,041 [er for task 839] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:00,041 [er for task 839] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:00,042 [er for task 839] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1257.0 (TID 839). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:00,042 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1257.0 (TID 839) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:00,042 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1257.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:00,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1257 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:39:00,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 419 finished: print at Transform.scala:36, took 0.029003 s
   [druid] 2019-06-11 09:39:00,045 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:00,045 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 419 is 160 bytes
   [druid] 2019-06-11 09:39:00,045 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 418 is 83 bytes
   [druid] 2019-06-11 09:39:00,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 420 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:00,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1260 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:00,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1258, ShuffleMapStage 1259)
   [druid] 2019-06-11 09:39:00,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:39:00,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1260 (MapPartitionsRDD[1471] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:00,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_630 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:00,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_630_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:00,048 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_630_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:00,048 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 630 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:00,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1260 (MapPartitionsRDD[1471] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:39:00,048 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1260.0 with 1 tasks
   [druid] 2019-06-11 09:39:00,049 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1260.0 (TID 840, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:00,049 [er for task 840] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1260.0 (TID 840)
   [druid] 2019-06-11 09:39:00,050 [er for task 840] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:00,050 [er for task 840] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:00,050 [er for task 840] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:00,050 [er for task 840] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:00,051 [er for task 840] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1260.0 (TID 840). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:39:00,051 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1260.0 (TID 840) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:00,051 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1260.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:00,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1260 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:39:00,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 420 finished: print at Transform.scala:36, took 0.007210 s
   [druid] 2019-06-11 09:39:00,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217140000 ms.0 from job set of time 1560217140000 ms
   [druid] 2019-06-11 09:39:00,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.052 s for time 1560217140000 ms (execution: 0.042 s)
   [druid] 2019-06-11 09:39:00,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1464 from persistence list
   [druid] 2019-06-11 09:39:00,052 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1464
   [druid] 2019-06-11 09:39:00,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1459 from persistence list
   [druid] 2019-06-11 09:39:00,052 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1459
   [druid] 2019-06-11 09:39:00,052 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1458 from persistence list
   [druid] 2019-06-11 09:39:00,052 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1458
   [druid] 2019-06-11 09:39:00,052 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1458] at socketTextStream at Transform.scala:18 of time 1560217140000 ms
   [druid] 2019-06-11 09:39:00,052 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217130000 ms
   [druid] 2019-06-11 09:39:00,052 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217130000 ms
   [druid] 2019-06-11 09:39:05,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217145000 ms
   [druid] 2019-06-11 09:39:05,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217145000 ms.0 from job set of time 1560217145000 ms
   [druid] 2019-06-11 09:39:05,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:39:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1473 (map at Transform.scala:20)
   [druid] 2019-06-11 09:39:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 421 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1263 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1261, ShuffleMapStage 1262)
   [druid] 2019-06-11 09:39:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1261)
   [druid] 2019-06-11 09:39:05,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1261 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:39:05,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_631 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:05,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_631_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:39:05,013 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_631_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 631 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:05,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1261 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:39:05,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1261.0 with 2 tasks
   [druid] 2019-06-11 09:39:05,014 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1261.0 (TID 841, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:05,014 [er for task 841] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1261.0 (TID 841)
   [druid] 2019-06-11 09:39:05,024 [er for task 841] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1261.0 (TID 841). 853 bytes result sent to driver
   [druid] 2019-06-11 09:39:05,024 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1261.0 (TID 842, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:05,024 [er for task 842] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1261.0 (TID 842)
   [druid] 2019-06-11 09:39:05,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1261.0 (TID 841) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:39:05,034 [er for task 842] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1261.0 (TID 842). 853 bytes result sent to driver
   [druid] 2019-06-11 09:39:05,035 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1261.0 (TID 842) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:39:05,035 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1261.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1261 (parallelize at Transform.scala:16) finished in 0.022 s
   [druid] 2019-06-11 09:39:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:39:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:39:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1263)
   [druid] 2019-06-11 09:39:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:39:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1263 (MapPartitionsRDD[1478] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:05,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_632 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:05,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_632_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:05,045 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_628_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,045 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_632_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 632 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:05,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1263 (MapPartitionsRDD[1478] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:39:05,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1263.0 with 1 tasks
   [druid] 2019-06-11 09:39:05,046 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_621_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,047 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1263.0 (TID 843, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:05,047 [er for task 843] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1263.0 (TID 843)
   [druid] 2019-06-11 09:39:05,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 415
   [druid] 2019-06-11 09:39:05,048 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_630_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,048 [er for task 843] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:05,048 [er for task 843] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:05,048 [er for task 843] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:05,048 [er for task 843] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:05,049 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_627_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,049 [er for task 843] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1263.0 (TID 843). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:39:05,050 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_624_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,050 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1263.0 (TID 843) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:05,050 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1263.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:05,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1263 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:39:05,050 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_625_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,050 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 421 finished: print at Transform.scala:36, took 0.039596 s
   [druid] 2019-06-11 09:39:05,051 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_623_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 416
   [druid] 2019-06-11 09:39:05,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 413
   [druid] 2019-06-11 09:39:05,052 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_629_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,054 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_620_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 410
   [druid] 2019-06-11 09:39:05,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 414
   [druid] 2019-06-11 09:39:05,056 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_626_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,056 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 411
   [druid] 2019-06-11 09:39:05,057 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:05,057 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 417
   [druid] 2019-06-11 09:39:05,058 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 421 is 160 bytes
   [druid] 2019-06-11 09:39:05,058 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 420 is 83 bytes
   [druid] 2019-06-11 09:39:05,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 422 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:05,058 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_622_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1266 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:05,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1265, ShuffleMapStage 1264)
   [druid] 2019-06-11 09:39:05,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:39:05,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1266 (MapPartitionsRDD[1478] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:05,059 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_633 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:05,061 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_633_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:05,061 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_633_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,062 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 412
   [druid] 2019-06-11 09:39:05,064 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 633 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:05,065 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1266 (MapPartitionsRDD[1478] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:39:05,065 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1266.0 with 1 tasks
   [druid] 2019-06-11 09:39:05,065 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_619_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:05,065 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1266.0 (TID 844, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:05,066 [er for task 844] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1266.0 (TID 844)
   [druid] 2019-06-11 09:39:05,067 [er for task 844] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:05,067 [er for task 844] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:05,067 [er for task 844] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:05,067 [er for task 844] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:05,069 [er for task 844] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1266.0 (TID 844). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:39:05,070 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1266.0 (TID 844) in 5 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:05,070 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1266.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:05,070 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1266 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:39:05,070 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 422 finished: print at Transform.scala:36, took 0.013559 s
   [druid] 2019-06-11 09:39:05,071 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217145000 ms.0 from job set of time 1560217145000 ms
   [druid] 2019-06-11 09:39:05,071 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.071 s for time 1560217145000 ms (execution: 0.063 s)
   [druid] 2019-06-11 09:39:05,071 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1471 from persistence list
   [druid] 2019-06-11 09:39:05,071 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1466 from persistence list
   [druid] 2019-06-11 09:39:05,071 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1471
   [druid] 2019-06-11 09:39:05,071 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1466
   [druid] 2019-06-11 09:39:05,071 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1465 from persistence list
   [druid] 2019-06-11 09:39:05,071 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1465
   [druid] 2019-06-11 09:39:05,071 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1465] at socketTextStream at Transform.scala:18 of time 1560217145000 ms
   [druid] 2019-06-11 09:39:05,071 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217135000 ms
   [druid] 2019-06-11 09:39:05,071 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217135000 ms
   [druid] 2019-06-11 09:39:10,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217150000 ms
   [druid] 2019-06-11 09:39:10,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217150000 ms.0 from job set of time 1560217150000 ms
   [druid] 2019-06-11 09:39:10,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:39:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1480 (map at Transform.scala:20)
   [druid] 2019-06-11 09:39:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 423 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1269 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1268, ShuffleMapStage 1267)
   [druid] 2019-06-11 09:39:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1267)
   [druid] 2019-06-11 09:39:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1267 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:39:10,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_634 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:10,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_634_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:39:10,011 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_634_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:10,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 634 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:10,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1267 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:39:10,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1267.0 with 2 tasks
   [druid] 2019-06-11 09:39:10,011 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1267.0 (TID 845, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:10,011 [er for task 845] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1267.0 (TID 845)
   [druid] 2019-06-11 09:39:10,023 [er for task 845] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1267.0 (TID 845). 810 bytes result sent to driver
   [druid] 2019-06-11 09:39:10,023 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1267.0 (TID 846, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:10,023 [er for task 846] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1267.0 (TID 846)
   [druid] 2019-06-11 09:39:10,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1267.0 (TID 845) in 12 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:39:10,033 [er for task 846] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1267.0 (TID 846). 853 bytes result sent to driver
   [druid] 2019-06-11 09:39:10,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1267.0 (TID 846) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:39:10,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1267.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1267 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:39:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:39:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:39:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1269)
   [druid] 2019-06-11 09:39:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:39:10,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1269 (MapPartitionsRDD[1485] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:10,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_635 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:10,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_635_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:10,036 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_635_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:10,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 635 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:10,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1269 (MapPartitionsRDD[1485] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:39:10,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1269.0 with 1 tasks
   [druid] 2019-06-11 09:39:10,037 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1269.0 (TID 847, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:10,037 [er for task 847] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1269.0 (TID 847)
   [druid] 2019-06-11 09:39:10,038 [er for task 847] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:10,038 [er for task 847] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:10,038 [er for task 847] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:10,038 [er for task 847] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:10,039 [er for task 847] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1269.0 (TID 847). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:10,040 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1269.0 (TID 847) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:10,040 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1269.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:10,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1269 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:39:10,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 423 finished: print at Transform.scala:36, took 0.031488 s
   [druid] 2019-06-11 09:39:10,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:10,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 423 is 160 bytes
   [druid] 2019-06-11 09:39:10,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 422 is 83 bytes
   [druid] 2019-06-11 09:39:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 424 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1272 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1271, ShuffleMapStage 1270)
   [druid] 2019-06-11 09:39:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:39:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1272 (MapPartitionsRDD[1485] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:10,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_636 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:10,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_636_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:10,046 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_636_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:10,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 636 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1272 (MapPartitionsRDD[1485] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:39:10,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1272.0 with 1 tasks
   [druid] 2019-06-11 09:39:10,046 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1272.0 (TID 848, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:10,047 [er for task 848] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1272.0 (TID 848)
   [druid] 2019-06-11 09:39:10,047 [er for task 848] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:10,047 [er for task 848] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:10,047 [er for task 848] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:10,047 [er for task 848] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:10,049 [er for task 848] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1272.0 (TID 848). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:39:10,049 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1272.0 (TID 848) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:10,049 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1272.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:10,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1272 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:39:10,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 424 finished: print at Transform.scala:36, took 0.006661 s
   [druid] 2019-06-11 09:39:10,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217150000 ms.0 from job set of time 1560217150000 ms
   [druid] 2019-06-11 09:39:10,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.050 s for time 1560217150000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:39:10,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1478 from persistence list
   [druid] 2019-06-11 09:39:10,050 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1478
   [druid] 2019-06-11 09:39:10,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1473 from persistence list
   [druid] 2019-06-11 09:39:10,050 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1473
   [druid] 2019-06-11 09:39:10,050 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1472 from persistence list
   [druid] 2019-06-11 09:39:10,050 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1472
   [druid] 2019-06-11 09:39:10,050 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1472] at socketTextStream at Transform.scala:18 of time 1560217150000 ms
   [druid] 2019-06-11 09:39:10,050 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217140000 ms
   [druid] 2019-06-11 09:39:10,050 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217140000 ms
   [druid] 2019-06-11 09:39:15,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217155000 ms
   [druid] 2019-06-11 09:39:15,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217155000 ms.0 from job set of time 1560217155000 ms
   [druid] 2019-06-11 09:39:15,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1487 (map at Transform.scala:20)
   [druid] 2019-06-11 09:39:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:39:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 425 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1275 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1274, ShuffleMapStage 1273)
   [druid] 2019-06-11 09:39:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1274)
   [druid] 2019-06-11 09:39:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1274 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:39:15,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_637 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:15,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_637_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:39:15,013 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_637_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:15,013 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 637 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:15,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1274 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:39:15,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1274.0 with 2 tasks
   [druid] 2019-06-11 09:39:15,014 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1274.0 (TID 849, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:15,014 [er for task 849] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1274.0 (TID 849)
   [druid] 2019-06-11 09:39:15,028 [er for task 849] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1274.0 (TID 849). 853 bytes result sent to driver
   [druid] 2019-06-11 09:39:15,028 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1274.0 (TID 850, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:15,028 [er for task 850] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1274.0 (TID 850)
   [druid] 2019-06-11 09:39:15,028 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1274.0 (TID 849) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:39:15,040 [er for task 850] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1274.0 (TID 850). 853 bytes result sent to driver
   [druid] 2019-06-11 09:39:15,040 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1274.0 (TID 850) in 12 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:39:15,040 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1274.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1274 (parallelize at Transform.scala:16) finished in 0.026 s
   [druid] 2019-06-11 09:39:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:39:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:39:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1275)
   [druid] 2019-06-11 09:39:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:39:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1275 (MapPartitionsRDD[1492] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:15,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_638 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:15,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_638_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:15,042 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_638_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:15,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 638 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1275 (MapPartitionsRDD[1492] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:39:15,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1275.0 with 1 tasks
   [druid] 2019-06-11 09:39:15,043 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1275.0 (TID 851, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:15,043 [er for task 851] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1275.0 (TID 851)
   [druid] 2019-06-11 09:39:15,044 [er for task 851] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:15,044 [er for task 851] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:15,044 [er for task 851] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:15,044 [er for task 851] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:15,045 [er for task 851] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1275.0 (TID 851). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:15,045 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1275.0 (TID 851) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:15,045 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1275.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:15,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1275 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:39:15,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 425 finished: print at Transform.scala:36, took 0.035650 s
   [druid] 2019-06-11 09:39:15,049 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:15,049 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 424 is 83 bytes
   [druid] 2019-06-11 09:39:15,049 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 425 is 160 bytes
   [druid] 2019-06-11 09:39:15,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 426 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:15,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1278 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:15,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1276, ShuffleMapStage 1277)
   [druid] 2019-06-11 09:39:15,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:39:15,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1278 (MapPartitionsRDD[1492] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:15,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_639 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:15,051 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_639_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:15,051 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_639_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:15,052 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 639 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:15,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1278 (MapPartitionsRDD[1492] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:39:15,052 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1278.0 with 1 tasks
   [druid] 2019-06-11 09:39:15,052 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1278.0 (TID 852, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:15,052 [er for task 852] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1278.0 (TID 852)
   [druid] 2019-06-11 09:39:15,053 [er for task 852] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:15,053 [er for task 852] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:15,053 [er for task 852] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:15,053 [er for task 852] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:15,054 [er for task 852] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1278.0 (TID 852). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:15,055 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1278.0 (TID 852) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:15,055 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1278.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:15,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1278 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:39:15,055 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 426 finished: print at Transform.scala:36, took 0.005982 s
   [druid] 2019-06-11 09:39:15,055 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217155000 ms.0 from job set of time 1560217155000 ms
   [druid] 2019-06-11 09:39:15,055 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.055 s for time 1560217155000 ms (execution: 0.048 s)
   [druid] 2019-06-11 09:39:15,055 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1485 from persistence list
   [druid] 2019-06-11 09:39:15,055 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1485
   [druid] 2019-06-11 09:39:15,055 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1480 from persistence list
   [druid] 2019-06-11 09:39:15,056 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1480
   [druid] 2019-06-11 09:39:15,056 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1479 from persistence list
   [druid] 2019-06-11 09:39:15,056 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1479
   [druid] 2019-06-11 09:39:15,056 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1479] at socketTextStream at Transform.scala:18 of time 1560217155000 ms
   [druid] 2019-06-11 09:39:15,056 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217145000 ms
   [druid] 2019-06-11 09:39:15,056 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217145000 ms
   [druid] 2019-06-11 09:39:20,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217160000 ms
   [druid] 2019-06-11 09:39:20,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217160000 ms.0 from job set of time 1560217160000 ms
   [druid] 2019-06-11 09:39:20,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1494 (map at Transform.scala:20)
   [druid] 2019-06-11 09:39:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:39:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 427 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1281 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1279, ShuffleMapStage 1280)
   [druid] 2019-06-11 09:39:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1280)
   [druid] 2019-06-11 09:39:20,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1280 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:39:20,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_640 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:20,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_640_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:39:20,011 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_640_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:20,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 640 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1280 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:39:20,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1280.0 with 2 tasks
   [druid] 2019-06-11 09:39:20,012 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1280.0 (TID 853, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:20,012 [er for task 853] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1280.0 (TID 853)
   [druid] 2019-06-11 09:39:20,023 [er for task 853] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1280.0 (TID 853). 810 bytes result sent to driver
   [druid] 2019-06-11 09:39:20,023 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1280.0 (TID 854, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:20,023 [er for task 854] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1280.0 (TID 854)
   [druid] 2019-06-11 09:39:20,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1280.0 (TID 853) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:39:20,035 [er for task 854] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1280.0 (TID 854). 810 bytes result sent to driver
   [druid] 2019-06-11 09:39:20,035 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1280.0 (TID 854) in 12 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:39:20,035 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1280.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1280 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:39:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:39:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:39:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1281)
   [druid] 2019-06-11 09:39:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:39:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1281 (MapPartitionsRDD[1499] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:20,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_641 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:20,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_641_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:20,037 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_641_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:20,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 641 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:20,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1281 (MapPartitionsRDD[1499] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:39:20,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1281.0 with 1 tasks
   [druid] 2019-06-11 09:39:20,038 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1281.0 (TID 855, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:20,038 [er for task 855] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1281.0 (TID 855)
   [druid] 2019-06-11 09:39:20,039 [er for task 855] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:20,039 [er for task 855] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:20,039 [er for task 855] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:20,039 [er for task 855] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:20,040 [er for task 855] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1281.0 (TID 855). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:20,041 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1281.0 (TID 855) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:20,041 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1281.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:20,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1281 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:39:20,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 427 finished: print at Transform.scala:36, took 0.032274 s
   [druid] 2019-06-11 09:39:20,044 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:20,045 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 426 is 83 bytes
   [druid] 2019-06-11 09:39:20,045 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 427 is 160 bytes
   [druid] 2019-06-11 09:39:20,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 428 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:20,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1284 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:20,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1283, ShuffleMapStage 1282)
   [druid] 2019-06-11 09:39:20,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:39:20,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1284 (MapPartitionsRDD[1499] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:20,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_642 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:20,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_642_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:20,047 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_642_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:20,048 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 642 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:20,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1284 (MapPartitionsRDD[1499] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:39:20,048 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1284.0 with 1 tasks
   [druid] 2019-06-11 09:39:20,048 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1284.0 (TID 856, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:20,048 [er for task 856] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1284.0 (TID 856)
   [druid] 2019-06-11 09:39:20,049 [er for task 856] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:20,049 [er for task 856] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:20,049 [er for task 856] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:20,050 [er for task 856] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:39:20,050 [er for task 856] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1284.0 (TID 856). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:39:20,051 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1284.0 (TID 856) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:20,051 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1284.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:20,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1284 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:39:20,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 428 finished: print at Transform.scala:36, took 0.006466 s
   [druid] 2019-06-11 09:39:20,051 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217160000 ms.0 from job set of time 1560217160000 ms
   [druid] 2019-06-11 09:39:20,051 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.051 s for time 1560217160000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:39:20,051 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1492 from persistence list
   [druid] 2019-06-11 09:39:20,051 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1492
   [druid] 2019-06-11 09:39:20,051 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1487 from persistence list
   [druid] 2019-06-11 09:39:20,052 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1486 from persistence list
   [druid] 2019-06-11 09:39:20,052 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1487
   [druid] 2019-06-11 09:39:20,052 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1486
   [druid] 2019-06-11 09:39:20,052 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1486] at socketTextStream at Transform.scala:18 of time 1560217160000 ms
   [druid] 2019-06-11 09:39:20,052 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217150000 ms
   [druid] 2019-06-11 09:39:20,052 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217150000 ms
   [druid] 2019-06-11 09:39:25,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217165000 ms
   [druid] 2019-06-11 09:39:25,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217165000 ms.0 from job set of time 1560217165000 ms
   [druid] 2019-06-11 09:39:25,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:25,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:39:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1501 (map at Transform.scala:20)
   [druid] 2019-06-11 09:39:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 429 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1287 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1286, ShuffleMapStage 1285)
   [druid] 2019-06-11 09:39:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1285)
   [druid] 2019-06-11 09:39:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1285 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:39:25,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_643 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:25,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_643_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:39:25,011 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_643_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 643 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:25,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1285 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:39:25,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1285.0 with 2 tasks
   [druid] 2019-06-11 09:39:25,012 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1285.0 (TID 857, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:25,012 [er for task 857] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1285.0 (TID 857)
   [druid] 2019-06-11 09:39:25,028 [er for task 857] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1285.0 (TID 857). 810 bytes result sent to driver
   [druid] 2019-06-11 09:39:25,028 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1285.0 (TID 858, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:25,028 [er for task 858] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1285.0 (TID 858)
   [druid] 2019-06-11 09:39:25,028 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1285.0 (TID 857) in 16 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:39:25,038 [er for task 858] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1285.0 (TID 858). 853 bytes result sent to driver
   [druid] 2019-06-11 09:39:25,038 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1285.0 (TID 858) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:39:25,039 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1285.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:25,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1285 (parallelize at Transform.scala:16) finished in 0.027 s
   [druid] 2019-06-11 09:39:25,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:39:25,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:39:25,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1287)
   [druid] 2019-06-11 09:39:25,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:39:25,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1287 (MapPartitionsRDD[1506] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:25,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_644 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:25,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_644_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:25,041 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_644_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 644 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:25,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1287 (MapPartitionsRDD[1506] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:39:25,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1287.0 with 1 tasks
   [druid] 2019-06-11 09:39:25,042 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1287.0 (TID 859, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:25,042 [er for task 859] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1287.0 (TID 859)
   [druid] 2019-06-11 09:39:25,043 [er for task 859] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:25,043 [er for task 859] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:25,043 [er for task 859] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:25,043 [er for task 859] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:25,044 [er for task 859] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1287.0 (TID 859). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:39:25,044 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1287.0 (TID 859) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:25,044 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1287.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:25,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1287 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:39:25,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 429 finished: print at Transform.scala:36, took 0.035863 s
   [druid] 2019-06-11 09:39:25,047 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:25,048 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 429 is 160 bytes
   [druid] 2019-06-11 09:39:25,048 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 428 is 83 bytes
   [druid] 2019-06-11 09:39:25,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 430 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:25,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1290 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:25,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1289, ShuffleMapStage 1288)
   [druid] 2019-06-11 09:39:25,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:39:25,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1290 (MapPartitionsRDD[1506] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:25,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_645 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:25,055 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_645_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:25,056 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_633_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,056 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_645_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,056 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 645 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:25,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1290 (MapPartitionsRDD[1506] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:39:25,056 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1290.0 with 1 tasks
   [druid] 2019-06-11 09:39:25,056 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_643_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,056 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1290.0 (TID 860, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:25,057 [er for task 860] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1290.0 (TID 860)
   [druid] 2019-06-11 09:39:25,057 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 418
   [druid] 2019-06-11 09:39:25,057 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_636_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,058 [er for task 860] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:25,058 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_637_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,058 [er for task 860] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:25,058 [er for task 860] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:25,058 [er for task 860] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:25,058 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 419
   [druid] 2019-06-11 09:39:25,059 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_642_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,060 [er for task 860] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1290.0 (TID 860). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:39:25,060 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1290.0 (TID 860) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:25,060 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1290.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:25,060 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1290 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:39:25,060 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 430 finished: print at Transform.scala:36, took 0.012502 s
   [druid] 2019-06-11 09:39:25,060 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_634_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,060 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217165000 ms.0 from job set of time 1560217165000 ms
   [druid] 2019-06-11 09:39:25,061 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.060 s for time 1560217165000 ms (execution: 0.053 s)
   [druid] 2019-06-11 09:39:25,061 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1499 from persistence list
   [druid] 2019-06-11 09:39:25,061 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1499
   [druid] 2019-06-11 09:39:25,061 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1494 from persistence list
   [druid] 2019-06-11 09:39:25,061 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1494
   [druid] 2019-06-11 09:39:25,061 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1493 from persistence list
   [druid] 2019-06-11 09:39:25,061 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_635_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,061 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1493
   [druid] 2019-06-11 09:39:25,061 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1493] at socketTextStream at Transform.scala:18 of time 1560217165000 ms
   [druid] 2019-06-11 09:39:25,061 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 421
   [druid] 2019-06-11 09:39:25,062 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217155000 ms
   [druid] 2019-06-11 09:39:25,062 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217155000 ms
   [druid] 2019-06-11 09:39:25,062 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 424
   [druid] 2019-06-11 09:39:25,062 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 422
   [druid] 2019-06-11 09:39:25,063 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_641_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,063 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 420
   [druid] 2019-06-11 09:39:25,064 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_632_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,065 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 425
   [druid] 2019-06-11 09:39:25,066 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_638_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,067 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_644_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,068 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 423
   [druid] 2019-06-11 09:39:25,069 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_639_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,070 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_631_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:25,071 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_640_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:30,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217170000 ms
   [druid] 2019-06-11 09:39:30,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217170000 ms.0 from job set of time 1560217170000 ms
   [druid] 2019-06-11 09:39:30,011 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:39:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1508 (map at Transform.scala:20)
   [druid] 2019-06-11 09:39:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 431 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1293 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1291, ShuffleMapStage 1292)
   [druid] 2019-06-11 09:39:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1291)
   [druid] 2019-06-11 09:39:30,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1291 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:39:30,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_646 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:30,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_646_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:39:30,014 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_646_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:30,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 646 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:30,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1291 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:39:30,014 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1291.0 with 2 tasks
   [druid] 2019-06-11 09:39:30,015 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1291.0 (TID 861, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:30,015 [er for task 861] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1291.0 (TID 861)
   [druid] 2019-06-11 09:39:30,026 [er for task 861] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1291.0 (TID 861). 853 bytes result sent to driver
   [druid] 2019-06-11 09:39:30,027 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1291.0 (TID 862, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:30,027 [er for task 862] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1291.0 (TID 862)
   [druid] 2019-06-11 09:39:30,027 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1291.0 (TID 861) in 12 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:39:30,038 [er for task 862] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1291.0 (TID 862). 853 bytes result sent to driver
   [druid] 2019-06-11 09:39:30,038 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1291.0 (TID 862) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:39:30,038 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1291.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1291 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:39:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:39:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:39:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1293)
   [druid] 2019-06-11 09:39:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:39:30,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1293 (MapPartitionsRDD[1513] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:30,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_647 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:30,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_647_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:30,041 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_647_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:30,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 647 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:30,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1293 (MapPartitionsRDD[1513] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:39:30,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1293.0 with 1 tasks
   [druid] 2019-06-11 09:39:30,041 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1293.0 (TID 863, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:30,041 [er for task 863] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1293.0 (TID 863)
   [druid] 2019-06-11 09:39:30,042 [er for task 863] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:30,042 [er for task 863] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:30,042 [er for task 863] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:30,042 [er for task 863] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:30,043 [er for task 863] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1293.0 (TID 863). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:39:30,043 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1293.0 (TID 863) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:30,043 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1293.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1293 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:39:30,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 431 finished: print at Transform.scala:36, took 0.032785 s
   [druid] 2019-06-11 09:39:30,047 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:30,047 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 431 is 160 bytes
   [druid] 2019-06-11 09:39:30,047 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 430 is 83 bytes
   [druid] 2019-06-11 09:39:30,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 432 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1296 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1294, ShuffleMapStage 1295)
   [druid] 2019-06-11 09:39:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:39:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1296 (MapPartitionsRDD[1513] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:30,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_648 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:30,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_648_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:30,050 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_648_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:30,050 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 648 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:30,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1296 (MapPartitionsRDD[1513] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:39:30,050 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1296.0 with 1 tasks
   [druid] 2019-06-11 09:39:30,050 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1296.0 (TID 864, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:30,051 [er for task 864] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1296.0 (TID 864)
   [druid] 2019-06-11 09:39:30,052 [er for task 864] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:30,052 [er for task 864] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:30,052 [er for task 864] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:30,052 [er for task 864] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:30,053 [er for task 864] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1296.0 (TID 864). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:30,053 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1296.0 (TID 864) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:30,053 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1296.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:30,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1296 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:39:30,053 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 432 finished: print at Transform.scala:36, took 0.006317 s
   [druid] 2019-06-11 09:39:30,053 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217170000 ms.0 from job set of time 1560217170000 ms
   [druid] 2019-06-11 09:39:30,053 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.053 s for time 1560217170000 ms (execution: 0.045 s)
   [druid] 2019-06-11 09:39:30,054 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1506 from persistence list
   [druid] 2019-06-11 09:39:30,054 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1506
   [druid] 2019-06-11 09:39:30,054 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1501 from persistence list
   [druid] 2019-06-11 09:39:30,054 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1501
   [druid] 2019-06-11 09:39:30,054 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1500 from persistence list
   [druid] 2019-06-11 09:39:30,054 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1500
   [druid] 2019-06-11 09:39:30,054 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1500] at socketTextStream at Transform.scala:18 of time 1560217170000 ms
   [druid] 2019-06-11 09:39:30,054 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217160000 ms
   [druid] 2019-06-11 09:39:30,054 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217160000 ms
   [druid] 2019-06-11 09:39:35,011 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217175000 ms
   [druid] 2019-06-11 09:39:35,013 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217175000 ms.0 from job set of time 1560217175000 ms
   [druid] 2019-06-11 09:39:35,019 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:35,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:39:35,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1515 (map at Transform.scala:20)
   [druid] 2019-06-11 09:39:35,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 433 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:35,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1299 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:35,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1298, ShuffleMapStage 1297)
   [druid] 2019-06-11 09:39:35,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1297)
   [druid] 2019-06-11 09:39:35,019 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1297 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:39:35,020 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_649 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:35,022 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_649_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:39:35,022 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_649_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:35,023 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 649 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:35,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1297 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:39:35,023 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1297.0 with 2 tasks
   [druid] 2019-06-11 09:39:35,023 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1297.0 (TID 865, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:35,023 [er for task 865] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1297.0 (TID 865)
   [druid] 2019-06-11 09:39:35,041 [er for task 865] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1297.0 (TID 865). 853 bytes result sent to driver
   [druid] 2019-06-11 09:39:35,042 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1297.0 (TID 866, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:35,042 [er for task 866] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1297.0 (TID 866)
   [druid] 2019-06-11 09:39:35,042 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1297.0 (TID 865) in 19 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:39:35,063 [er for task 866] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1297.0 (TID 866). 853 bytes result sent to driver
   [druid] 2019-06-11 09:39:35,063 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1297.0 (TID 866) in 22 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:39:35,063 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1297.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:35,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1297 (parallelize at Transform.scala:16) finished in 0.040 s
   [druid] 2019-06-11 09:39:35,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:39:35,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:39:35,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1299)
   [druid] 2019-06-11 09:39:35,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:39:35,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1299 (MapPartitionsRDD[1520] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:35,065 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_650 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:35,067 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_650_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:35,070 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_650_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:35,070 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 650 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:35,071 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1299 (MapPartitionsRDD[1520] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:39:35,071 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1299.0 with 1 tasks
   [druid] 2019-06-11 09:39:35,071 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1299.0 (TID 867, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:35,071 [er for task 867] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1299.0 (TID 867)
   [druid] 2019-06-11 09:39:35,073 [er for task 867] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:35,073 [er for task 867] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:35,073 [er for task 867] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:35,073 [er for task 867] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:35,074 [er for task 867] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1299.0 (TID 867). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:35,075 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1299.0 (TID 867) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:35,075 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1299.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:35,076 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1299 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:39:35,084 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 433 finished: print at Transform.scala:36, took 0.064788 s
   [druid] 2019-06-11 09:39:35,091 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:35,091 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 433 is 160 bytes
   [druid] 2019-06-11 09:39:35,092 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 432 is 83 bytes
   [druid] 2019-06-11 09:39:35,092 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 434 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:35,092 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1302 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:35,092 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1301, ShuffleMapStage 1300)
   [druid] 2019-06-11 09:39:35,092 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:39:35,092 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1302 (MapPartitionsRDD[1520] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:35,093 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_651 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:35,096 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_651_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:35,097 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_651_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:35,097 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 651 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:35,098 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1302 (MapPartitionsRDD[1520] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:39:35,098 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1302.0 with 1 tasks
   [druid] 2019-06-11 09:39:35,099 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1302.0 (TID 868, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:35,099 [er for task 868] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1302.0 (TID 868)
   [druid] 2019-06-11 09:39:35,100 [er for task 868] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:35,100 [er for task 868] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:35,101 [er for task 868] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:35,101 [er for task 868] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:35,102 [er for task 868] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1302.0 (TID 868). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:35,102 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1302.0 (TID 868) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:35,102 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1302.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:35,102 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1302 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:39:35,102 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 434 finished: print at Transform.scala:36, took 0.012061 s
   [druid] 2019-06-11 09:39:35,103 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217175000 ms.0 from job set of time 1560217175000 ms
   [druid] 2019-06-11 09:39:35,103 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.103 s for time 1560217175000 ms (execution: 0.090 s)
   [druid] 2019-06-11 09:39:35,103 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1513 from persistence list
   [druid] 2019-06-11 09:39:35,103 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1513
   [druid] 2019-06-11 09:39:35,103 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1508 from persistence list
   [druid] 2019-06-11 09:39:35,103 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1508
   [druid] 2019-06-11 09:39:35,103 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1507 from persistence list
   [druid] 2019-06-11 09:39:35,103 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1507
   [druid] 2019-06-11 09:39:35,103 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1507] at socketTextStream at Transform.scala:18 of time 1560217175000 ms
   [druid] 2019-06-11 09:39:35,103 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217165000 ms
   [druid] 2019-06-11 09:39:35,103 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217165000 ms
   [druid] 2019-06-11 09:39:40,011 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217180000 ms
   [druid] 2019-06-11 09:39:40,011 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217180000 ms.0 from job set of time 1560217180000 ms
   [druid] 2019-06-11 09:39:40,014 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:40,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:39:40,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1522 (map at Transform.scala:20)
   [druid] 2019-06-11 09:39:40,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 435 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:40,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1305 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:40,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1304, ShuffleMapStage 1303)
   [druid] 2019-06-11 09:39:40,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1303)
   [druid] 2019-06-11 09:39:40,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1303 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:39:40,016 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_652 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:40,019 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_652_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:39:40,020 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_652_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:40,021 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 652 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:40,021 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1303 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:39:40,021 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1303.0 with 2 tasks
   [druid] 2019-06-11 09:39:40,021 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1303.0 (TID 869, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:40,022 [er for task 869] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1303.0 (TID 869)
   [druid] 2019-06-11 09:39:40,045 [er for task 869] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1303.0 (TID 869). 810 bytes result sent to driver
   [druid] 2019-06-11 09:39:40,045 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1303.0 (TID 870, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:40,045 [er for task 870] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1303.0 (TID 870)
   [druid] 2019-06-11 09:39:40,045 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1303.0 (TID 869) in 24 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:39:40,065 [er for task 870] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1303.0 (TID 870). 896 bytes result sent to driver
   [druid] 2019-06-11 09:39:40,065 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1303.0 (TID 870) in 20 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:39:40,066 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1303.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:40,066 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1303 (parallelize at Transform.scala:16) finished in 0.045 s
   [druid] 2019-06-11 09:39:40,066 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:39:40,066 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:39:40,066 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1305)
   [druid] 2019-06-11 09:39:40,066 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:39:40,066 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1305 (MapPartitionsRDD[1527] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:40,067 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_653 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:40,072 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_653_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:40,073 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_653_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:40,073 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 653 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:40,073 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1305 (MapPartitionsRDD[1527] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:39:40,074 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1305.0 with 1 tasks
   [druid] 2019-06-11 09:39:40,074 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1305.0 (TID 871, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:40,074 [er for task 871] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1305.0 (TID 871)
   [druid] 2019-06-11 09:39:40,077 [er for task 871] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:40,077 [er for task 871] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:40,077 [er for task 871] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:40,077 [er for task 871] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:40,078 [er for task 871] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1305.0 (TID 871). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:39:40,079 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1305.0 (TID 871) in 5 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:40,079 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1305.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:40,079 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1305 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:39:40,079 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 435 finished: print at Transform.scala:36, took 0.065408 s
   [druid] 2019-06-11 09:39:40,086 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:40,089 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 435 is 160 bytes
   [druid] 2019-06-11 09:39:40,090 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 434 is 83 bytes
   [druid] 2019-06-11 09:39:40,090 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 436 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:40,090 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1308 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:40,090 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1307, ShuffleMapStage 1306)
   [druid] 2019-06-11 09:39:40,090 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:39:40,090 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1308 (MapPartitionsRDD[1527] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:40,091 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_654 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:40,092 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_654_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:40,093 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_654_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:40,093 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 654 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:40,094 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1308 (MapPartitionsRDD[1527] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:39:40,094 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1308.0 with 1 tasks
   [druid] 2019-06-11 09:39:40,094 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1308.0 (TID 872, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:40,094 [er for task 872] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1308.0 (TID 872)
   [druid] 2019-06-11 09:39:40,096 [er for task 872] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:40,096 [er for task 872] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:40,096 [er for task 872] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:40,096 [er for task 872] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:40,097 [er for task 872] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1308.0 (TID 872). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:39:40,097 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1308.0 (TID 872) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:40,097 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1308.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:40,097 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1308 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:39:40,098 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 436 finished: print at Transform.scala:36, took 0.011222 s
   [druid] 2019-06-11 09:39:40,098 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217180000 ms.0 from job set of time 1560217180000 ms
   [druid] 2019-06-11 09:39:40,098 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.098 s for time 1560217180000 ms (execution: 0.087 s)
   [druid] 2019-06-11 09:39:40,098 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1520 from persistence list
   [druid] 2019-06-11 09:39:40,098 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1520
   [druid] 2019-06-11 09:39:40,098 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1515 from persistence list
   [druid] 2019-06-11 09:39:40,098 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1515
   [druid] 2019-06-11 09:39:40,098 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1514 from persistence list
   [druid] 2019-06-11 09:39:40,098 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1514
   [druid] 2019-06-11 09:39:40,098 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1514] at socketTextStream at Transform.scala:18 of time 1560217180000 ms
   [druid] 2019-06-11 09:39:40,098 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217170000 ms
   [druid] 2019-06-11 09:39:40,099 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217170000 ms
   [druid] 2019-06-11 09:39:45,004 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217185000 ms
   [druid] 2019-06-11 09:39:45,004 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217185000 ms.0 from job set of time 1560217185000 ms
   [druid] 2019-06-11 09:39:45,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:39:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1529 (map at Transform.scala:20)
   [druid] 2019-06-11 09:39:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 437 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1311 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1309, ShuffleMapStage 1310)
   [druid] 2019-06-11 09:39:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1309)
   [druid] 2019-06-11 09:39:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1309 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:39:45,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_655 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:45,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_655_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:39:45,011 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_655_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:45,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 655 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:45,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1309 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:39:45,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1309.0 with 2 tasks
   [druid] 2019-06-11 09:39:45,012 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1309.0 (TID 873, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:45,012 [er for task 873] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1309.0 (TID 873)
   [druid] 2019-06-11 09:39:45,022 [er for task 873] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1309.0 (TID 873). 810 bytes result sent to driver
   [druid] 2019-06-11 09:39:45,023 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1309.0 (TID 874, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:45,023 [er for task 874] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1309.0 (TID 874)
   [druid] 2019-06-11 09:39:45,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1309.0 (TID 873) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:39:45,033 [er for task 874] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1309.0 (TID 874). 853 bytes result sent to driver
   [druid] 2019-06-11 09:39:45,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1309.0 (TID 874) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:39:45,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1309.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1309 (parallelize at Transform.scala:16) finished in 0.021 s
   [druid] 2019-06-11 09:39:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:39:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:39:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1311)
   [druid] 2019-06-11 09:39:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:39:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1311 (MapPartitionsRDD[1534] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:45,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_656 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:45,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_656_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:45,035 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_656_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:45,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 656 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:45,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1311 (MapPartitionsRDD[1534] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:39:45,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1311.0 with 1 tasks
   [druid] 2019-06-11 09:39:45,036 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1311.0 (TID 875, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:45,036 [er for task 875] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1311.0 (TID 875)
   [druid] 2019-06-11 09:39:45,037 [er for task 875] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:45,037 [er for task 875] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:45,037 [er for task 875] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:45,037 [er for task 875] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:45,038 [er for task 875] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1311.0 (TID 875). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:45,038 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1311.0 (TID 875) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:45,038 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1311.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:45,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1311 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:39:45,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 437 finished: print at Transform.scala:36, took 0.030884 s
   [druid] 2019-06-11 09:39:45,041 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:45,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 437 is 160 bytes
   [druid] 2019-06-11 09:39:45,041 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 436 is 83 bytes
   [druid] 2019-06-11 09:39:45,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 438 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:45,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1314 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:45,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1312, ShuffleMapStage 1313)
   [druid] 2019-06-11 09:39:45,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:39:45,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1314 (MapPartitionsRDD[1534] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:45,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_657 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:45,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_657_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:39:45,043 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_657_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:45,043 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 657 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1314 (MapPartitionsRDD[1534] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:39:45,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1314.0 with 1 tasks
   [druid] 2019-06-11 09:39:45,043 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1314.0 (TID 876, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:45,044 [er for task 876] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1314.0 (TID 876)
   [druid] 2019-06-11 09:39:45,045 [er for task 876] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:45,045 [er for task 876] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:39:45,045 [er for task 876] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:45,045 [er for task 876] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:45,046 [er for task 876] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1314.0 (TID 876). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:45,046 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1314.0 (TID 876) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:45,046 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1314.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:45,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1314 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:39:45,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 438 finished: print at Transform.scala:36, took 0.005666 s
   [druid] 2019-06-11 09:39:45,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217185000 ms.0 from job set of time 1560217185000 ms
   [druid] 2019-06-11 09:39:45,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.047 s for time 1560217185000 ms (execution: 0.043 s)
   [druid] 2019-06-11 09:39:45,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1527 from persistence list
   [druid] 2019-06-11 09:39:45,047 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1527
   [druid] 2019-06-11 09:39:45,047 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1522 from persistence list
   [druid] 2019-06-11 09:39:45,047 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1522
   [druid] 2019-06-11 09:39:45,047 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1521 from persistence list
   [druid] 2019-06-11 09:39:45,047 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1521] at socketTextStream at Transform.scala:18 of time 1560217185000 ms
   [druid] 2019-06-11 09:39:45,047 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1521
   [druid] 2019-06-11 09:39:45,048 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217175000 ms
   [druid] 2019-06-11 09:39:45,048 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217175000 ms
   [druid] 2019-06-11 09:39:50,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217190000 ms
   [druid] 2019-06-11 09:39:50,013 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217190000 ms.0 from job set of time 1560217190000 ms
   [druid] 2019-06-11 09:39:50,014 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_648_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,014 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_654_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,015 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 428
   [druid] 2019-06-11 09:39:50,015 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 431
   [druid] 2019-06-11 09:39:50,015 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 430
   [druid] 2019-06-11 09:39:50,016 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 427
   [druid] 2019-06-11 09:39:50,016 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_655_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,017 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:50,017 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_652_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1536 (map at Transform.scala:20)
   [druid] 2019-06-11 09:39:50,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:39:50,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 439 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:50,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1317 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:50,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1316, ShuffleMapStage 1315)
   [druid] 2019-06-11 09:39:50,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1316)
   [druid] 2019-06-11 09:39:50,018 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_657_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,018 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1316 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:39:50,018 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_658 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:50,019 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_651_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,020 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_658_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:39:50,020 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_646_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,020 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_658_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,020 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 658 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:50,021 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 429
   [druid] 2019-06-11 09:39:50,021 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1316 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:39:50,021 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1316.0 with 2 tasks
   [druid] 2019-06-11 09:39:50,021 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_650_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,021 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1316.0 (TID 877, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:50,021 [er for task 877] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1316.0 (TID 877)
   [druid] 2019-06-11 09:39:50,022 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 434
   [druid] 2019-06-11 09:39:50,022 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_653_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,023 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 426
   [druid] 2019-06-11 09:39:50,023 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_656_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,024 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_645_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,025 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_647_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,025 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_649_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,026 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 435
   [druid] 2019-06-11 09:39:50,026 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 432
   [druid] 2019-06-11 09:39:50,026 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 433
   [druid] 2019-06-11 09:39:50,034 [er for task 877] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1316.0 (TID 877). 853 bytes result sent to driver
   [druid] 2019-06-11 09:39:50,034 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1316.0 (TID 878, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:50,034 [er for task 878] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1316.0 (TID 878)
   [druid] 2019-06-11 09:39:50,034 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1316.0 (TID 877) in 13 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:39:50,044 [er for task 878] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1316.0 (TID 878). 810 bytes result sent to driver
   [druid] 2019-06-11 09:39:50,044 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1316.0 (TID 878) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:39:50,044 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1316.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:50,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1316 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:39:50,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:39:50,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:39:50,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1317)
   [druid] 2019-06-11 09:39:50,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:39:50,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1317 (MapPartitionsRDD[1541] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:50,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_659 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:50,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_659_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:50,047 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_659_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,048 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 659 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:50,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1317 (MapPartitionsRDD[1541] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:39:50,048 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1317.0 with 1 tasks
   [druid] 2019-06-11 09:39:50,048 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1317.0 (TID 879, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:50,048 [er for task 879] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1317.0 (TID 879)
   [druid] 2019-06-11 09:39:50,049 [er for task 879] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:50,050 [er for task 879] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:39:50,050 [er for task 879] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:50,050 [er for task 879] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:50,051 [er for task 879] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1317.0 (TID 879). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:50,051 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1317.0 (TID 879) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:50,051 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1317.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:50,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1317 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:39:50,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 439 finished: print at Transform.scala:36, took 0.034718 s
   [druid] 2019-06-11 09:39:50,056 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:50,056 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 438 is 83 bytes
   [druid] 2019-06-11 09:39:50,056 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 439 is 160 bytes
   [druid] 2019-06-11 09:39:50,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 440 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:50,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1320 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:50,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1319, ShuffleMapStage 1318)
   [druid] 2019-06-11 09:39:50,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:39:50,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1320 (MapPartitionsRDD[1541] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:50,057 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_660 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:50,058 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_660_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:50,058 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_660_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:50,059 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 660 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:50,059 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1320 (MapPartitionsRDD[1541] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:39:50,059 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1320.0 with 1 tasks
   [druid] 2019-06-11 09:39:50,059 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1320.0 (TID 880, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:50,059 [er for task 880] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1320.0 (TID 880)
   [druid] 2019-06-11 09:39:50,060 [er for task 880] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:50,060 [er for task 880] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:50,060 [er for task 880] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:50,060 [er for task 880] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:50,061 [er for task 880] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1320.0 (TID 880). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:50,061 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1320.0 (TID 880) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:50,062 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1320.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:50,062 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1320 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:39:50,062 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 440 finished: print at Transform.scala:36, took 0.005972 s
   [druid] 2019-06-11 09:39:50,062 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217190000 ms.0 from job set of time 1560217190000 ms
   [druid] 2019-06-11 09:39:50,062 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.062 s for time 1560217190000 ms (execution: 0.055 s)
   [druid] 2019-06-11 09:39:50,062 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1534 from persistence list
   [druid] 2019-06-11 09:39:50,062 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1534
   [druid] 2019-06-11 09:39:50,062 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1529 from persistence list
   [druid] 2019-06-11 09:39:50,062 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1529
   [druid] 2019-06-11 09:39:50,062 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1528 from persistence list
   [druid] 2019-06-11 09:39:50,063 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1528
   [druid] 2019-06-11 09:39:50,063 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1528] at socketTextStream at Transform.scala:18 of time 1560217190000 ms
   [druid] 2019-06-11 09:39:50,063 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217180000 ms
   [druid] 2019-06-11 09:39:50,063 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217180000 ms
   [druid] 2019-06-11 09:39:55,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217195000 ms
   [druid] 2019-06-11 09:39:55,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217195000 ms.0 from job set of time 1560217195000 ms
   [druid] 2019-06-11 09:39:55,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:39:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1543 (map at Transform.scala:20)
   [druid] 2019-06-11 09:39:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 441 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1323 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1322, ShuffleMapStage 1321)
   [druid] 2019-06-11 09:39:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1321)
   [druid] 2019-06-11 09:39:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1321 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:39:55,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_661 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:55,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_661_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:39:55,011 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_661_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:55,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 661 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:55,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1321 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:39:55,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1321.0 with 2 tasks
   [druid] 2019-06-11 09:39:55,012 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1321.0 (TID 881, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:55,012 [er for task 881] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1321.0 (TID 881)
   [druid] 2019-06-11 09:39:55,020 [er for task 881] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1321.0 (TID 881). 810 bytes result sent to driver
   [druid] 2019-06-11 09:39:55,020 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1321.0 (TID 882, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:39:55,021 [er for task 882] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1321.0 (TID 882)
   [druid] 2019-06-11 09:39:55,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1321.0 (TID 881) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:39:55,029 [er for task 882] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1321.0 (TID 882). 810 bytes result sent to driver
   [druid] 2019-06-11 09:39:55,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1321.0 (TID 882) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:39:55,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1321.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:55,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1321 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:39:55,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:39:55,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:39:55,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1323)
   [druid] 2019-06-11 09:39:55,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:39:55,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1323 (MapPartitionsRDD[1548] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:55,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_662 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:55,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_662_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:55,031 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_662_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:55,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 662 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:55,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1323 (MapPartitionsRDD[1548] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:39:55,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1323.0 with 1 tasks
   [druid] 2019-06-11 09:39:55,032 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1323.0 (TID 883, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:55,032 [er for task 883] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1323.0 (TID 883)
   [druid] 2019-06-11 09:39:55,032 [er for task 883] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:55,033 [er for task 883] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:39:55,033 [er for task 883] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:55,033 [er for task 883] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:55,033 [er for task 883] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1323.0 (TID 883). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:55,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1323.0 (TID 883) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:55,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1323.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:55,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1323 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:39:55,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 441 finished: print at Transform.scala:36, took 0.024826 s
   [druid] 2019-06-11 09:39:55,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:39:55,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 441 is 160 bytes
   [druid] 2019-06-11 09:39:55,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 440 is 83 bytes
   [druid] 2019-06-11 09:39:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 442 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:39:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1326 (print at Transform.scala:36)
   [druid] 2019-06-11 09:39:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1325, ShuffleMapStage 1324)
   [druid] 2019-06-11 09:39:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:39:55,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1326 (MapPartitionsRDD[1548] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:39:55,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_663 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:55,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_663_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:39:55,039 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_663_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:39:55,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 663 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:39:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1326 (MapPartitionsRDD[1548] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:39:55,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1326.0 with 1 tasks
   [druid] 2019-06-11 09:39:55,039 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1326.0 (TID 884, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:39:55,040 [er for task 884] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1326.0 (TID 884)
   [druid] 2019-06-11 09:39:55,040 [er for task 884] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:39:55,040 [er for task 884] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:55,040 [er for task 884] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:39:55,040 [er for task 884] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:39:55,041 [er for task 884] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1326.0 (TID 884). 966 bytes result sent to driver
   [druid] 2019-06-11 09:39:55,046 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1326.0 (TID 884) in 7 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:39:55,046 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1326.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:39:55,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1326 (print at Transform.scala:36) finished in 0.007 s
   [druid] 2019-06-11 09:39:55,047 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 442 finished: print at Transform.scala:36, took 0.010001 s
   [druid] 2019-06-11 09:39:55,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217195000 ms.0 from job set of time 1560217195000 ms
   [druid] 2019-06-11 09:39:55,047 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.047 s for time 1560217195000 ms (execution: 0.040 s)
   [druid] 2019-06-11 09:39:55,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1541 from persistence list
   [druid] 2019-06-11 09:39:55,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1536 from persistence list
   [druid] 2019-06-11 09:39:55,048 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1541
   [druid] 2019-06-11 09:39:55,048 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1536
   [druid] 2019-06-11 09:39:55,048 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1535 from persistence list
   [druid] 2019-06-11 09:39:55,048 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1535
   [druid] 2019-06-11 09:39:55,048 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1535] at socketTextStream at Transform.scala:18 of time 1560217195000 ms
   [druid] 2019-06-11 09:39:55,048 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217185000 ms
   [druid] 2019-06-11 09:39:55,048 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217185000 ms
   [druid] 2019-06-11 09:40:00,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217200000 ms
   [druid] 2019-06-11 09:40:00,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217200000 ms.0 from job set of time 1560217200000 ms
   [druid] 2019-06-11 09:40:00,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1550 (map at Transform.scala:20)
   [druid] 2019-06-11 09:40:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:40:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 443 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1329 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1327, ShuffleMapStage 1328)
   [druid] 2019-06-11 09:40:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1328)
   [druid] 2019-06-11 09:40:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1328 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:40:00,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_664 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:00,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_664_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:40:00,012 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_664_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:00,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 664 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1328 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:40:00,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1328.0 with 2 tasks
   [druid] 2019-06-11 09:40:00,013 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1328.0 (TID 885, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:00,013 [er for task 885] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1328.0 (TID 885)
   [druid] 2019-06-11 09:40:00,023 [er for task 885] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1328.0 (TID 885). 853 bytes result sent to driver
   [druid] 2019-06-11 09:40:00,023 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1328.0 (TID 886, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:00,023 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1328.0 (TID 885) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:40:00,023 [er for task 886] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1328.0 (TID 886)
   [druid] 2019-06-11 09:40:00,030 [er for task 886] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1328.0 (TID 886). 810 bytes result sent to driver
   [druid] 2019-06-11 09:40:00,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1328.0 (TID 886) in 7 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:40:00,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1328.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1328 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:40:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:40:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:40:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1329)
   [druid] 2019-06-11 09:40:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:40:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1329 (MapPartitionsRDD[1555] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:00,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_665 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:00,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_665_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:00,033 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_665_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:00,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 665 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1329 (MapPartitionsRDD[1555] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:40:00,033 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1329.0 with 1 tasks
   [druid] 2019-06-11 09:40:00,034 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1329.0 (TID 887, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:00,034 [er for task 887] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1329.0 (TID 887)
   [druid] 2019-06-11 09:40:00,034 [er for task 887] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:00,034 [er for task 887] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:00,034 [er for task 887] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:00,034 [er for task 887] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:00,035 [er for task 887] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1329.0 (TID 887). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:00,035 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1329.0 (TID 887) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:00,035 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1329.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:00,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1329 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:00,035 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 443 finished: print at Transform.scala:36, took 0.025947 s
   [druid] 2019-06-11 09:40:00,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:00,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 442 is 83 bytes
   [druid] 2019-06-11 09:40:00,038 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 443 is 160 bytes
   [druid] 2019-06-11 09:40:00,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 444 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:00,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1332 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:00,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1330, ShuffleMapStage 1331)
   [druid] 2019-06-11 09:40:00,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:40:00,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1332 (MapPartitionsRDD[1555] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:00,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_666 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:00,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_666_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:00,039 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_666_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:00,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 666 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1332 (MapPartitionsRDD[1555] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:40:00,040 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1332.0 with 1 tasks
   [druid] 2019-06-11 09:40:00,040 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1332.0 (TID 888, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:00,040 [er for task 888] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1332.0 (TID 888)
   [druid] 2019-06-11 09:40:00,041 [er for task 888] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:00,041 [er for task 888] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:00,041 [er for task 888] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:00,041 [er for task 888] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:00,041 [er for task 888] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1332.0 (TID 888). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:00,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1332.0 (TID 888) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:00,042 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1332.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:00,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1332 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:00,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 444 finished: print at Transform.scala:36, took 0.004339 s
   [druid] 2019-06-11 09:40:00,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217200000 ms.0 from job set of time 1560217200000 ms
   [druid] 2019-06-11 09:40:00,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560217200000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:40:00,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1548 from persistence list
   [druid] 2019-06-11 09:40:00,042 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1548
   [druid] 2019-06-11 09:40:00,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1543 from persistence list
   [druid] 2019-06-11 09:40:00,042 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1543
   [druid] 2019-06-11 09:40:00,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1542 from persistence list
   [druid] 2019-06-11 09:40:00,043 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1542
   [druid] 2019-06-11 09:40:00,043 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1542] at socketTextStream at Transform.scala:18 of time 1560217200000 ms
   [druid] 2019-06-11 09:40:00,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217190000 ms
   [druid] 2019-06-11 09:40:00,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217190000 ms
   [druid] 2019-06-11 09:40:05,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217205000 ms
   [druid] 2019-06-11 09:40:05,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217205000 ms.0 from job set of time 1560217205000 ms
   [druid] 2019-06-11 09:40:05,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:40:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1557 (map at Transform.scala:20)
   [druid] 2019-06-11 09:40:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 445 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1335 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1334, ShuffleMapStage 1333)
   [druid] 2019-06-11 09:40:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1333)
   [druid] 2019-06-11 09:40:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1333 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:40:05,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_667 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:05,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_667_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:40:05,009 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_667_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:05,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 667 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1333 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:40:05,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1333.0 with 2 tasks
   [druid] 2019-06-11 09:40:05,010 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1333.0 (TID 889, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:05,010 [er for task 889] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1333.0 (TID 889)
   [druid] 2019-06-11 09:40:05,020 [er for task 889] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1333.0 (TID 889). 810 bytes result sent to driver
   [druid] 2019-06-11 09:40:05,020 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1333.0 (TID 890, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:05,020 [er for task 890] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1333.0 (TID 890)
   [druid] 2019-06-11 09:40:05,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1333.0 (TID 889) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:40:05,028 [er for task 890] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1333.0 (TID 890). 767 bytes result sent to driver
   [druid] 2019-06-11 09:40:05,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1333.0 (TID 890) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:40:05,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1333.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1333 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:40:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:40:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:40:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1335)
   [druid] 2019-06-11 09:40:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:40:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1335 (MapPartitionsRDD[1562] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:05,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_668 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:05,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_668_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:05,030 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_668_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:05,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 668 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:05,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1335 (MapPartitionsRDD[1562] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:40:05,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1335.0 with 1 tasks
   [druid] 2019-06-11 09:40:05,031 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1335.0 (TID 891, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:05,031 [er for task 891] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1335.0 (TID 891)
   [druid] 2019-06-11 09:40:05,031 [er for task 891] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:05,031 [er for task 891] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:05,031 [er for task 891] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:05,031 [er for task 891] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:05,032 [er for task 891] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1335.0 (TID 891). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:05,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1335.0 (TID 891) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:05,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1335.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:05,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1335 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:05,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 445 finished: print at Transform.scala:36, took 0.024998 s
   [druid] 2019-06-11 09:40:05,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:05,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 445 is 160 bytes
   [druid] 2019-06-11 09:40:05,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 444 is 83 bytes
   [druid] 2019-06-11 09:40:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 446 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1338 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1337, ShuffleMapStage 1336)
   [druid] 2019-06-11 09:40:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:40:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1338 (MapPartitionsRDD[1562] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:05,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_669 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:05,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_669_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:05,036 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_669_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:05,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 669 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:05,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1338 (MapPartitionsRDD[1562] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:40:05,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1338.0 with 1 tasks
   [druid] 2019-06-11 09:40:05,037 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1338.0 (TID 892, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:05,037 [er for task 892] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1338.0 (TID 892)
   [druid] 2019-06-11 09:40:05,038 [er for task 892] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:05,038 [er for task 892] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:05,038 [er for task 892] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:05,038 [er for task 892] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:05,039 [er for task 892] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1338.0 (TID 892). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:05,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1338.0 (TID 892) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:05,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1338.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1338 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:05,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 446 finished: print at Transform.scala:36, took 0.004960 s
   [druid] 2019-06-11 09:40:05,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217205000 ms.0 from job set of time 1560217205000 ms
   [druid] 2019-06-11 09:40:05,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560217205000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:40:05,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1555 from persistence list
   [druid] 2019-06-11 09:40:05,040 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1555
   [druid] 2019-06-11 09:40:05,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1550 from persistence list
   [druid] 2019-06-11 09:40:05,040 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1550
   [druid] 2019-06-11 09:40:05,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1549 from persistence list
   [druid] 2019-06-11 09:40:05,040 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1549
   [druid] 2019-06-11 09:40:05,040 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1549] at socketTextStream at Transform.scala:18 of time 1560217205000 ms
   [druid] 2019-06-11 09:40:05,040 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217195000 ms
   [druid] 2019-06-11 09:40:05,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217195000 ms
   [druid] 2019-06-11 09:40:10,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217210000 ms
   [druid] 2019-06-11 09:40:10,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217210000 ms.0 from job set of time 1560217210000 ms
   [druid] 2019-06-11 09:40:10,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1564 (map at Transform.scala:20)
   [druid] 2019-06-11 09:40:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:40:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 447 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1341 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1340, ShuffleMapStage 1339)
   [druid] 2019-06-11 09:40:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1340)
   [druid] 2019-06-11 09:40:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1340 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:40:10,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_670 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:10,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_670_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:40:10,010 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_670_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 670 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:10,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1340 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:40:10,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1340.0 with 2 tasks
   [druid] 2019-06-11 09:40:10,011 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1340.0 (TID 893, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:10,011 [er for task 893] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1340.0 (TID 893)
   [druid] 2019-06-11 09:40:10,020 [er for task 893] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1340.0 (TID 893). 853 bytes result sent to driver
   [druid] 2019-06-11 09:40:10,021 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1340.0 (TID 894, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:10,021 [er for task 894] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1340.0 (TID 894)
   [druid] 2019-06-11 09:40:10,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1340.0 (TID 893) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:40:10,029 [er for task 894] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1340.0 (TID 894). 767 bytes result sent to driver
   [druid] 2019-06-11 09:40:10,037 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1340.0 (TID 894) in 16 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:40:10,037 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1340.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:10,037 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 443
   [druid] 2019-06-11 09:40:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1340 (parallelize at Transform.scala:16) finished in 0.027 s
   [druid] 2019-06-11 09:40:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:40:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:40:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1341)
   [druid] 2019-06-11 09:40:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:40:10,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1341 (MapPartitionsRDD[1569] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:10,038 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_664_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_671 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:10,039 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_668_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_671_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:10,040 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_671_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,040 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_665_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,040 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 671 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:10,041 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 442
   [druid] 2019-06-11 09:40:10,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1341 (MapPartitionsRDD[1569] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:40:10,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1341.0 with 1 tasks
   [druid] 2019-06-11 09:40:10,041 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1341.0 (TID 895, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:10,041 [er for task 895] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1341.0 (TID 895)
   [druid] 2019-06-11 09:40:10,041 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_669_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,042 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 437
   [druid] 2019-06-11 09:40:10,043 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_662_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,043 [er for task 895] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:10,043 [er for task 895] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:10,043 [er for task 895] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:10,043 [er for task 895] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:10,043 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_667_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 438
   [druid] 2019-06-11 09:40:10,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 441
   [druid] 2019-06-11 09:40:10,045 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 440
   [druid] 2019-06-11 09:40:10,045 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 439
   [druid] 2019-06-11 09:40:10,045 [er for task 895] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1341.0 (TID 895). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:40:10,046 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1341.0 (TID 895) in 5 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:10,046 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1341.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1341 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:40:10,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 447 finished: print at Transform.scala:36, took 0.036935 s
   [druid] 2019-06-11 09:40:10,046 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_661_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,047 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_659_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,048 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_666_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,049 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_663_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,050 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_660_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,050 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_658_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 436
   [druid] 2019-06-11 09:40:10,052 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:10,052 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 446 is 83 bytes
   [druid] 2019-06-11 09:40:10,053 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 447 is 160 bytes
   [druid] 2019-06-11 09:40:10,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 448 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:10,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1344 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:10,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1342, ShuffleMapStage 1343)
   [druid] 2019-06-11 09:40:10,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:40:10,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1344 (MapPartitionsRDD[1569] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:10,053 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_672 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:10,054 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_672_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:10,054 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_672_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:10,055 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 672 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:10,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1344 (MapPartitionsRDD[1569] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:40:10,055 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1344.0 with 1 tasks
   [druid] 2019-06-11 09:40:10,055 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1344.0 (TID 896, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:10,055 [er for task 896] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1344.0 (TID 896)
   [druid] 2019-06-11 09:40:10,056 [er for task 896] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:10,056 [er for task 896] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:10,056 [er for task 896] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:10,056 [er for task 896] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:10,057 [er for task 896] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1344.0 (TID 896). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:10,058 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1344.0 (TID 896) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:10,058 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1344.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:10,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1344 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:40:10,058 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 448 finished: print at Transform.scala:36, took 0.005735 s
   [druid] 2019-06-11 09:40:10,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217210000 ms.0 from job set of time 1560217210000 ms
   [druid] 2019-06-11 09:40:10,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.058 s for time 1560217210000 ms (execution: 0.051 s)
   [druid] 2019-06-11 09:40:10,058 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1562 from persistence list
   [druid] 2019-06-11 09:40:10,058 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1562
   [druid] 2019-06-11 09:40:10,058 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1557 from persistence list
   [druid] 2019-06-11 09:40:10,059 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1557
   [druid] 2019-06-11 09:40:10,059 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1556 from persistence list
   [druid] 2019-06-11 09:40:10,059 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1556
   [druid] 2019-06-11 09:40:10,059 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1556] at socketTextStream at Transform.scala:18 of time 1560217210000 ms
   [druid] 2019-06-11 09:40:10,059 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217200000 ms
   [druid] 2019-06-11 09:40:10,059 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217200000 ms
   [druid] 2019-06-11 09:40:15,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217215000 ms
   [druid] 2019-06-11 09:40:15,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217215000 ms.0 from job set of time 1560217215000 ms
   [druid] 2019-06-11 09:40:15,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1571 (map at Transform.scala:20)
   [druid] 2019-06-11 09:40:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:40:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 449 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1347 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1345, ShuffleMapStage 1346)
   [druid] 2019-06-11 09:40:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1346)
   [druid] 2019-06-11 09:40:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1346 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:40:15,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_673 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:15,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_673_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:40:15,010 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_673_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:15,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 673 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1346 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:40:15,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1346.0 with 2 tasks
   [druid] 2019-06-11 09:40:15,011 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1346.0 (TID 897, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:15,011 [er for task 897] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1346.0 (TID 897)
   [druid] 2019-06-11 09:40:15,020 [er for task 897] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1346.0 (TID 897). 810 bytes result sent to driver
   [druid] 2019-06-11 09:40:15,021 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1346.0 (TID 898, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:15,021 [er for task 898] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1346.0 (TID 898)
   [druid] 2019-06-11 09:40:15,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1346.0 (TID 897) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:40:15,028 [er for task 898] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1346.0 (TID 898). 767 bytes result sent to driver
   [druid] 2019-06-11 09:40:15,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1346.0 (TID 898) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:40:15,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1346.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:15,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1346 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:40:15,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:40:15,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:40:15,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1347)
   [druid] 2019-06-11 09:40:15,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:40:15,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1347 (MapPartitionsRDD[1576] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:15,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_674 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:15,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_674_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:15,030 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_674_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:15,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 674 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:15,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1347 (MapPartitionsRDD[1576] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:40:15,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1347.0 with 1 tasks
   [druid] 2019-06-11 09:40:15,030 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1347.0 (TID 899, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:15,030 [er for task 899] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1347.0 (TID 899)
   [druid] 2019-06-11 09:40:15,031 [er for task 899] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:15,031 [er for task 899] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:15,031 [er for task 899] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:15,031 [er for task 899] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:15,032 [er for task 899] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1347.0 (TID 899). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:40:15,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1347.0 (TID 899) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:15,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1347.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1347 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:15,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 449 finished: print at Transform.scala:36, took 0.024793 s
   [druid] 2019-06-11 09:40:15,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:15,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 448 is 83 bytes
   [druid] 2019-06-11 09:40:15,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 449 is 160 bytes
   [druid] 2019-06-11 09:40:15,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 450 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:15,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1350 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:15,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1348, ShuffleMapStage 1349)
   [druid] 2019-06-11 09:40:15,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:40:15,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1350 (MapPartitionsRDD[1576] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:15,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_675 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:15,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_675_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:15,038 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_675_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:15,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 675 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1350 (MapPartitionsRDD[1576] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:40:15,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1350.0 with 1 tasks
   [druid] 2019-06-11 09:40:15,038 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1350.0 (TID 900, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:15,038 [er for task 900] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1350.0 (TID 900)
   [druid] 2019-06-11 09:40:15,039 [er for task 900] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:15,039 [er for task 900] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:15,039 [er for task 900] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:15,039 [er for task 900] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:15,040 [er for task 900] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1350.0 (TID 900). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:15,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1350.0 (TID 900) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:15,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1350.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1350 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:15,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 450 finished: print at Transform.scala:36, took 0.004623 s
   [druid] 2019-06-11 09:40:15,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217215000 ms.0 from job set of time 1560217215000 ms
   [druid] 2019-06-11 09:40:15,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560217215000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:40:15,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1569 from persistence list
   [druid] 2019-06-11 09:40:15,041 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1569
   [druid] 2019-06-11 09:40:15,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1564 from persistence list
   [druid] 2019-06-11 09:40:15,041 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1564
   [druid] 2019-06-11 09:40:15,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1563 from persistence list
   [druid] 2019-06-11 09:40:15,041 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1563
   [druid] 2019-06-11 09:40:15,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1563] at socketTextStream at Transform.scala:18 of time 1560217215000 ms
   [druid] 2019-06-11 09:40:15,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217205000 ms
   [druid] 2019-06-11 09:40:15,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217205000 ms
   [druid] 2019-06-11 09:40:20,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217220000 ms
   [druid] 2019-06-11 09:40:20,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217220000 ms.0 from job set of time 1560217220000 ms
   [druid] 2019-06-11 09:40:20,006 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:20,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:40:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1578 (map at Transform.scala:20)
   [druid] 2019-06-11 09:40:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 451 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1353 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1352, ShuffleMapStage 1351)
   [druid] 2019-06-11 09:40:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1351)
   [druid] 2019-06-11 09:40:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1351 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:40:20,007 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_676 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:20,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_676_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:40:20,010 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_676_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:20,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 676 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1351 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:40:20,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1351.0 with 2 tasks
   [druid] 2019-06-11 09:40:20,010 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1351.0 (TID 901, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:20,010 [er for task 901] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1351.0 (TID 901)
   [druid] 2019-06-11 09:40:20,019 [er for task 901] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1351.0 (TID 901). 853 bytes result sent to driver
   [druid] 2019-06-11 09:40:20,019 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1351.0 (TID 902, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:20,019 [er for task 902] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1351.0 (TID 902)
   [druid] 2019-06-11 09:40:20,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1351.0 (TID 901) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:40:20,028 [er for task 902] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1351.0 (TID 902). 810 bytes result sent to driver
   [druid] 2019-06-11 09:40:20,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1351.0 (TID 902) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:40:20,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1351.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1351 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:40:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:40:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:40:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1353)
   [druid] 2019-06-11 09:40:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:40:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1353 (MapPartitionsRDD[1583] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:20,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_677 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:20,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_677_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:20,029 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_677_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:20,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 677 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1353 (MapPartitionsRDD[1583] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:40:20,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1353.0 with 1 tasks
   [druid] 2019-06-11 09:40:20,030 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1353.0 (TID 903, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:20,030 [er for task 903] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1353.0 (TID 903)
   [druid] 2019-06-11 09:40:20,031 [er for task 903] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:20,031 [er for task 903] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:20,031 [er for task 903] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:20,031 [er for task 903] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:20,031 [er for task 903] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1353.0 (TID 903). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:40:20,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1353.0 (TID 903) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:20,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1353.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1353 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:20,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 451 finished: print at Transform.scala:36, took 0.025217 s
   [druid] 2019-06-11 09:40:20,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:20,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 451 is 160 bytes
   [druid] 2019-06-11 09:40:20,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 450 is 83 bytes
   [druid] 2019-06-11 09:40:20,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 452 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:20,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1356 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:20,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1355, ShuffleMapStage 1354)
   [druid] 2019-06-11 09:40:20,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:40:20,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1356 (MapPartitionsRDD[1583] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:20,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_678 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:20,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_678_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:20,035 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_678_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:20,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 678 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:20,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1356 (MapPartitionsRDD[1583] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:40:20,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1356.0 with 1 tasks
   [druid] 2019-06-11 09:40:20,036 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1356.0 (TID 904, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:20,036 [er for task 904] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1356.0 (TID 904)
   [druid] 2019-06-11 09:40:20,037 [er for task 904] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:20,037 [er for task 904] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:20,037 [er for task 904] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:20,037 [er for task 904] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:20,037 [er for task 904] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1356.0 (TID 904). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:20,037 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1356.0 (TID 904) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:20,037 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1356.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:20,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1356 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:40:20,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 452 finished: print at Transform.scala:36, took 0.003959 s
   [druid] 2019-06-11 09:40:20,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217220000 ms.0 from job set of time 1560217220000 ms
   [druid] 2019-06-11 09:40:20,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.038 s for time 1560217220000 ms (execution: 0.033 s)
   [druid] 2019-06-11 09:40:20,038 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1576 from persistence list
   [druid] 2019-06-11 09:40:20,038 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1576
   [druid] 2019-06-11 09:40:20,038 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1571 from persistence list
   [druid] 2019-06-11 09:40:20,038 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1571
   [druid] 2019-06-11 09:40:20,038 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1570 from persistence list
   [druid] 2019-06-11 09:40:20,038 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1570
   [druid] 2019-06-11 09:40:20,038 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1570] at socketTextStream at Transform.scala:18 of time 1560217220000 ms
   [druid] 2019-06-11 09:40:20,038 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217210000 ms
   [druid] 2019-06-11 09:40:20,038 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217210000 ms
   [druid] 2019-06-11 09:40:25,004 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217225000 ms
   [druid] 2019-06-11 09:40:25,004 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217225000 ms.0 from job set of time 1560217225000 ms
   [druid] 2019-06-11 09:40:25,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:25,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:40:25,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1585 (map at Transform.scala:20)
   [druid] 2019-06-11 09:40:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 453 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1359 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1358, ShuffleMapStage 1357)
   [druid] 2019-06-11 09:40:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1357)
   [druid] 2019-06-11 09:40:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1357 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:40:25,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_679 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_679_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:40:25,009 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_679_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:25,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 679 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1357 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:40:25,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1357.0 with 2 tasks
   [druid] 2019-06-11 09:40:25,010 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1357.0 (TID 905, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:25,010 [er for task 905] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1357.0 (TID 905)
   [druid] 2019-06-11 09:40:25,019 [er for task 905] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1357.0 (TID 905). 853 bytes result sent to driver
   [druid] 2019-06-11 09:40:25,019 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1357.0 (TID 906, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:25,019 [er for task 906] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1357.0 (TID 906)
   [druid] 2019-06-11 09:40:25,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1357.0 (TID 905) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:40:25,027 [er for task 906] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1357.0 (TID 906). 810 bytes result sent to driver
   [druid] 2019-06-11 09:40:25,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1357.0 (TID 906) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:40:25,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1357.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:25,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1357 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:40:25,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:40:25,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:40:25,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1359)
   [druid] 2019-06-11 09:40:25,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:40:25,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1359 (MapPartitionsRDD[1590] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:25,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_680 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:25,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_680_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:25,029 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_680_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:25,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 680 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1359 (MapPartitionsRDD[1590] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:40:25,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1359.0 with 1 tasks
   [druid] 2019-06-11 09:40:25,029 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1359.0 (TID 907, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:25,029 [er for task 907] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1359.0 (TID 907)
   [druid] 2019-06-11 09:40:25,030 [er for task 907] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:25,030 [er for task 907] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:25,030 [er for task 907] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:25,030 [er for task 907] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:25,031 [er for task 907] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1359.0 (TID 907). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:25,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1359.0 (TID 907) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:25,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1359.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:25,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1359 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:25,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 453 finished: print at Transform.scala:36, took 0.023892 s
   [druid] 2019-06-11 09:40:25,033 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:25,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 453 is 160 bytes
   [druid] 2019-06-11 09:40:25,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 452 is 83 bytes
   [druid] 2019-06-11 09:40:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 454 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1362 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1360, ShuffleMapStage 1361)
   [druid] 2019-06-11 09:40:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:40:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1362 (MapPartitionsRDD[1590] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:25,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_681 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:25,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_681_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:25,036 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_681_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:25,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 681 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:25,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1362 (MapPartitionsRDD[1590] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:40:25,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1362.0 with 1 tasks
   [druid] 2019-06-11 09:40:25,037 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1362.0 (TID 908, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:25,037 [er for task 908] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1362.0 (TID 908)
   [druid] 2019-06-11 09:40:25,038 [er for task 908] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:25,038 [er for task 908] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:25,038 [er for task 908] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:25,038 [er for task 908] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:25,039 [er for task 908] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1362.0 (TID 908). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:25,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1362.0 (TID 908) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:25,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1362.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:25,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1362 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:40:25,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 454 finished: print at Transform.scala:36, took 0.006156 s
   [druid] 2019-06-11 09:40:25,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217225000 ms.0 from job set of time 1560217225000 ms
   [druid] 2019-06-11 09:40:25,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.039 s for time 1560217225000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:40:25,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1583 from persistence list
   [druid] 2019-06-11 09:40:25,039 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1583
   [druid] 2019-06-11 09:40:25,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1578 from persistence list
   [druid] 2019-06-11 09:40:25,040 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1578
   [druid] 2019-06-11 09:40:25,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1577 from persistence list
   [druid] 2019-06-11 09:40:25,040 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1577
   [druid] 2019-06-11 09:40:25,040 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1577] at socketTextStream at Transform.scala:18 of time 1560217225000 ms
   [druid] 2019-06-11 09:40:25,040 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217215000 ms
   [druid] 2019-06-11 09:40:25,040 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217215000 ms
   [druid] 2019-06-11 09:40:30,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217230000 ms
   [druid] 2019-06-11 09:40:30,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217230000 ms.0 from job set of time 1560217230000 ms
   [druid] 2019-06-11 09:40:30,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:40:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1592 (map at Transform.scala:20)
   [druid] 2019-06-11 09:40:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 455 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1365 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1363, ShuffleMapStage 1364)
   [druid] 2019-06-11 09:40:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1363)
   [druid] 2019-06-11 09:40:30,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1363 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:40:30,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_682 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:30,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_682_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:40:30,009 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_682_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 682 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1363 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:40:30,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1363.0 with 2 tasks
   [druid] 2019-06-11 09:40:30,010 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1363.0 (TID 909, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:30,010 [er for task 909] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1363.0 (TID 909)
   [druid] 2019-06-11 09:40:30,020 [er for task 909] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1363.0 (TID 909). 853 bytes result sent to driver
   [druid] 2019-06-11 09:40:30,020 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1363.0 (TID 910, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:30,020 [er for task 910] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1363.0 (TID 910)
   [druid] 2019-06-11 09:40:30,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1363.0 (TID 909) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:40:30,029 [er for task 910] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1363.0 (TID 910). 810 bytes result sent to driver
   [druid] 2019-06-11 09:40:30,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1363.0 (TID 910) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:40:30,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1363.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1363 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:40:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:40:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:40:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1365)
   [druid] 2019-06-11 09:40:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:40:30,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1365 (MapPartitionsRDD[1597] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:30,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_683 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:30,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_683_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:30,031 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_683_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 683 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:30,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1365 (MapPartitionsRDD[1597] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:40:30,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1365.0 with 1 tasks
   [druid] 2019-06-11 09:40:30,032 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1365.0 (TID 911, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:30,032 [er for task 911] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1365.0 (TID 911)
   [druid] 2019-06-11 09:40:30,033 [er for task 911] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:30,033 [er for task 911] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:30,033 [er for task 911] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:30,033 [er for task 911] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:30,033 [er for task 911] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1365.0 (TID 911). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:30,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1365.0 (TID 911) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:30,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1365.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:30,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1365 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:30,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 455 finished: print at Transform.scala:36, took 0.027094 s
   [druid] 2019-06-11 09:40:30,040 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 445
   [druid] 2019-06-11 09:40:30,041 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_670_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,042 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_677_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,042 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_671_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:30,043 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_676_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 455 is 160 bytes
   [druid] 2019-06-11 09:40:30,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 454 is 83 bytes
   [druid] 2019-06-11 09:40:30,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 456 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:30,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1368 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:30,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1367, ShuffleMapStage 1366)
   [druid] 2019-06-11 09:40:30,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:40:30,044 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_675_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1368 (MapPartitionsRDD[1597] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:30,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_684 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:30,045 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_682_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_684_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:30,046 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 449
   [druid] 2019-06-11 09:40:30,046 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_684_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,047 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 684 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:30,047 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_683_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 447
   [druid] 2019-06-11 09:40:30,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1368 (MapPartitionsRDD[1597] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:40:30,047 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1368.0 with 1 tasks
   [druid] 2019-06-11 09:40:30,047 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1368.0 (TID 912, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:30,047 [er for task 912] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1368.0 (TID 912)
   [druid] 2019-06-11 09:40:30,048 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_673_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,048 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_678_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,049 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 444
   [druid] 2019-06-11 09:40:30,049 [er for task 912] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:30,049 [er for task 912] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:30,049 [er for task 912] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:30,049 [er for task 912] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:30,049 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_679_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,050 [er for task 912] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1368.0 (TID 912). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:40:30,050 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_674_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,051 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1368.0 (TID 912) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:30,051 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1368.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:30,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 448
   [druid] 2019-06-11 09:40:30,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 451
   [druid] 2019-06-11 09:40:30,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1368 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:40:30,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 456 finished: print at Transform.scala:36, took 0.008566 s
   [druid] 2019-06-11 09:40:30,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217230000 ms.0 from job set of time 1560217230000 ms
   [druid] 2019-06-11 09:40:30,052 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.052 s for time 1560217230000 ms (execution: 0.047 s)
   [druid] 2019-06-11 09:40:30,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1590 from persistence list
   [druid] 2019-06-11 09:40:30,052 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_672_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,052 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1590
   [druid] 2019-06-11 09:40:30,052 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1585 from persistence list
   [druid] 2019-06-11 09:40:30,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 450
   [druid] 2019-06-11 09:40:30,052 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1585
   [druid] 2019-06-11 09:40:30,052 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1584 from persistence list
   [druid] 2019-06-11 09:40:30,052 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1584
   [druid] 2019-06-11 09:40:30,052 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1584] at socketTextStream at Transform.scala:18 of time 1560217230000 ms
   [druid] 2019-06-11 09:40:30,052 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217220000 ms
   [druid] 2019-06-11 09:40:30,052 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217220000 ms
   [druid] 2019-06-11 09:40:30,053 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_681_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:30,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 446
   [druid] 2019-06-11 09:40:30,053 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_680_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:35,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217235000 ms
   [druid] 2019-06-11 09:40:35,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217235000 ms.0 from job set of time 1560217235000 ms
   [druid] 2019-06-11 09:40:35,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:40:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1599 (map at Transform.scala:20)
   [druid] 2019-06-11 09:40:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 457 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1371 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1370, ShuffleMapStage 1369)
   [druid] 2019-06-11 09:40:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1369)
   [druid] 2019-06-11 09:40:35,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1369 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:40:35,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_685 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_685_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:40:35,009 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_685_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:35,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 685 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:35,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1369 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:40:35,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1369.0 with 2 tasks
   [druid] 2019-06-11 09:40:35,010 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1369.0 (TID 913, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:35,010 [er for task 913] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1369.0 (TID 913)
   [druid] 2019-06-11 09:40:35,019 [er for task 913] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1369.0 (TID 913). 853 bytes result sent to driver
   [druid] 2019-06-11 09:40:35,019 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1369.0 (TID 914, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:35,020 [er for task 914] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1369.0 (TID 914)
   [druid] 2019-06-11 09:40:35,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1369.0 (TID 913) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:40:35,027 [er for task 914] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1369.0 (TID 914). 767 bytes result sent to driver
   [druid] 2019-06-11 09:40:35,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1369.0 (TID 914) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:40:35,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1369.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1369 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:40:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:40:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:40:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1371)
   [druid] 2019-06-11 09:40:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:40:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1371 (MapPartitionsRDD[1604] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:35,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_686 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:35,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_686_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:35,029 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_686_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:35,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 686 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1371 (MapPartitionsRDD[1604] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:40:35,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1371.0 with 1 tasks
   [druid] 2019-06-11 09:40:35,031 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1371.0 (TID 915, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:35,031 [er for task 915] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1371.0 (TID 915)
   [druid] 2019-06-11 09:40:35,031 [er for task 915] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:35,031 [er for task 915] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:35,031 [er for task 915] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:35,031 [er for task 915] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:35,032 [er for task 915] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1371.0 (TID 915). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:35,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1371.0 (TID 915) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:35,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1371.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:35,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1371 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:35,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 457 finished: print at Transform.scala:36, took 0.025694 s
   [druid] 2019-06-11 09:40:35,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:35,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 457 is 160 bytes
   [druid] 2019-06-11 09:40:35,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 456 is 83 bytes
   [druid] 2019-06-11 09:40:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 458 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1374 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1373, ShuffleMapStage 1372)
   [druid] 2019-06-11 09:40:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:40:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1374 (MapPartitionsRDD[1604] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:35,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_687 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:35,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_687_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:35,037 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_687_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:35,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 687 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1374 (MapPartitionsRDD[1604] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:40:35,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1374.0 with 1 tasks
   [druid] 2019-06-11 09:40:35,038 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1374.0 (TID 916, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:35,038 [er for task 916] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1374.0 (TID 916)
   [druid] 2019-06-11 09:40:35,039 [er for task 916] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:35,039 [er for task 916] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:35,039 [er for task 916] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:35,039 [er for task 916] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:35,040 [er for task 916] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1374.0 (TID 916). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:40:35,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1374.0 (TID 916) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:35,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1374.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1374 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:35,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 458 finished: print at Transform.scala:36, took 0.005450 s
   [druid] 2019-06-11 09:40:35,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217235000 ms.0 from job set of time 1560217235000 ms
   [druid] 2019-06-11 09:40:35,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560217235000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:40:35,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1597 from persistence list
   [druid] 2019-06-11 09:40:35,041 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1597
   [druid] 2019-06-11 09:40:35,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1592 from persistence list
   [druid] 2019-06-11 09:40:35,041 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1592
   [druid] 2019-06-11 09:40:35,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1591 from persistence list
   [druid] 2019-06-11 09:40:35,041 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1591
   [druid] 2019-06-11 09:40:35,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1591] at socketTextStream at Transform.scala:18 of time 1560217235000 ms
   [druid] 2019-06-11 09:40:35,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217225000 ms
   [druid] 2019-06-11 09:40:35,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217225000 ms
   [druid] 2019-06-11 09:40:40,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217240000 ms
   [druid] 2019-06-11 09:40:40,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217240000 ms.0 from job set of time 1560217240000 ms
   [druid] 2019-06-11 09:40:40,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1606 (map at Transform.scala:20)
   [druid] 2019-06-11 09:40:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:40:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 459 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1377 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1376, ShuffleMapStage 1375)
   [druid] 2019-06-11 09:40:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1376)
   [druid] 2019-06-11 09:40:40,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1376 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:40:40,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_688 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:40,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_688_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:40:40,010 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_688_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:40,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 688 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:40,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1376 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:40:40,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1376.0 with 2 tasks
   [druid] 2019-06-11 09:40:40,011 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1376.0 (TID 917, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:40,011 [er for task 917] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1376.0 (TID 917)
   [druid] 2019-06-11 09:40:40,020 [er for task 917] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1376.0 (TID 917). 810 bytes result sent to driver
   [druid] 2019-06-11 09:40:40,020 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1376.0 (TID 918, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:40,020 [er for task 918] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1376.0 (TID 918)
   [druid] 2019-06-11 09:40:40,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1376.0 (TID 917) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:40:40,028 [er for task 918] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1376.0 (TID 918). 810 bytes result sent to driver
   [druid] 2019-06-11 09:40:40,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1376.0 (TID 918) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:40:40,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1376.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1376 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:40:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:40:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:40:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1377)
   [druid] 2019-06-11 09:40:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:40:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1377 (MapPartitionsRDD[1611] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:40,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_689 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:40,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_689_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:40,030 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_689_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:40,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 689 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1377 (MapPartitionsRDD[1611] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:40:40,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1377.0 with 1 tasks
   [druid] 2019-06-11 09:40:40,031 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1377.0 (TID 919, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:40,031 [er for task 919] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1377.0 (TID 919)
   [druid] 2019-06-11 09:40:40,031 [er for task 919] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:40,031 [er for task 919] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:40,031 [er for task 919] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:40,031 [er for task 919] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:40,032 [er for task 919] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1377.0 (TID 919). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:40,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1377.0 (TID 919) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:40,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1377.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1377 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:40,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 459 finished: print at Transform.scala:36, took 0.024096 s
   [druid] 2019-06-11 09:40:40,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:40,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 458 is 83 bytes
   [druid] 2019-06-11 09:40:40,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 459 is 160 bytes
   [druid] 2019-06-11 09:40:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 460 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1380 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1378, ShuffleMapStage 1379)
   [druid] 2019-06-11 09:40:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:40:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1380 (MapPartitionsRDD[1611] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:40,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_690 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:40,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_690_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:40,037 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_690_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:40,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 690 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1380 (MapPartitionsRDD[1611] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:40:40,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1380.0 with 1 tasks
   [druid] 2019-06-11 09:40:40,038 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1380.0 (TID 920, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:40,038 [er for task 920] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1380.0 (TID 920)
   [druid] 2019-06-11 09:40:40,039 [er for task 920] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:40,039 [er for task 920] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:40,039 [er for task 920] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:40,039 [er for task 920] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:40,040 [er for task 920] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1380.0 (TID 920). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:40:40,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1380.0 (TID 920) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:40,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1380.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:40,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1380 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:40,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 460 finished: print at Transform.scala:36, took 0.005659 s
   [druid] 2019-06-11 09:40:40,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217240000 ms.0 from job set of time 1560217240000 ms
   [druid] 2019-06-11 09:40:40,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560217240000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:40:40,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1604 from persistence list
   [druid] 2019-06-11 09:40:40,041 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1604
   [druid] 2019-06-11 09:40:40,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1599 from persistence list
   [druid] 2019-06-11 09:40:40,041 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1599
   [druid] 2019-06-11 09:40:40,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1598 from persistence list
   [druid] 2019-06-11 09:40:40,041 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1598
   [druid] 2019-06-11 09:40:40,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1598] at socketTextStream at Transform.scala:18 of time 1560217240000 ms
   [druid] 2019-06-11 09:40:40,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217230000 ms
   [druid] 2019-06-11 09:40:40,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217230000 ms
   [druid] 2019-06-11 09:40:45,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217245000 ms
   [druid] 2019-06-11 09:40:45,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217245000 ms.0 from job set of time 1560217245000 ms
   [druid] 2019-06-11 09:40:45,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:40:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1613 (map at Transform.scala:20)
   [druid] 2019-06-11 09:40:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 461 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1383 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1381, ShuffleMapStage 1382)
   [druid] 2019-06-11 09:40:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1381)
   [druid] 2019-06-11 09:40:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1381 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:40:45,007 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_691 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:45,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_691_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:40:45,009 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_691_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:45,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 691 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1381 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:40:45,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1381.0 with 2 tasks
   [druid] 2019-06-11 09:40:45,010 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1381.0 (TID 921, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:45,010 [er for task 921] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1381.0 (TID 921)
   [druid] 2019-06-11 09:40:45,019 [er for task 921] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1381.0 (TID 921). 810 bytes result sent to driver
   [druid] 2019-06-11 09:40:45,020 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1381.0 (TID 922, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:45,020 [er for task 922] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1381.0 (TID 922)
   [druid] 2019-06-11 09:40:45,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1381.0 (TID 921) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:40:45,028 [er for task 922] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1381.0 (TID 922). 810 bytes result sent to driver
   [druid] 2019-06-11 09:40:45,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1381.0 (TID 922) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:40:45,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1381.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1381 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:40:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:40:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:40:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1383)
   [druid] 2019-06-11 09:40:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:40:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1383 (MapPartitionsRDD[1618] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:45,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_692 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:45,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_692_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:45,030 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_692_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:45,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 692 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:45,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1383 (MapPartitionsRDD[1618] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:40:45,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1383.0 with 1 tasks
   [druid] 2019-06-11 09:40:45,031 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1383.0 (TID 923, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:45,031 [er for task 923] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1383.0 (TID 923)
   [druid] 2019-06-11 09:40:45,032 [er for task 923] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:45,032 [er for task 923] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:45,032 [er for task 923] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:45,032 [er for task 923] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:45,034 [er for task 923] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1383.0 (TID 923). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:40:45,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1383.0 (TID 923) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:45,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1383.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1383 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:40:45,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 461 finished: print at Transform.scala:36, took 0.027305 s
   [druid] 2019-06-11 09:40:45,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:45,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 461 is 160 bytes
   [druid] 2019-06-11 09:40:45,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 460 is 83 bytes
   [druid] 2019-06-11 09:40:45,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 462 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:45,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1386 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:45,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1385, ShuffleMapStage 1384)
   [druid] 2019-06-11 09:40:45,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:40:45,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1386 (MapPartitionsRDD[1618] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:45,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_693 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:45,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_693_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:45,039 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_693_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:45,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 693 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:45,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1386 (MapPartitionsRDD[1618] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:40:45,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1386.0 with 1 tasks
   [druid] 2019-06-11 09:40:45,039 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1386.0 (TID 924, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:45,039 [er for task 924] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1386.0 (TID 924)
   [druid] 2019-06-11 09:40:45,040 [er for task 924] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:45,040 [er for task 924] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:45,040 [er for task 924] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:45,040 [er for task 924] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:45,041 [er for task 924] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1386.0 (TID 924). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:40:45,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1386.0 (TID 924) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:45,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1386.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:45,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1386 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:45,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 462 finished: print at Transform.scala:36, took 0.004834 s
   [druid] 2019-06-11 09:40:45,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217245000 ms.0 from job set of time 1560217245000 ms
   [druid] 2019-06-11 09:40:45,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560217245000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:40:45,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1611 from persistence list
   [druid] 2019-06-11 09:40:45,042 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1611
   [druid] 2019-06-11 09:40:45,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1606 from persistence list
   [druid] 2019-06-11 09:40:45,042 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1606
   [druid] 2019-06-11 09:40:45,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1605 from persistence list
   [druid] 2019-06-11 09:40:45,042 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1605
   [druid] 2019-06-11 09:40:45,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1605] at socketTextStream at Transform.scala:18 of time 1560217245000 ms
   [druid] 2019-06-11 09:40:45,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217235000 ms
   [druid] 2019-06-11 09:40:45,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217235000 ms
   [druid] 2019-06-11 09:40:50,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217250000 ms
   [druid] 2019-06-11 09:40:50,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217250000 ms.0 from job set of time 1560217250000 ms
   [druid] 2019-06-11 09:40:50,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1620 (map at Transform.scala:20)
   [druid] 2019-06-11 09:40:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:40:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 463 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1389 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1388, ShuffleMapStage 1387)
   [druid] 2019-06-11 09:40:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1388)
   [druid] 2019-06-11 09:40:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1388 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:40:50,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_694 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:50,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_694_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:40:50,010 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_694_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 694 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:50,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1388 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:40:50,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1388.0 with 2 tasks
   [druid] 2019-06-11 09:40:50,010 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1388.0 (TID 925, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:50,010 [er for task 925] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1388.0 (TID 925)
   [druid] 2019-06-11 09:40:50,019 [er for task 925] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1388.0 (TID 925). 853 bytes result sent to driver
   [druid] 2019-06-11 09:40:50,020 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1388.0 (TID 926, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:50,020 [er for task 926] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1388.0 (TID 926)
   [druid] 2019-06-11 09:40:50,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1388.0 (TID 925) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:40:50,028 [er for task 926] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1388.0 (TID 926). 767 bytes result sent to driver
   [druid] 2019-06-11 09:40:50,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1388.0 (TID 926) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:40:50,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1388.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1388 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:40:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:40:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:40:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1389)
   [druid] 2019-06-11 09:40:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:40:50,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1389 (MapPartitionsRDD[1625] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:50,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_695 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:50,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_695_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:50,030 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_695_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 695 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1389 (MapPartitionsRDD[1625] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:40:50,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1389.0 with 1 tasks
   [druid] 2019-06-11 09:40:50,031 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1389.0 (TID 927, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:50,031 [er for task 927] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1389.0 (TID 927)
   [druid] 2019-06-11 09:40:50,032 [er for task 927] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:50,032 [er for task 927] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:50,032 [er for task 927] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:50,032 [er for task 927] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:50,033 [er for task 927] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1389.0 (TID 927). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:40:50,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1389.0 (TID 927) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:50,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1389.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:50,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1389 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:50,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 463 finished: print at Transform.scala:36, took 0.025117 s
   [druid] 2019-06-11 09:40:50,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:50,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 462 is 83 bytes
   [druid] 2019-06-11 09:40:50,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 463 is 160 bytes
   [druid] 2019-06-11 09:40:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 464 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1392 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1391, ShuffleMapStage 1390)
   [druid] 2019-06-11 09:40:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:40:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1392 (MapPartitionsRDD[1625] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:50,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_696 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:50,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_696_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:40:50,044 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_688_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,044 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_696_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 696 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:50,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1392 (MapPartitionsRDD[1625] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:40:50,045 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1392.0 with 1 tasks
   [druid] 2019-06-11 09:40:50,045 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1392.0 (TID 928, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:50,045 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_695_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,045 [er for task 928] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1392.0 (TID 928)
   [druid] 2019-06-11 09:40:50,046 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_687_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,046 [er for task 928] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:50,047 [er for task 928] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:40:50,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 459
   [druid] 2019-06-11 09:40:50,047 [er for task 928] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:50,047 [er for task 928] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:50,047 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_684_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,048 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 456
   [druid] 2019-06-11 09:40:50,048 [er for task 928] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1392.0 (TID 928). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:40:50,048 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1392.0 (TID 928) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:50,048 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1392.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:50,048 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_691_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1392 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:40:50,049 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 455
   [druid] 2019-06-11 09:40:50,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 464 finished: print at Transform.scala:36, took 0.013121 s
   [druid] 2019-06-11 09:40:50,049 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217250000 ms.0 from job set of time 1560217250000 ms
   [druid] 2019-06-11 09:40:50,049 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.049 s for time 1560217250000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:40:50,049 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1618 from persistence list
   [druid] 2019-06-11 09:40:50,049 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_685_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,049 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1618
   [druid] 2019-06-11 09:40:50,049 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1613 from persistence list
   [druid] 2019-06-11 09:40:50,050 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1613
   [druid] 2019-06-11 09:40:50,050 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1612 from persistence list
   [druid] 2019-06-11 09:40:50,050 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1612] at socketTextStream at Transform.scala:18 of time 1560217250000 ms
   [druid] 2019-06-11 09:40:50,050 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 457
   [druid] 2019-06-11 09:40:50,050 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1612
   [druid] 2019-06-11 09:40:50,050 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217240000 ms
   [druid] 2019-06-11 09:40:50,050 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 458
   [druid] 2019-06-11 09:40:50,050 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217240000 ms
   [druid] 2019-06-11 09:40:50,050 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_689_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,051 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_693_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 452
   [druid] 2019-06-11 09:40:50,052 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_686_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 454
   [druid] 2019-06-11 09:40:50,053 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_690_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,054 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_694_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:50,054 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 453
   [druid] 2019-06-11 09:40:50,055 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_692_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:55,011 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217255000 ms
   [druid] 2019-06-11 09:40:55,011 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217255000 ms.0 from job set of time 1560217255000 ms
   [druid] 2019-06-11 09:40:55,013 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1627 (map at Transform.scala:20)
   [druid] 2019-06-11 09:40:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:40:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 465 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1395 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1393, ShuffleMapStage 1394)
   [druid] 2019-06-11 09:40:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1394)
   [druid] 2019-06-11 09:40:55,013 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1394 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:40:55,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_697 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:55,014 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_697_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:40:55,015 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_697_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:55,015 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 697 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:55,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1394 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:40:55,015 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1394.0 with 2 tasks
   [druid] 2019-06-11 09:40:55,015 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1394.0 (TID 929, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:55,015 [er for task 929] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1394.0 (TID 929)
   [druid] 2019-06-11 09:40:55,027 [er for task 929] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1394.0 (TID 929). 810 bytes result sent to driver
   [druid] 2019-06-11 09:40:55,027 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1394.0 (TID 930, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:40:55,027 [er for task 930] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1394.0 (TID 930)
   [druid] 2019-06-11 09:40:55,027 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1394.0 (TID 929) in 12 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:40:55,035 [er for task 930] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1394.0 (TID 930). 810 bytes result sent to driver
   [druid] 2019-06-11 09:40:55,036 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1394.0 (TID 930) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:40:55,036 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1394.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1394 (parallelize at Transform.scala:16) finished in 0.021 s
   [druid] 2019-06-11 09:40:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:40:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:40:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1395)
   [druid] 2019-06-11 09:40:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:40:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1395 (MapPartitionsRDD[1632] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:55,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_698 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:55,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_698_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:55,038 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_698_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:55,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 698 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:55,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1395 (MapPartitionsRDD[1632] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:40:55,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1395.0 with 1 tasks
   [druid] 2019-06-11 09:40:55,039 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1395.0 (TID 931, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:55,039 [er for task 931] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1395.0 (TID 931)
   [druid] 2019-06-11 09:40:55,040 [er for task 931] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:55,040 [er for task 931] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:55,040 [er for task 931] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:55,040 [er for task 931] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:55,040 [er for task 931] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1395.0 (TID 931). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:55,041 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1395.0 (TID 931) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:55,041 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1395.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:55,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1395 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:55,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 465 finished: print at Transform.scala:36, took 0.028381 s
   [druid] 2019-06-11 09:40:55,043 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:40:55,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 464 is 83 bytes
   [druid] 2019-06-11 09:40:55,044 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 465 is 160 bytes
   [druid] 2019-06-11 09:40:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 466 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:40:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1398 (print at Transform.scala:36)
   [druid] 2019-06-11 09:40:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1396, ShuffleMapStage 1397)
   [druid] 2019-06-11 09:40:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:40:55,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1398 (MapPartitionsRDD[1632] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:40:55,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_699 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:55,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_699_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:40:55,045 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_699_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:40:55,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 699 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:40:55,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1398 (MapPartitionsRDD[1632] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:40:55,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1398.0 with 1 tasks
   [druid] 2019-06-11 09:40:55,046 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1398.0 (TID 932, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:40:55,046 [er for task 932] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1398.0 (TID 932)
   [druid] 2019-06-11 09:40:55,047 [er for task 932] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:40:55,047 [er for task 932] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:55,047 [er for task 932] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:40:55,047 [er for task 932] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:40:55,047 [er for task 932] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1398.0 (TID 932). 966 bytes result sent to driver
   [druid] 2019-06-11 09:40:55,048 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1398.0 (TID 932) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:40:55,048 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1398.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:40:55,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1398 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:40:55,048 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 466 finished: print at Transform.scala:36, took 0.004766 s
   [druid] 2019-06-11 09:40:55,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217255000 ms.0 from job set of time 1560217255000 ms
   [druid] 2019-06-11 09:40:55,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.048 s for time 1560217255000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:40:55,048 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1625 from persistence list
   [druid] 2019-06-11 09:40:55,049 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1625
   [druid] 2019-06-11 09:40:55,049 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1620 from persistence list
   [druid] 2019-06-11 09:40:55,049 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1620
   [druid] 2019-06-11 09:40:55,049 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1619 from persistence list
   [druid] 2019-06-11 09:40:55,049 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1619
   [druid] 2019-06-11 09:40:55,049 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1619] at socketTextStream at Transform.scala:18 of time 1560217255000 ms
   [druid] 2019-06-11 09:40:55,049 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217245000 ms
   [druid] 2019-06-11 09:40:55,049 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217245000 ms
   [druid] 2019-06-11 09:41:00,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217260000 ms
   [druid] 2019-06-11 09:41:00,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217260000 ms.0 from job set of time 1560217260000 ms
   [druid] 2019-06-11 09:41:00,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:41:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1634 (map at Transform.scala:20)
   [druid] 2019-06-11 09:41:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 467 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1401 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1399, ShuffleMapStage 1400)
   [druid] 2019-06-11 09:41:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1399)
   [druid] 2019-06-11 09:41:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1399 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:41:00,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_700 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:00,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_700_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:41:00,011 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_700_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:00,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 700 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1399 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:41:00,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1399.0 with 2 tasks
   [druid] 2019-06-11 09:41:00,012 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1399.0 (TID 933, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:00,012 [er for task 933] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1399.0 (TID 933)
   [druid] 2019-06-11 09:41:00,021 [er for task 933] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1399.0 (TID 933). 853 bytes result sent to driver
   [druid] 2019-06-11 09:41:00,021 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1399.0 (TID 934, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:00,022 [er for task 934] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1399.0 (TID 934)
   [druid] 2019-06-11 09:41:00,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1399.0 (TID 933) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:41:00,029 [er for task 934] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1399.0 (TID 934). 767 bytes result sent to driver
   [druid] 2019-06-11 09:41:00,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1399.0 (TID 934) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:41:00,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1399.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1399 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:41:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:41:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:41:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1401)
   [druid] 2019-06-11 09:41:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:41:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1401 (MapPartitionsRDD[1639] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:00,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_701 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:00,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_701_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:00,031 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_701_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:00,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 701 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:00,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1401 (MapPartitionsRDD[1639] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:41:00,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1401.0 with 1 tasks
   [druid] 2019-06-11 09:41:00,032 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1401.0 (TID 935, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:00,032 [er for task 935] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1401.0 (TID 935)
   [druid] 2019-06-11 09:41:00,032 [er for task 935] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:00,032 [er for task 935] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:00,033 [er for task 935] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:00,033 [er for task 935] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:00,033 [er for task 935] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1401.0 (TID 935). 966 bytes result sent to driver
   [druid] 2019-06-11 09:41:00,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1401.0 (TID 935) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:00,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1401.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:00,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1401 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:41:00,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 467 finished: print at Transform.scala:36, took 0.024127 s
   [druid] 2019-06-11 09:41:00,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:00,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 467 is 160 bytes
   [druid] 2019-06-11 09:41:00,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 466 is 83 bytes
   [druid] 2019-06-11 09:41:00,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 468 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:00,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1404 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:00,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1403, ShuffleMapStage 1402)
   [druid] 2019-06-11 09:41:00,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:41:00,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1404 (MapPartitionsRDD[1639] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:00,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_702 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:00,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_702_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:00,038 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_702_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:00,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 702 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:00,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1404 (MapPartitionsRDD[1639] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:41:00,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1404.0 with 1 tasks
   [druid] 2019-06-11 09:41:00,038 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1404.0 (TID 936, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:00,039 [er for task 936] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1404.0 (TID 936)
   [druid] 2019-06-11 09:41:00,039 [er for task 936] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:00,039 [er for task 936] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:00,039 [er for task 936] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:00,039 [er for task 936] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:00,040 [er for task 936] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1404.0 (TID 936). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:41:00,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1404.0 (TID 936) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:00,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1404.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:00,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1404 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:41:00,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 468 finished: print at Transform.scala:36, took 0.005141 s
   [druid] 2019-06-11 09:41:00,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217260000 ms.0 from job set of time 1560217260000 ms
   [druid] 2019-06-11 09:41:00,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560217260000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:41:00,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1632 from persistence list
   [druid] 2019-06-11 09:41:00,041 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1632
   [druid] 2019-06-11 09:41:00,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1627 from persistence list
   [druid] 2019-06-11 09:41:00,041 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1627
   [druid] 2019-06-11 09:41:00,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1626 from persistence list
   [druid] 2019-06-11 09:41:00,042 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1626
   [druid] 2019-06-11 09:41:00,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1626] at socketTextStream at Transform.scala:18 of time 1560217260000 ms
   [druid] 2019-06-11 09:41:00,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217250000 ms
   [druid] 2019-06-11 09:41:00,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217250000 ms
   [druid] 2019-06-11 09:41:05,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217265000 ms
   [druid] 2019-06-11 09:41:05,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217265000 ms.0 from job set of time 1560217265000 ms
   [druid] 2019-06-11 09:41:05,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:41:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1641 (map at Transform.scala:20)
   [druid] 2019-06-11 09:41:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 469 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1407 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1406, ShuffleMapStage 1405)
   [druid] 2019-06-11 09:41:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1405)
   [druid] 2019-06-11 09:41:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1405 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:41:05,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_703 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:05,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_703_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:41:05,010 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_703_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:05,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 703 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1405 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:41:05,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1405.0 with 2 tasks
   [druid] 2019-06-11 09:41:05,010 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1405.0 (TID 937, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:05,010 [er for task 937] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1405.0 (TID 937)
   [druid] 2019-06-11 09:41:05,019 [er for task 937] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1405.0 (TID 937). 853 bytes result sent to driver
   [druid] 2019-06-11 09:41:05,019 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1405.0 (TID 938, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:05,019 [er for task 938] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1405.0 (TID 938)
   [druid] 2019-06-11 09:41:05,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1405.0 (TID 937) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:41:05,028 [er for task 938] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1405.0 (TID 938). 810 bytes result sent to driver
   [druid] 2019-06-11 09:41:05,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1405.0 (TID 938) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:41:05,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1405.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1405 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:41:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:41:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:41:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1407)
   [druid] 2019-06-11 09:41:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:41:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1407 (MapPartitionsRDD[1646] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:05,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_704 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:05,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_704_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:05,030 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_704_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:05,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 704 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:05,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1407 (MapPartitionsRDD[1646] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:41:05,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1407.0 with 1 tasks
   [druid] 2019-06-11 09:41:05,030 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1407.0 (TID 939, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:05,030 [er for task 939] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1407.0 (TID 939)
   [druid] 2019-06-11 09:41:05,031 [er for task 939] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:05,031 [er for task 939] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:05,031 [er for task 939] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:05,031 [er for task 939] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:05,031 [er for task 939] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1407.0 (TID 939). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:41:05,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1407.0 (TID 939) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:05,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1407.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:05,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1407 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:41:05,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 469 finished: print at Transform.scala:36, took 0.023874 s
   [druid] 2019-06-11 09:41:05,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:05,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 469 is 160 bytes
   [druid] 2019-06-11 09:41:05,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 468 is 83 bytes
   [druid] 2019-06-11 09:41:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 470 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1410 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1409, ShuffleMapStage 1408)
   [druid] 2019-06-11 09:41:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:41:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1410 (MapPartitionsRDD[1646] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:05,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_705 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:05,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_705_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:05,036 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_705_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:05,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 705 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1410 (MapPartitionsRDD[1646] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:41:05,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1410.0 with 1 tasks
   [druid] 2019-06-11 09:41:05,036 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1410.0 (TID 940, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:05,036 [er for task 940] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1410.0 (TID 940)
   [druid] 2019-06-11 09:41:05,038 [er for task 940] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:05,038 [er for task 940] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:05,038 [er for task 940] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:05,038 [er for task 940] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:05,038 [er for task 940] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1410.0 (TID 940). 966 bytes result sent to driver
   [druid] 2019-06-11 09:41:05,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1410.0 (TID 940) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:05,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1410.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:05,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1410 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:41:05,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 470 finished: print at Transform.scala:36, took 0.005582 s
   [druid] 2019-06-11 09:41:05,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217265000 ms.0 from job set of time 1560217265000 ms
   [druid] 2019-06-11 09:41:05,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.039 s for time 1560217265000 ms (execution: 0.033 s)
   [druid] 2019-06-11 09:41:05,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1639 from persistence list
   [druid] 2019-06-11 09:41:05,039 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1639
   [druid] 2019-06-11 09:41:05,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1634 from persistence list
   [druid] 2019-06-11 09:41:05,040 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1634
   [druid] 2019-06-11 09:41:05,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1633 from persistence list
   [druid] 2019-06-11 09:41:05,040 [-thread-pool-23] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1633
   [druid] 2019-06-11 09:41:05,040 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1633] at socketTextStream at Transform.scala:18 of time 1560217265000 ms
   [druid] 2019-06-11 09:41:05,040 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217255000 ms
   [druid] 2019-06-11 09:41:05,040 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217255000 ms
   [druid] 2019-06-11 09:41:10,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217270000 ms
   [druid] 2019-06-11 09:41:10,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217270000 ms.0 from job set of time 1560217270000 ms
   [druid] 2019-06-11 09:41:10,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:10,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1648 (map at Transform.scala:20)
   [druid] 2019-06-11 09:41:10,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:41:10,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 471 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:10,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1413 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1411, ShuffleMapStage 1412)
   [druid] 2019-06-11 09:41:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1412)
   [druid] 2019-06-11 09:41:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1412 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:41:10,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_706 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:10,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_706_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:41:10,009 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_706_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 706 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1412 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:41:10,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1412.0 with 2 tasks
   [druid] 2019-06-11 09:41:10,010 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1412.0 (TID 941, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:10,010 [er for task 941] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1412.0 (TID 941)
   [druid] 2019-06-11 09:41:10,019 [er for task 941] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1412.0 (TID 941). 810 bytes result sent to driver
   [druid] 2019-06-11 09:41:10,019 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1412.0 (TID 942, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:10,019 [er for task 942] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1412.0 (TID 942)
   [druid] 2019-06-11 09:41:10,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1412.0 (TID 941) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:41:10,028 [er for task 942] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1412.0 (TID 942). 767 bytes result sent to driver
   [druid] 2019-06-11 09:41:10,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1412.0 (TID 942) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:41:10,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1412.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:10,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1412 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:41:10,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:41:10,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:41:10,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1413)
   [druid] 2019-06-11 09:41:10,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:41:10,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1413 (MapPartitionsRDD[1653] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:10,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_707 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:10,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_707_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:10,030 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_707_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 707 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:10,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1413 (MapPartitionsRDD[1653] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:41:10,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1413.0 with 1 tasks
   [druid] 2019-06-11 09:41:10,030 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1413.0 (TID 943, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:10,030 [er for task 943] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1413.0 (TID 943)
   [druid] 2019-06-11 09:41:10,032 [er for task 943] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:10,032 [er for task 943] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:10,032 [er for task 943] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:10,032 [er for task 943] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:10,033 [er for task 943] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1413.0 (TID 943). 966 bytes result sent to driver
   [druid] 2019-06-11 09:41:10,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1413.0 (TID 943) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:10,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1413.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1413 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:41:10,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 471 finished: print at Transform.scala:36, took 0.026486 s
   [druid] 2019-06-11 09:41:10,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:10,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 470 is 83 bytes
   [druid] 2019-06-11 09:41:10,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 471 is 160 bytes
   [druid] 2019-06-11 09:41:10,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 472 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:10,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1416 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:10,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1414, ShuffleMapStage 1415)
   [druid] 2019-06-11 09:41:10,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:41:10,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1416 (MapPartitionsRDD[1653] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:10,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_708 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:10,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_708_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:10,043 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_708_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,044 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_698_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 708 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:10,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1416 (MapPartitionsRDD[1653] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:41:10,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1416.0 with 1 tasks
   [druid] 2019-06-11 09:41:10,044 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1416.0 (TID 944, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:10,045 [er for task 944] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1416.0 (TID 944)
   [druid] 2019-06-11 09:41:10,045 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_700_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,045 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_706_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,046 [er for task 944] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:10,046 [er for task 944] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:10,046 [er for task 944] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:10,046 [er for task 944] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:10,046 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 467
   [druid] 2019-06-11 09:41:10,046 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 464
   [druid] 2019-06-11 09:41:10,047 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_696_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,047 [er for task 944] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1416.0 (TID 944). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:41:10,048 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1416.0 (TID 944) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:10,048 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1416.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:10,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1416 (print at Transform.scala:36) finished in 0.004 s
   [druid] 2019-06-11 09:41:10,048 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_703_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,048 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 472 finished: print at Transform.scala:36, took 0.012207 s
   [druid] 2019-06-11 09:41:10,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217270000 ms.0 from job set of time 1560217270000 ms
   [druid] 2019-06-11 09:41:10,048 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.048 s for time 1560217270000 ms (execution: 0.043 s)
   [druid] 2019-06-11 09:41:10,049 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1646 from persistence list
   [druid] 2019-06-11 09:41:10,049 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 460
   [druid] 2019-06-11 09:41:10,049 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1641 from persistence list
   [druid] 2019-06-11 09:41:10,049 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1646
   [druid] 2019-06-11 09:41:10,049 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1641
   [druid] 2019-06-11 09:41:10,049 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1640 from persistence list
   [druid] 2019-06-11 09:41:10,049 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1640
   [druid] 2019-06-11 09:41:10,050 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1640] at socketTextStream at Transform.scala:18 of time 1560217270000 ms
   [druid] 2019-06-11 09:41:10,050 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_707_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,050 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217260000 ms
   [druid] 2019-06-11 09:41:10,050 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217260000 ms
   [druid] 2019-06-11 09:41:10,050 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 466
   [druid] 2019-06-11 09:41:10,051 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_705_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,051 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_704_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,052 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_699_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 462
   [druid] 2019-06-11 09:41:10,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 461
   [druid] 2019-06-11 09:41:10,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 463
   [druid] 2019-06-11 09:41:10,053 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_701_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 465
   [druid] 2019-06-11 09:41:10,054 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_702_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:10,054 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_697_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:15,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217275000 ms
   [druid] 2019-06-11 09:41:15,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217275000 ms.0 from job set of time 1560217275000 ms
   [druid] 2019-06-11 09:41:15,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1655 (map at Transform.scala:20)
   [druid] 2019-06-11 09:41:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:41:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 473 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1419 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1417, ShuffleMapStage 1418)
   [druid] 2019-06-11 09:41:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1418)
   [druid] 2019-06-11 09:41:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1418 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:41:15,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_709 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:15,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_709_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:41:15,009 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_709_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:15,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 709 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1418 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:41:15,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1418.0 with 2 tasks
   [druid] 2019-06-11 09:41:15,010 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1418.0 (TID 945, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:15,010 [er for task 945] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1418.0 (TID 945)
   [druid] 2019-06-11 09:41:15,019 [er for task 945] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1418.0 (TID 945). 810 bytes result sent to driver
   [druid] 2019-06-11 09:41:15,019 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1418.0 (TID 946, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:15,019 [er for task 946] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1418.0 (TID 946)
   [druid] 2019-06-11 09:41:15,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1418.0 (TID 945) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:41:15,027 [er for task 946] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1418.0 (TID 946). 810 bytes result sent to driver
   [druid] 2019-06-11 09:41:15,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1418.0 (TID 946) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:41:15,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1418.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:15,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1418 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:41:15,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:41:15,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:41:15,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1419)
   [druid] 2019-06-11 09:41:15,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:41:15,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1419 (MapPartitionsRDD[1660] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:15,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_710 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:15,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_710_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:15,029 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_710_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:15,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 710 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:15,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1419 (MapPartitionsRDD[1660] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:41:15,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1419.0 with 1 tasks
   [druid] 2019-06-11 09:41:15,029 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1419.0 (TID 947, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:15,029 [er for task 947] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1419.0 (TID 947)
   [druid] 2019-06-11 09:41:15,030 [er for task 947] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:15,030 [er for task 947] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:15,030 [er for task 947] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:15,030 [er for task 947] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:15,031 [er for task 947] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1419.0 (TID 947). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:41:15,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1419.0 (TID 947) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:15,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1419.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:15,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1419 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:41:15,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 473 finished: print at Transform.scala:36, took 0.023413 s
   [druid] 2019-06-11 09:41:15,033 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:15,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 472 is 83 bytes
   [druid] 2019-06-11 09:41:15,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 473 is 160 bytes
   [druid] 2019-06-11 09:41:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 474 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1422 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1421, ShuffleMapStage 1420)
   [druid] 2019-06-11 09:41:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:41:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1422 (MapPartitionsRDD[1660] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:15,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_711 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:15,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_711_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:15,035 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_711_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:15,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 711 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1422 (MapPartitionsRDD[1660] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:41:15,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1422.0 with 1 tasks
   [druid] 2019-06-11 09:41:15,035 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1422.0 (TID 948, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:15,035 [er for task 948] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1422.0 (TID 948)
   [druid] 2019-06-11 09:41:15,036 [er for task 948] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:15,036 [er for task 948] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:15,036 [er for task 948] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:15,036 [er for task 948] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:15,038 [er for task 948] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1422.0 (TID 948). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:41:15,038 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1422.0 (TID 948) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:15,038 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1422.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:15,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1422 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:41:15,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 474 finished: print at Transform.scala:36, took 0.005258 s
   [druid] 2019-06-11 09:41:15,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217275000 ms.0 from job set of time 1560217275000 ms
   [druid] 2019-06-11 09:41:15,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.038 s for time 1560217275000 ms (execution: 0.033 s)
   [druid] 2019-06-11 09:41:15,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1653 from persistence list
   [druid] 2019-06-11 09:41:15,039 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1653
   [druid] 2019-06-11 09:41:15,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1648 from persistence list
   [druid] 2019-06-11 09:41:15,039 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1648
   [druid] 2019-06-11 09:41:15,039 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1647 from persistence list
   [druid] 2019-06-11 09:41:15,039 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1647
   [druid] 2019-06-11 09:41:15,039 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1647] at socketTextStream at Transform.scala:18 of time 1560217275000 ms
   [druid] 2019-06-11 09:41:15,039 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217265000 ms
   [druid] 2019-06-11 09:41:15,040 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217265000 ms
   [druid] 2019-06-11 09:41:20,004 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217280000 ms
   [druid] 2019-06-11 09:41:20,004 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217280000 ms.0 from job set of time 1560217280000 ms
   [druid] 2019-06-11 09:41:20,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1662 (map at Transform.scala:20)
   [druid] 2019-06-11 09:41:20,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:41:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 475 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1425 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1424, ShuffleMapStage 1423)
   [druid] 2019-06-11 09:41:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1424)
   [druid] 2019-06-11 09:41:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1424 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:41:20,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_712 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:20,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_712_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:41:20,009 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_712_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:20,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 712 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1424 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:41:20,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1424.0 with 2 tasks
   [druid] 2019-06-11 09:41:20,010 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1424.0 (TID 949, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:20,010 [er for task 949] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1424.0 (TID 949)
   [druid] 2019-06-11 09:41:20,019 [er for task 949] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1424.0 (TID 949). 810 bytes result sent to driver
   [druid] 2019-06-11 09:41:20,019 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1424.0 (TID 950, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:20,019 [er for task 950] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1424.0 (TID 950)
   [druid] 2019-06-11 09:41:20,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1424.0 (TID 949) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:41:20,027 [er for task 950] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1424.0 (TID 950). 810 bytes result sent to driver
   [druid] 2019-06-11 09:41:20,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1424.0 (TID 950) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:41:20,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1424.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1424 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:41:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:41:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:41:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1425)
   [druid] 2019-06-11 09:41:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:41:20,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1425 (MapPartitionsRDD[1667] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:20,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_713 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:20,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_713_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:20,029 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_713_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:20,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 713 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1425 (MapPartitionsRDD[1667] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:41:20,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1425.0 with 1 tasks
   [druid] 2019-06-11 09:41:20,030 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1425.0 (TID 951, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:20,030 [er for task 951] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1425.0 (TID 951)
   [druid] 2019-06-11 09:41:20,031 [er for task 951] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:20,031 [er for task 951] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:20,031 [er for task 951] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:20,031 [er for task 951] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:20,032 [er for task 951] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1425.0 (TID 951). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:41:20,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1425.0 (TID 951) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:20,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1425.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1425 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:41:20,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 475 finished: print at Transform.scala:36, took 0.025790 s
   [druid] 2019-06-11 09:41:20,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:20,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 474 is 83 bytes
   [druid] 2019-06-11 09:41:20,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 475 is 160 bytes
   [druid] 2019-06-11 09:41:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 476 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1428 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1427, ShuffleMapStage 1426)
   [druid] 2019-06-11 09:41:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:41:20,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1428 (MapPartitionsRDD[1667] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:20,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_714 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:20,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_714_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:20,037 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_714_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:20,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 714 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:20,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1428 (MapPartitionsRDD[1667] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:41:20,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1428.0 with 1 tasks
   [druid] 2019-06-11 09:41:20,038 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1428.0 (TID 952, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:20,038 [er for task 952] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1428.0 (TID 952)
   [druid] 2019-06-11 09:41:20,038 [er for task 952] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:20,038 [er for task 952] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:20,038 [er for task 952] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:20,038 [er for task 952] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:20,039 [er for task 952] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1428.0 (TID 952). 966 bytes result sent to driver
   [druid] 2019-06-11 09:41:20,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1428.0 (TID 952) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:20,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1428.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1428 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:41:20,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 476 finished: print at Transform.scala:36, took 0.004569 s
   [druid] 2019-06-11 09:41:20,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217280000 ms.0 from job set of time 1560217280000 ms
   [druid] 2019-06-11 09:41:20,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560217280000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:41:20,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1660 from persistence list
   [druid] 2019-06-11 09:41:20,040 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1660
   [druid] 2019-06-11 09:41:20,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1655 from persistence list
   [druid] 2019-06-11 09:41:20,040 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1655
   [druid] 2019-06-11 09:41:20,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1654 from persistence list
   [druid] 2019-06-11 09:41:20,041 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1654
   [druid] 2019-06-11 09:41:20,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1654] at socketTextStream at Transform.scala:18 of time 1560217280000 ms
   [druid] 2019-06-11 09:41:20,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217270000 ms
   [druid] 2019-06-11 09:41:20,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217270000 ms
   [druid] 2019-06-11 09:41:25,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217285000 ms
   [druid] 2019-06-11 09:41:25,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217285000 ms.0 from job set of time 1560217285000 ms
   [druid] 2019-06-11 09:41:25,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1669 (map at Transform.scala:20)
   [druid] 2019-06-11 09:41:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:41:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 477 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1431 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1429, ShuffleMapStage 1430)
   [druid] 2019-06-11 09:41:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1430)
   [druid] 2019-06-11 09:41:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1430 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:41:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_715 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:25,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_715_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:41:25,011 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_715_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:25,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 715 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:25,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1430 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:41:25,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1430.0 with 2 tasks
   [druid] 2019-06-11 09:41:25,011 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1430.0 (TID 953, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:25,011 [er for task 953] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1430.0 (TID 953)
   [druid] 2019-06-11 09:41:25,020 [er for task 953] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1430.0 (TID 953). 853 bytes result sent to driver
   [druid] 2019-06-11 09:41:25,020 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1430.0 (TID 954, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:25,020 [er for task 954] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1430.0 (TID 954)
   [druid] 2019-06-11 09:41:25,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1430.0 (TID 953) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:41:25,028 [er for task 954] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1430.0 (TID 954). 810 bytes result sent to driver
   [druid] 2019-06-11 09:41:25,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1430.0 (TID 954) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:41:25,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1430.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1430 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:41:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:41:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:41:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1431)
   [druid] 2019-06-11 09:41:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:41:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1431 (MapPartitionsRDD[1674] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:25,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_716 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:25,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_716_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:25,030 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_716_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:25,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 716 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1431 (MapPartitionsRDD[1674] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:41:25,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1431.0 with 1 tasks
   [druid] 2019-06-11 09:41:25,031 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1431.0 (TID 955, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:25,031 [er for task 955] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1431.0 (TID 955)
   [druid] 2019-06-11 09:41:25,031 [er for task 955] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:25,031 [er for task 955] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:25,031 [er for task 955] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:25,031 [er for task 955] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:25,032 [er for task 955] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1431.0 (TID 955). 966 bytes result sent to driver
   [druid] 2019-06-11 09:41:25,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1431.0 (TID 955) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:25,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1431.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1431 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:41:25,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 477 finished: print at Transform.scala:36, took 0.025155 s
   [druid] 2019-06-11 09:41:25,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:25,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 476 is 83 bytes
   [druid] 2019-06-11 09:41:25,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 477 is 160 bytes
   [druid] 2019-06-11 09:41:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 478 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1434 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1432, ShuffleMapStage 1433)
   [druid] 2019-06-11 09:41:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:41:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1434 (MapPartitionsRDD[1674] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:25,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_717 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:25,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_717_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:25,037 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_717_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:25,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 717 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:25,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1434 (MapPartitionsRDD[1674] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:41:25,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1434.0 with 1 tasks
   [druid] 2019-06-11 09:41:25,038 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1434.0 (TID 956, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:25,038 [er for task 956] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1434.0 (TID 956)
   [druid] 2019-06-11 09:41:25,039 [er for task 956] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:25,039 [er for task 956] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:25,039 [er for task 956] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:25,039 [er for task 956] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:25,040 [er for task 956] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1434.0 (TID 956). 966 bytes result sent to driver
   [druid] 2019-06-11 09:41:25,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1434.0 (TID 956) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:25,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1434.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1434 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:41:25,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 478 finished: print at Transform.scala:36, took 0.005652 s
   [druid] 2019-06-11 09:41:25,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217285000 ms.0 from job set of time 1560217285000 ms
   [druid] 2019-06-11 09:41:25,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560217285000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:41:25,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1667 from persistence list
   [druid] 2019-06-11 09:41:25,041 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1667
   [druid] 2019-06-11 09:41:25,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1662 from persistence list
   [druid] 2019-06-11 09:41:25,041 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1662
   [druid] 2019-06-11 09:41:25,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1661 from persistence list
   [druid] 2019-06-11 09:41:25,041 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1661
   [druid] 2019-06-11 09:41:25,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1661] at socketTextStream at Transform.scala:18 of time 1560217285000 ms
   [druid] 2019-06-11 09:41:25,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217275000 ms
   [druid] 2019-06-11 09:41:25,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217275000 ms
   [druid] 2019-06-11 09:41:30,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217290000 ms
   [druid] 2019-06-11 09:41:30,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217290000 ms.0 from job set of time 1560217290000 ms
   [druid] 2019-06-11 09:41:30,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:41:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1676 (map at Transform.scala:20)
   [druid] 2019-06-11 09:41:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 479 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1437 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1435, ShuffleMapStage 1436)
   [druid] 2019-06-11 09:41:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1435)
   [druid] 2019-06-11 09:41:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1435 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:41:30,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_718 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:30,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_718_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:41:30,010 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_718_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 718 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1435 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:41:30,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1435.0 with 2 tasks
   [druid] 2019-06-11 09:41:30,011 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1435.0 (TID 957, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:30,011 [er for task 957] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1435.0 (TID 957)
   [druid] 2019-06-11 09:41:30,020 [er for task 957] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1435.0 (TID 957). 810 bytes result sent to driver
   [druid] 2019-06-11 09:41:30,020 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1435.0 (TID 958, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:30,020 [er for task 958] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1435.0 (TID 958)
   [druid] 2019-06-11 09:41:30,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1435.0 (TID 957) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:41:30,048 [er for task 958] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1435.0 (TID 958). 853 bytes result sent to driver
   [druid] 2019-06-11 09:41:30,048 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1435.0 (TID 958) in 28 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:41:30,048 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1435.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1435 (parallelize at Transform.scala:16) finished in 0.037 s
   [druid] 2019-06-11 09:41:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:41:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:41:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1437)
   [druid] 2019-06-11 09:41:30,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:41:30,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1437 (MapPartitionsRDD[1681] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:30,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_719 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:30,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_719_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:30,050 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_719_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,050 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 719 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:30,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1437 (MapPartitionsRDD[1681] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:41:30,050 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1437.0 with 1 tasks
   [druid] 2019-06-11 09:41:30,050 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1437.0 (TID 959, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:30,051 [er for task 959] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1437.0 (TID 959)
   [druid] 2019-06-11 09:41:30,051 [er for task 959] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:30,051 [er for task 959] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:30,051 [er for task 959] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:30,051 [er for task 959] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:30,052 [er for task 959] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1437.0 (TID 959). 966 bytes result sent to driver
   [druid] 2019-06-11 09:41:30,052 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1437.0 (TID 959) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:30,052 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1437.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:30,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1437 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:41:30,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 479 finished: print at Transform.scala:36, took 0.043846 s
   [druid] 2019-06-11 09:41:30,054 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:30,054 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 479 is 160 bytes
   [druid] 2019-06-11 09:41:30,054 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 478 is 83 bytes
   [druid] 2019-06-11 09:41:30,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 480 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:30,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1440 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:30,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1439, ShuffleMapStage 1438)
   [druid] 2019-06-11 09:41:30,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:41:30,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1440 (MapPartitionsRDD[1681] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:30,055 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_720 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:30,061 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 475
   [druid] 2019-06-11 09:41:30,062 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_720_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:30,062 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 472
   [druid] 2019-06-11 09:41:30,062 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 471
   [druid] 2019-06-11 09:41:30,062 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_720_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,063 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 720 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:30,064 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_713_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,064 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1440 (MapPartitionsRDD[1681] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:41:30,064 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1440.0 with 1 tasks
   [druid] 2019-06-11 09:41:30,064 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1440.0 (TID 960, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:30,064 [er for task 960] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1440.0 (TID 960)
   [druid] 2019-06-11 09:41:30,064 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_708_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,065 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 470
   [druid] 2019-06-11 09:41:30,065 [er for task 960] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:30,066 [er for task 960] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:41:30,066 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_710_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,066 [er for task 960] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:30,066 [er for task 960] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:30,066 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_718_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,067 [er for task 960] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1440.0 (TID 960). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:41:30,067 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_709_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,067 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1440.0 (TID 960) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:30,067 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1440.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:30,068 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1440 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:41:30,068 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 480 finished: print at Transform.scala:36, took 0.013377 s
   [druid] 2019-06-11 09:41:30,068 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217290000 ms.0 from job set of time 1560217290000 ms
   [druid] 2019-06-11 09:41:30,068 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1674 from persistence list
   [druid] 2019-06-11 09:41:30,068 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.068 s for time 1560217290000 ms (execution: 0.061 s)
   [druid] 2019-06-11 09:41:30,068 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_719_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,068 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1674
   [druid] 2019-06-11 09:41:30,068 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1669 from persistence list
   [druid] 2019-06-11 09:41:30,068 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1669
   [druid] 2019-06-11 09:41:30,068 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1668 from persistence list
   [druid] 2019-06-11 09:41:30,069 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 474
   [druid] 2019-06-11 09:41:30,069 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1668
   [druid] 2019-06-11 09:41:30,069 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1668] at socketTextStream at Transform.scala:18 of time 1560217290000 ms
   [druid] 2019-06-11 09:41:30,069 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 473
   [druid] 2019-06-11 09:41:30,069 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217280000 ms
   [druid] 2019-06-11 09:41:30,069 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 468
   [druid] 2019-06-11 09:41:30,069 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217280000 ms
   [druid] 2019-06-11 09:41:30,070 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_716_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,070 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_715_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,071 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_714_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,072 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 469
   [druid] 2019-06-11 09:41:30,072 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_712_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,073 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_717_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:30,074 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_711_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:35,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217295000 ms
   [druid] 2019-06-11 09:41:35,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217295000 ms.0 from job set of time 1560217295000 ms
   [druid] 2019-06-11 09:41:35,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:41:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1683 (map at Transform.scala:20)
   [druid] 2019-06-11 09:41:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 481 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1443 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1442, ShuffleMapStage 1441)
   [druid] 2019-06-11 09:41:35,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1441)
   [druid] 2019-06-11 09:41:35,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1441 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:41:35,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_721 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:35,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_721_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:41:35,011 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_721_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:35,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 721 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:35,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1441 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:41:35,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1441.0 with 2 tasks
   [druid] 2019-06-11 09:41:35,012 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1441.0 (TID 961, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:35,012 [er for task 961] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1441.0 (TID 961)
   [druid] 2019-06-11 09:41:35,022 [er for task 961] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1441.0 (TID 961). 810 bytes result sent to driver
   [druid] 2019-06-11 09:41:35,022 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1441.0 (TID 962, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:35,022 [er for task 962] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1441.0 (TID 962)
   [druid] 2019-06-11 09:41:35,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1441.0 (TID 961) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:41:35,031 [er for task 962] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1441.0 (TID 962). 853 bytes result sent to driver
   [druid] 2019-06-11 09:41:35,032 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1441.0 (TID 962) in 10 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:41:35,032 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1441.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1441 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:41:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:41:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:41:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1443)
   [druid] 2019-06-11 09:41:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:41:35,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1443 (MapPartitionsRDD[1688] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:35,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_722 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:35,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_722_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:35,033 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_722_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:35,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 722 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:35,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1443 (MapPartitionsRDD[1688] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:41:35,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1443.0 with 1 tasks
   [druid] 2019-06-11 09:41:35,034 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1443.0 (TID 963, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:35,034 [er for task 963] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1443.0 (TID 963)
   [druid] 2019-06-11 09:41:35,035 [er for task 963] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:35,035 [er for task 963] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:35,035 [er for task 963] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:35,035 [er for task 963] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:35,036 [er for task 963] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1443.0 (TID 963). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:41:35,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1443.0 (TID 963) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:35,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1443.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1443 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:41:35,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 481 finished: print at Transform.scala:36, took 0.027285 s
   [druid] 2019-06-11 09:41:35,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:35,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 481 is 160 bytes
   [druid] 2019-06-11 09:41:35,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 480 is 83 bytes
   [druid] 2019-06-11 09:41:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 482 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1446 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1445, ShuffleMapStage 1444)
   [druid] 2019-06-11 09:41:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:41:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1446 (MapPartitionsRDD[1688] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:35,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_723 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:35,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_723_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:35,041 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_723_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:35,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 723 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:35,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1446 (MapPartitionsRDD[1688] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:41:35,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1446.0 with 1 tasks
   [druid] 2019-06-11 09:41:35,042 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1446.0 (TID 964, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:35,042 [er for task 964] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1446.0 (TID 964)
   [druid] 2019-06-11 09:41:35,043 [er for task 964] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:35,043 [er for task 964] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:35,043 [er for task 964] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:35,043 [er for task 964] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:35,044 [er for task 964] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1446.0 (TID 964). 966 bytes result sent to driver
   [druid] 2019-06-11 09:41:35,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1446.0 (TID 964) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:35,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1446.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:35,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1446 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:41:35,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 482 finished: print at Transform.scala:36, took 0.004651 s
   [druid] 2019-06-11 09:41:35,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217295000 ms.0 from job set of time 1560217295000 ms
   [druid] 2019-06-11 09:41:35,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.044 s for time 1560217295000 ms (execution: 0.037 s)
   [druid] 2019-06-11 09:41:35,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1681 from persistence list
   [druid] 2019-06-11 09:41:35,045 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1681
   [druid] 2019-06-11 09:41:35,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1676 from persistence list
   [druid] 2019-06-11 09:41:35,045 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1676
   [druid] 2019-06-11 09:41:35,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1675 from persistence list
   [druid] 2019-06-11 09:41:35,045 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1675
   [druid] 2019-06-11 09:41:35,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1675] at socketTextStream at Transform.scala:18 of time 1560217295000 ms
   [druid] 2019-06-11 09:41:35,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217285000 ms
   [druid] 2019-06-11 09:41:35,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217285000 ms
   [druid] 2019-06-11 09:41:40,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217300000 ms
   [druid] 2019-06-11 09:41:40,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217300000 ms.0 from job set of time 1560217300000 ms
   [druid] 2019-06-11 09:41:40,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1690 (map at Transform.scala:20)
   [druid] 2019-06-11 09:41:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:41:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 483 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1449 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1447, ShuffleMapStage 1448)
   [druid] 2019-06-11 09:41:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1448)
   [druid] 2019-06-11 09:41:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1448 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:41:40,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_724 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:40,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_724_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:41:40,010 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_724_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:40,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 724 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1448 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:41:40,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1448.0 with 2 tasks
   [druid] 2019-06-11 09:41:40,011 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1448.0 (TID 965, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:40,011 [er for task 965] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1448.0 (TID 965)
   [druid] 2019-06-11 09:41:40,019 [er for task 965] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1448.0 (TID 965). 810 bytes result sent to driver
   [druid] 2019-06-11 09:41:40,020 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1448.0 (TID 966, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:40,020 [er for task 966] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1448.0 (TID 966)
   [druid] 2019-06-11 09:41:40,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1448.0 (TID 965) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:41:40,028 [er for task 966] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1448.0 (TID 966). 767 bytes result sent to driver
   [druid] 2019-06-11 09:41:40,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1448.0 (TID 966) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:41:40,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1448.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1448 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:41:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:41:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:41:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1449)
   [druid] 2019-06-11 09:41:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:41:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1449 (MapPartitionsRDD[1695] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:40,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_725 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:40,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_725_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:40,030 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_725_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:40,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 725 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1449 (MapPartitionsRDD[1695] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:41:40,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1449.0 with 1 tasks
   [druid] 2019-06-11 09:41:40,030 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1449.0 (TID 967, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:40,031 [er for task 967] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1449.0 (TID 967)
   [druid] 2019-06-11 09:41:40,032 [er for task 967] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:40,032 [er for task 967] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:40,032 [er for task 967] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:40,032 [er for task 967] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:40,032 [er for task 967] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1449.0 (TID 967). 966 bytes result sent to driver
   [druid] 2019-06-11 09:41:40,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1449.0 (TID 967) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:40,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1449.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:40,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1449 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:41:40,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 483 finished: print at Transform.scala:36, took 0.024836 s
   [druid] 2019-06-11 09:41:40,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:40,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 482 is 83 bytes
   [druid] 2019-06-11 09:41:40,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 483 is 160 bytes
   [druid] 2019-06-11 09:41:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 484 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1452 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:40,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1450, ShuffleMapStage 1451)
   [druid] 2019-06-11 09:41:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:41:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1452 (MapPartitionsRDD[1695] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:40,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_726 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:40,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_726_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:40,038 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_726_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:40,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 726 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1452 (MapPartitionsRDD[1695] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:41:40,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1452.0 with 1 tasks
   [druid] 2019-06-11 09:41:40,038 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1452.0 (TID 968, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:40,038 [er for task 968] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1452.0 (TID 968)
   [druid] 2019-06-11 09:41:40,039 [er for task 968] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:40,039 [er for task 968] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:40,039 [er for task 968] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:40,039 [er for task 968] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:40,040 [er for task 968] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1452.0 (TID 968). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:41:40,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1452.0 (TID 968) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:40,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1452.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:40,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1452 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:41:40,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 484 finished: print at Transform.scala:36, took 0.005516 s
   [druid] 2019-06-11 09:41:40,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217300000 ms.0 from job set of time 1560217300000 ms
   [druid] 2019-06-11 09:41:40,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560217300000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:41:40,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1688 from persistence list
   [druid] 2019-06-11 09:41:40,041 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1688
   [druid] 2019-06-11 09:41:40,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1683 from persistence list
   [druid] 2019-06-11 09:41:40,041 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1683
   [druid] 2019-06-11 09:41:40,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1682 from persistence list
   [druid] 2019-06-11 09:41:40,041 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1682
   [druid] 2019-06-11 09:41:40,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1682] at socketTextStream at Transform.scala:18 of time 1560217300000 ms
   [druid] 2019-06-11 09:41:40,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217290000 ms
   [druid] 2019-06-11 09:41:40,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217290000 ms
   [druid] 2019-06-11 09:41:45,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217305000 ms
   [druid] 2019-06-11 09:41:45,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217305000 ms.0 from job set of time 1560217305000 ms
   [druid] 2019-06-11 09:41:45,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1697 (map at Transform.scala:20)
   [druid] 2019-06-11 09:41:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:41:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 485 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1455 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1453, ShuffleMapStage 1454)
   [druid] 2019-06-11 09:41:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1454)
   [druid] 2019-06-11 09:41:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1454 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:41:45,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_727 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:45,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_727_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:41:45,011 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_727_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:45,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 727 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:45,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1454 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:41:45,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1454.0 with 2 tasks
   [druid] 2019-06-11 09:41:45,012 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1454.0 (TID 969, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:45,012 [er for task 969] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1454.0 (TID 969)
   [druid] 2019-06-11 09:41:45,028 [er for task 969] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1454.0 (TID 969). 853 bytes result sent to driver
   [druid] 2019-06-11 09:41:45,029 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1454.0 (TID 970, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:45,029 [er for task 970] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1454.0 (TID 970)
   [druid] 2019-06-11 09:41:45,029 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1454.0 (TID 969) in 17 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:41:45,040 [er for task 970] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1454.0 (TID 970). 853 bytes result sent to driver
   [druid] 2019-06-11 09:41:45,040 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1454.0 (TID 970) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:41:45,040 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1454.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1454 (parallelize at Transform.scala:16) finished in 0.028 s
   [druid] 2019-06-11 09:41:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:41:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:41:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1455)
   [druid] 2019-06-11 09:41:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:41:45,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1455 (MapPartitionsRDD[1702] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:45,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_728 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:45,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_728_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:45,042 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_728_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:45,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 728 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:45,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1455 (MapPartitionsRDD[1702] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:41:45,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1455.0 with 1 tasks
   [druid] 2019-06-11 09:41:45,043 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1455.0 (TID 971, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:45,043 [er for task 971] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1455.0 (TID 971)
   [druid] 2019-06-11 09:41:45,043 [er for task 971] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:45,043 [er for task 971] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:45,043 [er for task 971] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:45,043 [er for task 971] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:45,044 [er for task 971] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1455.0 (TID 971). 966 bytes result sent to driver
   [druid] 2019-06-11 09:41:45,044 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1455.0 (TID 971) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:45,044 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1455.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:45,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1455 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:41:45,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 485 finished: print at Transform.scala:36, took 0.035635 s
   [druid] 2019-06-11 09:41:45,048 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:45,048 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 484 is 83 bytes
   [druid] 2019-06-11 09:41:45,049 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 485 is 160 bytes
   [druid] 2019-06-11 09:41:45,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 486 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:45,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1458 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:45,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1457, ShuffleMapStage 1456)
   [druid] 2019-06-11 09:41:45,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:41:45,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1458 (MapPartitionsRDD[1702] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:45,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_729 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:45,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_729_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:45,051 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_729_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:45,051 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 729 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:45,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1458 (MapPartitionsRDD[1702] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:41:45,051 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1458.0 with 1 tasks
   [druid] 2019-06-11 09:41:45,051 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1458.0 (TID 972, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:45,051 [er for task 972] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1458.0 (TID 972)
   [druid] 2019-06-11 09:41:45,052 [er for task 972] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:45,052 [er for task 972] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:45,052 [er for task 972] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:45,052 [er for task 972] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:45,053 [er for task 972] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1458.0 (TID 972). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:41:45,053 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1458.0 (TID 972) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:45,053 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1458.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:45,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1458 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:41:45,054 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 486 finished: print at Transform.scala:36, took 0.005484 s
   [druid] 2019-06-11 09:41:45,054 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217305000 ms.0 from job set of time 1560217305000 ms
   [druid] 2019-06-11 09:41:45,054 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.054 s for time 1560217305000 ms (execution: 0.047 s)
   [druid] 2019-06-11 09:41:45,054 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1695 from persistence list
   [druid] 2019-06-11 09:41:45,054 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1695
   [druid] 2019-06-11 09:41:45,054 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1690 from persistence list
   [druid] 2019-06-11 09:41:45,054 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1690
   [druid] 2019-06-11 09:41:45,054 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1689 from persistence list
   [druid] 2019-06-11 09:41:45,054 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1689
   [druid] 2019-06-11 09:41:45,054 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1689] at socketTextStream at Transform.scala:18 of time 1560217305000 ms
   [druid] 2019-06-11 09:41:45,054 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217295000 ms
   [druid] 2019-06-11 09:41:45,055 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217295000 ms
   [druid] 2019-06-11 09:41:50,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217310000 ms
   [druid] 2019-06-11 09:41:50,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217310000 ms.0 from job set of time 1560217310000 ms
   [druid] 2019-06-11 09:41:50,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1704 (map at Transform.scala:20)
   [druid] 2019-06-11 09:41:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:41:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 487 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1461 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1460, ShuffleMapStage 1459)
   [druid] 2019-06-11 09:41:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1460)
   [druid] 2019-06-11 09:41:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1460 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:41:50,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_730 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:50,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_730_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:41:50,010 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_730_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 730 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:50,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1460 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:41:50,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1460.0 with 2 tasks
   [druid] 2019-06-11 09:41:50,010 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1460.0 (TID 973, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:50,010 [er for task 973] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1460.0 (TID 973)
   [druid] 2019-06-11 09:41:50,024 [er for task 973] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1460.0 (TID 973). 810 bytes result sent to driver
   [druid] 2019-06-11 09:41:50,024 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1460.0 (TID 974, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:50,024 [er for task 974] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1460.0 (TID 974)
   [druid] 2019-06-11 09:41:50,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1460.0 (TID 973) in 14 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:41:50,032 [er for task 974] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1460.0 (TID 974). 853 bytes result sent to driver
   [druid] 2019-06-11 09:41:50,032 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1460.0 (TID 974) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:41:50,032 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1460.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:50,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1460 (parallelize at Transform.scala:16) finished in 0.022 s
   [druid] 2019-06-11 09:41:50,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:41:50,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:41:50,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1461)
   [druid] 2019-06-11 09:41:50,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:41:50,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1461 (MapPartitionsRDD[1709] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:50,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_731 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:50,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_731_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:41:50,034 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_731_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 731 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1461 (MapPartitionsRDD[1709] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:41:50,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1461.0 with 1 tasks
   [druid] 2019-06-11 09:41:50,034 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1461.0 (TID 975, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:50,034 [er for task 975] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1461.0 (TID 975)
   [druid] 2019-06-11 09:41:50,035 [er for task 975] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:50,035 [er for task 975] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:50,035 [er for task 975] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:50,035 [er for task 975] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:50,035 [er for task 975] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1461.0 (TID 975). 966 bytes result sent to driver
   [druid] 2019-06-11 09:41:50,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1461.0 (TID 975) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:50,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1461.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1461 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:41:50,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 487 finished: print at Transform.scala:36, took 0.028167 s
   [druid] 2019-06-11 09:41:50,041 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_724_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,042 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 479
   [druid] 2019-06-11 09:41:50,043 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_727_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 481
   [druid] 2019-06-11 09:41:50,044 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_731_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,045 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:50,045 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_726_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,046 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 476
   [druid] 2019-06-11 09:41:50,046 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 486 is 83 bytes
   [druid] 2019-06-11 09:41:50,046 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 487 is 160 bytes
   [druid] 2019-06-11 09:41:50,046 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_721_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 488 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:50,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1464 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:50,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1462, ShuffleMapStage 1463)
   [druid] 2019-06-11 09:41:50,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:41:50,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1464 (MapPartitionsRDD[1709] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:50,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_732 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:50,047 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_728_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,048 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_723_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_732_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:50,049 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_732_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,049 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 732 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:50,049 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_725_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1464 (MapPartitionsRDD[1709] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:41:50,049 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1464.0 with 1 tasks
   [druid] 2019-06-11 09:41:50,049 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1464.0 (TID 976, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:50,050 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 483
   [druid] 2019-06-11 09:41:50,050 [er for task 976] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1464.0 (TID 976)
   [druid] 2019-06-11 09:41:50,050 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 482
   [druid] 2019-06-11 09:41:50,051 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_729_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,051 [er for task 976] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:50,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 477
   [druid] 2019-06-11 09:41:50,052 [er for task 976] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:41:50,052 [er for task 976] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:50,052 [er for task 976] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:50,052 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_730_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,053 [er for task 976] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1464.0 (TID 976). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:41:50,053 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1464.0 (TID 976) in 4 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:50,053 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_720_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,053 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1464.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:50,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1464 (print at Transform.scala:36) finished in 0.005 s
   [druid] 2019-06-11 09:41:50,054 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 478
   [druid] 2019-06-11 09:41:50,054 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 488 finished: print at Transform.scala:36, took 0.008751 s
   [druid] 2019-06-11 09:41:50,054 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 480
   [druid] 2019-06-11 09:41:50,055 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217310000 ms.0 from job set of time 1560217310000 ms
   [druid] 2019-06-11 09:41:50,055 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.055 s for time 1560217310000 ms (execution: 0.050 s)
   [druid] 2019-06-11 09:41:50,055 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1702 from persistence list
   [druid] 2019-06-11 09:41:50,055 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1702
   [druid] 2019-06-11 09:41:50,055 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1697 from persistence list
   [druid] 2019-06-11 09:41:50,055 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1697
   [druid] 2019-06-11 09:41:50,055 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1696 from persistence list
   [druid] 2019-06-11 09:41:50,055 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_722_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:50,055 [-thread-pool-19] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1696
   [druid] 2019-06-11 09:41:50,055 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1696] at socketTextStream at Transform.scala:18 of time 1560217310000 ms
   [druid] 2019-06-11 09:41:50,056 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217300000 ms
   [druid] 2019-06-11 09:41:50,056 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217300000 ms
   [druid] 2019-06-11 09:41:55,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217315000 ms
   [druid] 2019-06-11 09:41:55,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217315000 ms.0 from job set of time 1560217315000 ms
   [druid] 2019-06-11 09:41:55,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:41:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1711 (map at Transform.scala:20)
   [druid] 2019-06-11 09:41:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 489 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1467 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1465, ShuffleMapStage 1466)
   [druid] 2019-06-11 09:41:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1465)
   [druid] 2019-06-11 09:41:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1465 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:41:55,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_733 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:55,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_733_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:41:55,009 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_733_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:55,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 733 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:55,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1465 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:41:55,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1465.0 with 2 tasks
   [druid] 2019-06-11 09:41:55,010 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1465.0 (TID 977, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:55,010 [er for task 977] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1465.0 (TID 977)
   [druid] 2019-06-11 09:41:55,019 [er for task 977] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1465.0 (TID 977). 810 bytes result sent to driver
   [druid] 2019-06-11 09:41:55,019 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1465.0 (TID 978, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:41:55,019 [er for task 978] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1465.0 (TID 978)
   [druid] 2019-06-11 09:41:55,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1465.0 (TID 977) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:41:55,028 [er for task 978] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1465.0 (TID 978). 810 bytes result sent to driver
   [druid] 2019-06-11 09:41:55,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1465.0 (TID 978) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:41:55,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1465.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1465 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:41:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:41:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:41:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1467)
   [druid] 2019-06-11 09:41:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:41:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1467 (MapPartitionsRDD[1716] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:55,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_734 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:55,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_734_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:55,030 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_734_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:55,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 734 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:55,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1467 (MapPartitionsRDD[1716] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:41:55,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1467.0 with 1 tasks
   [druid] 2019-06-11 09:41:55,031 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1467.0 (TID 979, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:55,031 [er for task 979] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1467.0 (TID 979)
   [druid] 2019-06-11 09:41:55,032 [er for task 979] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:55,032 [er for task 979] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:55,032 [er for task 979] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:55,032 [er for task 979] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:55,033 [er for task 979] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1467.0 (TID 979). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:41:55,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1467.0 (TID 979) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:55,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1467.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1467 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:41:55,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 489 finished: print at Transform.scala:36, took 0.026390 s
   [druid] 2019-06-11 09:41:55,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:41:55,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 489 is 160 bytes
   [druid] 2019-06-11 09:41:55,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 488 is 83 bytes
   [druid] 2019-06-11 09:41:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 490 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:41:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1470 (print at Transform.scala:36)
   [druid] 2019-06-11 09:41:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1468, ShuffleMapStage 1469)
   [druid] 2019-06-11 09:41:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:41:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1470 (MapPartitionsRDD[1716] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:41:55,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_735 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:55,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_735_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:41:55,039 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_735_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:41:55,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 735 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:41:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1470 (MapPartitionsRDD[1716] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:41:55,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1470.0 with 1 tasks
   [druid] 2019-06-11 09:41:55,039 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1470.0 (TID 980, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:41:55,039 [er for task 980] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1470.0 (TID 980)
   [druid] 2019-06-11 09:41:55,040 [er for task 980] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:41:55,040 [er for task 980] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:55,040 [er for task 980] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:41:55,040 [er for task 980] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:41:55,041 [er for task 980] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1470.0 (TID 980). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:41:55,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1470.0 (TID 980) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:41:55,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1470.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:41:55,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1470 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:41:55,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 490 finished: print at Transform.scala:36, took 0.005477 s
   [druid] 2019-06-11 09:41:55,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217315000 ms.0 from job set of time 1560217315000 ms
   [druid] 2019-06-11 09:41:55,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560217315000 ms (execution: 0.036 s)
   [druid] 2019-06-11 09:41:55,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1709 from persistence list
   [druid] 2019-06-11 09:41:55,042 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1709
   [druid] 2019-06-11 09:41:55,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1704 from persistence list
   [druid] 2019-06-11 09:41:55,042 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1704
   [druid] 2019-06-11 09:41:55,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1703 from persistence list
   [druid] 2019-06-11 09:41:55,042 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1703
   [druid] 2019-06-11 09:41:55,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1703] at socketTextStream at Transform.scala:18 of time 1560217315000 ms
   [druid] 2019-06-11 09:41:55,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217305000 ms
   [druid] 2019-06-11 09:41:55,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217305000 ms
   [druid] 2019-06-11 09:42:00,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217320000 ms
   [druid] 2019-06-11 09:42:00,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217320000 ms.0 from job set of time 1560217320000 ms
   [druid] 2019-06-11 09:42:00,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1718 (map at Transform.scala:20)
   [druid] 2019-06-11 09:42:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:42:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 491 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1473 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1471, ShuffleMapStage 1472)
   [druid] 2019-06-11 09:42:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1472)
   [druid] 2019-06-11 09:42:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1472 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:42:00,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_736 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:00,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_736_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:42:00,011 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_736_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:00,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 736 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1472 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:42:00,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1472.0 with 2 tasks
   [druid] 2019-06-11 09:42:00,012 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1472.0 (TID 981, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:00,012 [er for task 981] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1472.0 (TID 981)
   [druid] 2019-06-11 09:42:00,021 [er for task 981] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1472.0 (TID 981). 853 bytes result sent to driver
   [druid] 2019-06-11 09:42:00,021 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1472.0 (TID 982, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:00,022 [er for task 982] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1472.0 (TID 982)
   [druid] 2019-06-11 09:42:00,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1472.0 (TID 981) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:42:00,029 [er for task 982] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1472.0 (TID 982). 767 bytes result sent to driver
   [druid] 2019-06-11 09:42:00,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1472.0 (TID 982) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:42:00,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1472.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1472 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:42:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:42:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:42:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1473)
   [druid] 2019-06-11 09:42:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:42:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1473 (MapPartitionsRDD[1723] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:00,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_737 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:00,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_737_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:00,031 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_737_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:00,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 737 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:00,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1473 (MapPartitionsRDD[1723] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:42:00,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1473.0 with 1 tasks
   [druid] 2019-06-11 09:42:00,032 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1473.0 (TID 983, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:00,032 [er for task 983] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1473.0 (TID 983)
   [druid] 2019-06-11 09:42:00,032 [er for task 983] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:00,032 [er for task 983] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:00,032 [er for task 983] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:00,032 [er for task 983] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:00,033 [er for task 983] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1473.0 (TID 983). 966 bytes result sent to driver
   [druid] 2019-06-11 09:42:00,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1473.0 (TID 983) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:00,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1473.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1473 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:42:00,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 491 finished: print at Transform.scala:36, took 0.024685 s
   [druid] 2019-06-11 09:42:00,037 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:00,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 490 is 83 bytes
   [druid] 2019-06-11 09:42:00,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 491 is 160 bytes
   [druid] 2019-06-11 09:42:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 492 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1476 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1475, ShuffleMapStage 1474)
   [druid] 2019-06-11 09:42:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:42:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1476 (MapPartitionsRDD[1723] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:00,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_738 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:00,039 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_738_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:00,039 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_738_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:00,039 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 738 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1476 (MapPartitionsRDD[1723] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:42:00,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1476.0 with 1 tasks
   [druid] 2019-06-11 09:42:00,039 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1476.0 (TID 984, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:00,040 [er for task 984] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1476.0 (TID 984)
   [druid] 2019-06-11 09:42:00,040 [er for task 984] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:00,040 [er for task 984] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:00,040 [er for task 984] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:00,040 [er for task 984] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:00,041 [er for task 984] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1476.0 (TID 984). 966 bytes result sent to driver
   [druid] 2019-06-11 09:42:00,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1476.0 (TID 984) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:00,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1476.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:00,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1476 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:00,042 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 492 finished: print at Transform.scala:36, took 0.004601 s
   [druid] 2019-06-11 09:42:00,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217320000 ms.0 from job set of time 1560217320000 ms
   [druid] 2019-06-11 09:42:00,042 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.042 s for time 1560217320000 ms (execution: 0.035 s)
   [druid] 2019-06-11 09:42:00,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1716 from persistence list
   [druid] 2019-06-11 09:42:00,042 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1716
   [druid] 2019-06-11 09:42:00,042 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1711 from persistence list
   [druid] 2019-06-11 09:42:00,042 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1711
   [druid] 2019-06-11 09:42:00,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1710 from persistence list
   [druid] 2019-06-11 09:42:00,042 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1710
   [druid] 2019-06-11 09:42:00,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1710] at socketTextStream at Transform.scala:18 of time 1560217320000 ms
   [druid] 2019-06-11 09:42:00,043 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217310000 ms
   [druid] 2019-06-11 09:42:00,043 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217310000 ms
   [druid] 2019-06-11 09:42:05,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217325000 ms
   [druid] 2019-06-11 09:42:05,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217325000 ms.0 from job set of time 1560217325000 ms
   [druid] 2019-06-11 09:42:05,006 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:42:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1725 (map at Transform.scala:20)
   [druid] 2019-06-11 09:42:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 493 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1479 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1478, ShuffleMapStage 1477)
   [druid] 2019-06-11 09:42:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1477)
   [druid] 2019-06-11 09:42:05,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1477 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:42:05,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_739 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:05,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_739_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:42:05,009 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_739_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:05,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 739 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:05,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1477 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:42:05,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1477.0 with 2 tasks
   [druid] 2019-06-11 09:42:05,009 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1477.0 (TID 985, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:05,009 [er for task 985] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1477.0 (TID 985)
   [druid] 2019-06-11 09:42:05,018 [er for task 985] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1477.0 (TID 985). 853 bytes result sent to driver
   [druid] 2019-06-11 09:42:05,018 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1477.0 (TID 986, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:05,018 [er for task 986] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1477.0 (TID 986)
   [druid] 2019-06-11 09:42:05,018 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1477.0 (TID 985) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:42:05,026 [er for task 986] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1477.0 (TID 986). 810 bytes result sent to driver
   [druid] 2019-06-11 09:42:05,026 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1477.0 (TID 986) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:42:05,026 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1477.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1477 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:42:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:42:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:42:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1479)
   [druid] 2019-06-11 09:42:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:42:05,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1479 (MapPartitionsRDD[1730] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:05,027 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_740 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:05,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_740_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:05,028 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_740_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:05,028 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 740 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1479 (MapPartitionsRDD[1730] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:42:05,028 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1479.0 with 1 tasks
   [druid] 2019-06-11 09:42:05,029 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1479.0 (TID 987, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:05,029 [er for task 987] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1479.0 (TID 987)
   [druid] 2019-06-11 09:42:05,029 [er for task 987] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:05,029 [er for task 987] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:05,029 [er for task 987] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:05,029 [er for task 987] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:05,030 [er for task 987] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1479.0 (TID 987). 966 bytes result sent to driver
   [druid] 2019-06-11 09:42:05,030 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1479.0 (TID 987) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:05,030 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1479.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:05,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1479 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:05,030 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 493 finished: print at Transform.scala:36, took 0.023504 s
   [druid] 2019-06-11 09:42:05,032 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:05,032 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 493 is 160 bytes
   [druid] 2019-06-11 09:42:05,032 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 492 is 83 bytes
   [druid] 2019-06-11 09:42:05,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 494 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:05,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1482 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:05,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1480, ShuffleMapStage 1481)
   [druid] 2019-06-11 09:42:05,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:42:05,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1482 (MapPartitionsRDD[1730] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:05,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_741 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:05,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_741_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:05,034 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_741_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:05,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 741 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:05,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1482 (MapPartitionsRDD[1730] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:42:05,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1482.0 with 1 tasks
   [druid] 2019-06-11 09:42:05,035 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1482.0 (TID 988, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:05,035 [er for task 988] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1482.0 (TID 988)
   [druid] 2019-06-11 09:42:05,035 [er for task 988] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:05,035 [er for task 988] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:05,035 [er for task 988] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:05,035 [er for task 988] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:05,036 [er for task 988] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1482.0 (TID 988). 966 bytes result sent to driver
   [druid] 2019-06-11 09:42:05,036 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1482.0 (TID 988) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:05,036 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1482.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:05,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1482 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:05,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 494 finished: print at Transform.scala:36, took 0.004282 s
   [druid] 2019-06-11 09:42:05,037 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217325000 ms.0 from job set of time 1560217325000 ms
   [druid] 2019-06-11 09:42:05,037 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.037 s for time 1560217325000 ms (execution: 0.032 s)
   [druid] 2019-06-11 09:42:05,037 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1723 from persistence list
   [druid] 2019-06-11 09:42:05,037 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1723
   [druid] 2019-06-11 09:42:05,037 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1718 from persistence list
   [druid] 2019-06-11 09:42:05,037 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1718
   [druid] 2019-06-11 09:42:05,037 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1717 from persistence list
   [druid] 2019-06-11 09:42:05,038 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1717
   [druid] 2019-06-11 09:42:05,038 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1717] at socketTextStream at Transform.scala:18 of time 1560217325000 ms
   [druid] 2019-06-11 09:42:05,038 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217315000 ms
   [druid] 2019-06-11 09:42:05,038 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217315000 ms
   [druid] 2019-06-11 09:42:10,004 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217330000 ms
   [druid] 2019-06-11 09:42:10,004 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217330000 ms.0 from job set of time 1560217330000 ms
   [druid] 2019-06-11 09:42:10,005 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:10,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:42:10,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1732 (map at Transform.scala:20)
   [druid] 2019-06-11 09:42:10,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 495 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:10,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1485 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:10,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1483, ShuffleMapStage 1484)
   [druid] 2019-06-11 09:42:10,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1483)
   [druid] 2019-06-11 09:42:10,006 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1483 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:42:10,006 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_742 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:10,007 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_742_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:42:10,007 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_742_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,007 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 742 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:10,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1483 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:42:10,008 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1483.0 with 2 tasks
   [druid] 2019-06-11 09:42:10,008 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1483.0 (TID 989, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:10,008 [er for task 989] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1483.0 (TID 989)
   [druid] 2019-06-11 09:42:10,018 [er for task 989] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1483.0 (TID 989). 853 bytes result sent to driver
   [druid] 2019-06-11 09:42:10,018 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1483.0 (TID 990, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:10,018 [er for task 990] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1483.0 (TID 990)
   [druid] 2019-06-11 09:42:10,018 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1483.0 (TID 989) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:42:10,032 [er for task 990] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1483.0 (TID 990). 810 bytes result sent to driver
   [druid] 2019-06-11 09:42:10,032 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1483.0 (TID 990) in 14 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:42:10,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1483.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1483 (parallelize at Transform.scala:16) finished in 0.025 s
   [druid] 2019-06-11 09:42:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:42:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:42:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1485)
   [druid] 2019-06-11 09:42:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:42:10,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1485 (MapPartitionsRDD[1737] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:10,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_743 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:10,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_743_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:10,034 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_743_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 743 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:10,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1485 (MapPartitionsRDD[1737] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:42:10,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1485.0 with 1 tasks
   [druid] 2019-06-11 09:42:10,035 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1485.0 (TID 991, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:10,035 [er for task 991] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1485.0 (TID 991)
   [druid] 2019-06-11 09:42:10,036 [er for task 991] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:10,036 [er for task 991] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:10,036 [er for task 991] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:10,036 [er for task 991] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:10,037 [er for task 991] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1485.0 (TID 991). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:42:10,037 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1485.0 (TID 991) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:10,037 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1485.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:10,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1485 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:10,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 495 finished: print at Transform.scala:36, took 0.031643 s
   [druid] 2019-06-11 09:42:10,043 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_734_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 491
   [druid] 2019-06-11 09:42:10,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 484
   [druid] 2019-06-11 09:42:10,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 487
   [druid] 2019-06-11 09:42:10,045 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_736_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,045 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 486
   [druid] 2019-06-11 09:42:10,045 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:10,046 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_732_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,046 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 495 is 160 bytes
   [druid] 2019-06-11 09:42:10,046 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 494 is 83 bytes
   [druid] 2019-06-11 09:42:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 496 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:10,046 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_742_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1488 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1486, ShuffleMapStage 1487)
   [druid] 2019-06-11 09:42:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:42:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1488 (MapPartitionsRDD[1737] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:10,047 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_737_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,047 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_744 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:10,048 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_733_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_744_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:10,049 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_744_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,049 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_740_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,049 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 744 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:10,049 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 488
   [druid] 2019-06-11 09:42:10,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1488 (MapPartitionsRDD[1737] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:42:10,050 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1488.0 with 1 tasks
   [druid] 2019-06-11 09:42:10,050 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1488.0 (TID 992, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:10,050 [er for task 992] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1488.0 (TID 992)
   [druid] 2019-06-11 09:42:10,050 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_735_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,050 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 485
   [druid] 2019-06-11 09:42:10,050 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_739_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 489
   [druid] 2019-06-11 09:42:10,051 [er for task 992] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:10,051 [er for task 992] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:10,051 [er for task 992] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:10,051 [er for task 992] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:10,052 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_741_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,052 [er for task 992] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1488.0 (TID 992). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:42:10,053 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1488.0 (TID 992) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:10,053 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1488.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:10,053 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_738_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,053 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1488 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:42:10,053 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 496 finished: print at Transform.scala:36, took 0.007306 s
   [druid] 2019-06-11 09:42:10,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 490
   [druid] 2019-06-11 09:42:10,053 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217330000 ms.0 from job set of time 1560217330000 ms
   [druid] 2019-06-11 09:42:10,053 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.053 s for time 1560217330000 ms (execution: 0.049 s)
   [druid] 2019-06-11 09:42:10,053 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1730 from persistence list
   [druid] 2019-06-11 09:42:10,054 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1730
   [druid] 2019-06-11 09:42:10,054 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1725 from persistence list
   [druid] 2019-06-11 09:42:10,054 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1725
   [druid] 2019-06-11 09:42:10,054 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1724 from persistence list
   [druid] 2019-06-11 09:42:10,054 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1724
   [druid] 2019-06-11 09:42:10,054 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1724] at socketTextStream at Transform.scala:18 of time 1560217330000 ms
   [druid] 2019-06-11 09:42:10,054 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_743_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:10,054 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217320000 ms
   [druid] 2019-06-11 09:42:10,057 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217320000 ms
   [druid] 2019-06-11 09:42:15,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217335000 ms
   [druid] 2019-06-11 09:42:15,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217335000 ms.0 from job set of time 1560217335000 ms
   [druid] 2019-06-11 09:42:15,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:42:15,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1739 (map at Transform.scala:20)
   [druid] 2019-06-11 09:42:15,020 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 497 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:15,020 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1491 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:15,020 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1489, ShuffleMapStage 1490)
   [druid] 2019-06-11 09:42:15,020 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1489)
   [druid] 2019-06-11 09:42:15,020 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1489 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:42:15,021 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_745 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:15,022 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_745_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:42:15,022 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_745_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:15,022 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 745 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:15,022 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1489 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:42:15,022 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1489.0 with 2 tasks
   [druid] 2019-06-11 09:42:15,023 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1489.0 (TID 993, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:15,023 [er for task 993] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1489.0 (TID 993)
   [druid] 2019-06-11 09:42:15,031 [er for task 993] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1489.0 (TID 993). 810 bytes result sent to driver
   [druid] 2019-06-11 09:42:15,031 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1489.0 (TID 994, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:15,031 [er for task 994] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1489.0 (TID 994)
   [druid] 2019-06-11 09:42:15,031 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1489.0 (TID 993) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:42:15,040 [er for task 994] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1489.0 (TID 994). 767 bytes result sent to driver
   [druid] 2019-06-11 09:42:15,040 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1489.0 (TID 994) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:42:15,040 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1489.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1489 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:42:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:42:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:42:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1491)
   [druid] 2019-06-11 09:42:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:42:15,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1491 (MapPartitionsRDD[1744] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:15,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_746 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:15,042 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_746_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:15,042 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_746_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:15,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 746 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:15,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1491 (MapPartitionsRDD[1744] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:42:15,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1491.0 with 1 tasks
   [druid] 2019-06-11 09:42:15,043 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1491.0 (TID 995, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:15,043 [er for task 995] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1491.0 (TID 995)
   [druid] 2019-06-11 09:42:15,043 [er for task 995] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:15,043 [er for task 995] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:15,044 [er for task 995] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:15,044 [er for task 995] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:15,044 [er for task 995] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1491.0 (TID 995). 966 bytes result sent to driver
   [druid] 2019-06-11 09:42:15,044 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1491.0 (TID 995) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:15,044 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1491.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:15,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1491 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:15,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 497 finished: print at Transform.scala:36, took 0.035525 s
   [druid] 2019-06-11 09:42:15,046 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:15,047 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 497 is 160 bytes
   [druid] 2019-06-11 09:42:15,047 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 496 is 83 bytes
   [druid] 2019-06-11 09:42:15,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 498 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:15,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1494 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:15,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1493, ShuffleMapStage 1492)
   [druid] 2019-06-11 09:42:15,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:42:15,047 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1494 (MapPartitionsRDD[1744] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:15,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_747 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:15,049 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_747_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:15,049 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_747_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:15,050 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 747 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:15,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1494 (MapPartitionsRDD[1744] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:42:15,050 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1494.0 with 1 tasks
   [druid] 2019-06-11 09:42:15,050 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1494.0 (TID 996, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:15,050 [er for task 996] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1494.0 (TID 996)
   [druid] 2019-06-11 09:42:15,051 [er for task 996] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:15,051 [er for task 996] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:15,051 [er for task 996] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:15,051 [er for task 996] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:15,052 [er for task 996] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1494.0 (TID 996). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:42:15,052 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1494.0 (TID 996) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:15,052 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1494.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:15,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1494 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:15,052 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 498 finished: print at Transform.scala:36, took 0.006320 s
   [druid] 2019-06-11 09:42:15,053 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217335000 ms.0 from job set of time 1560217335000 ms
   [druid] 2019-06-11 09:42:15,053 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.053 s for time 1560217335000 ms (execution: 0.047 s)
   [druid] 2019-06-11 09:42:15,053 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1737 from persistence list
   [druid] 2019-06-11 09:42:15,053 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1737
   [druid] 2019-06-11 09:42:15,053 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1732 from persistence list
   [druid] 2019-06-11 09:42:15,053 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1732
   [druid] 2019-06-11 09:42:15,053 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1731 from persistence list
   [druid] 2019-06-11 09:42:15,053 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1731
   [druid] 2019-06-11 09:42:15,053 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1731] at socketTextStream at Transform.scala:18 of time 1560217335000 ms
   [druid] 2019-06-11 09:42:15,053 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217325000 ms
   [druid] 2019-06-11 09:42:15,053 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217325000 ms
   [druid] 2019-06-11 09:42:20,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217340000 ms
   [druid] 2019-06-11 09:42:20,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217340000 ms.0 from job set of time 1560217340000 ms
   [druid] 2019-06-11 09:42:20,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1746 (map at Transform.scala:20)
   [druid] 2019-06-11 09:42:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:42:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 499 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1497 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1496, ShuffleMapStage 1495)
   [druid] 2019-06-11 09:42:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1496)
   [druid] 2019-06-11 09:42:20,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1496 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:42:20,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_748 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:20,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_748_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:42:20,009 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_748_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:20,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 748 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1496 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:42:20,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1496.0 with 2 tasks
   [druid] 2019-06-11 09:42:20,010 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1496.0 (TID 997, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:20,010 [er for task 997] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1496.0 (TID 997)
   [druid] 2019-06-11 09:42:20,021 [er for task 997] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1496.0 (TID 997). 810 bytes result sent to driver
   [druid] 2019-06-11 09:42:20,022 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1496.0 (TID 998, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:20,022 [er for task 998] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1496.0 (TID 998)
   [druid] 2019-06-11 09:42:20,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1496.0 (TID 997) in 12 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:42:20,031 [er for task 998] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1496.0 (TID 998). 810 bytes result sent to driver
   [druid] 2019-06-11 09:42:20,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1496.0 (TID 998) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:42:20,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1496.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1496 (parallelize at Transform.scala:16) finished in 0.021 s
   [druid] 2019-06-11 09:42:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:42:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:42:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1497)
   [druid] 2019-06-11 09:42:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:42:20,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1497 (MapPartitionsRDD[1751] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:20,032 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_749 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:20,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_749_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:20,033 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_749_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:20,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 749 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:20,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1497 (MapPartitionsRDD[1751] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:42:20,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1497.0 with 1 tasks
   [druid] 2019-06-11 09:42:20,034 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1497.0 (TID 999, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:20,034 [er for task 999] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1497.0 (TID 999)
   [druid] 2019-06-11 09:42:20,035 [er for task 999] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:20,035 [er for task 999] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:20,035 [er for task 999] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:20,035 [er for task 999] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:20,035 [er for task 999] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1497.0 (TID 999). 966 bytes result sent to driver
   [druid] 2019-06-11 09:42:20,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1497.0 (TID 999) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:20,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1497.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:20,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1497 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:20,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 499 finished: print at Transform.scala:36, took 0.028688 s
   [druid] 2019-06-11 09:42:20,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:20,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 498 is 83 bytes
   [druid] 2019-06-11 09:42:20,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 499 is 160 bytes
   [druid] 2019-06-11 09:42:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 500 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1500 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1498, ShuffleMapStage 1499)
   [druid] 2019-06-11 09:42:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:42:20,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1500 (MapPartitionsRDD[1751] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:20,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_750 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:20,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_750_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:20,041 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_750_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:20,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 750 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:20,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1500 (MapPartitionsRDD[1751] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:42:20,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1500.0 with 1 tasks
   [druid] 2019-06-11 09:42:20,042 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1500.0 (TID 1000, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:20,042 [r for task 1000] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1500.0 (TID 1000)
   [druid] 2019-06-11 09:42:20,043 [r for task 1000] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:20,043 [r for task 1000] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:20,043 [r for task 1000] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:20,043 [r for task 1000] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:20,044 [r for task 1000] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1500.0 (TID 1000). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:42:20,044 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1500.0 (TID 1000) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:20,044 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1500.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:20,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1500 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:20,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 500 finished: print at Transform.scala:36, took 0.005598 s
   [druid] 2019-06-11 09:42:20,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217340000 ms.0 from job set of time 1560217340000 ms
   [druid] 2019-06-11 09:42:20,045 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.045 s for time 1560217340000 ms (execution: 0.040 s)
   [druid] 2019-06-11 09:42:20,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1744 from persistence list
   [druid] 2019-06-11 09:42:20,045 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1744
   [druid] 2019-06-11 09:42:20,045 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1739 from persistence list
   [druid] 2019-06-11 09:42:20,045 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1739
   [druid] 2019-06-11 09:42:20,045 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1738 from persistence list
   [druid] 2019-06-11 09:42:20,045 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1738
   [druid] 2019-06-11 09:42:20,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1738] at socketTextStream at Transform.scala:18 of time 1560217340000 ms
   [druid] 2019-06-11 09:42:20,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217330000 ms
   [druid] 2019-06-11 09:42:20,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217330000 ms
   [druid] 2019-06-11 09:42:25,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217345000 ms
   [druid] 2019-06-11 09:42:25,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217345000 ms.0 from job set of time 1560217345000 ms
   [druid] 2019-06-11 09:42:25,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:42:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1753 (map at Transform.scala:20)
   [druid] 2019-06-11 09:42:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 501 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1503 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1501, ShuffleMapStage 1502)
   [druid] 2019-06-11 09:42:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1501)
   [druid] 2019-06-11 09:42:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1501 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:42:25,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_751 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_751_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:42:25,009 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_751_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:25,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 751 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1501 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:42:25,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1501.0 with 2 tasks
   [druid] 2019-06-11 09:42:25,010 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1501.0 (TID 1001, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:25,010 [r for task 1001] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1501.0 (TID 1001)
   [druid] 2019-06-11 09:42:25,019 [r for task 1001] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1501.0 (TID 1001). 853 bytes result sent to driver
   [druid] 2019-06-11 09:42:25,019 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1501.0 (TID 1002, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:25,020 [r for task 1002] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1501.0 (TID 1002)
   [druid] 2019-06-11 09:42:25,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1501.0 (TID 1001) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:42:25,028 [r for task 1002] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1501.0 (TID 1002). 767 bytes result sent to driver
   [druid] 2019-06-11 09:42:25,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1501.0 (TID 1002) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:42:25,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1501.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1501 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:42:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:42:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:42:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1503)
   [druid] 2019-06-11 09:42:25,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:42:25,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1503 (MapPartitionsRDD[1758] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:25,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_752 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:25,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_752_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:25,030 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_752_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:25,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 752 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:25,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1503 (MapPartitionsRDD[1758] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:42:25,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1503.0 with 1 tasks
   [druid] 2019-06-11 09:42:25,030 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1503.0 (TID 1003, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:25,031 [r for task 1003] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1503.0 (TID 1003)
   [druid] 2019-06-11 09:42:25,031 [r for task 1003] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:25,031 [r for task 1003] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:25,031 [r for task 1003] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:25,031 [r for task 1003] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:25,032 [r for task 1003] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1503.0 (TID 1003). 966 bytes result sent to driver
   [druid] 2019-06-11 09:42:25,032 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1503.0 (TID 1003) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:25,032 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1503.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:25,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1503 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:25,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 501 finished: print at Transform.scala:36, took 0.024399 s
   [druid] 2019-06-11 09:42:25,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:25,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 501 is 160 bytes
   [druid] 2019-06-11 09:42:25,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 500 is 83 bytes
   [druid] 2019-06-11 09:42:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 502 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1506 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1504, ShuffleMapStage 1505)
   [druid] 2019-06-11 09:42:25,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:42:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1506 (MapPartitionsRDD[1758] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:25,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_753 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:25,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_753_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:25,036 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_753_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:25,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 753 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:25,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1506 (MapPartitionsRDD[1758] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:42:25,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1506.0 with 1 tasks
   [druid] 2019-06-11 09:42:25,037 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1506.0 (TID 1004, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:25,037 [r for task 1004] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1506.0 (TID 1004)
   [druid] 2019-06-11 09:42:25,038 [r for task 1004] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:25,038 [r for task 1004] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:25,038 [r for task 1004] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:25,038 [r for task 1004] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:25,039 [r for task 1004] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1506.0 (TID 1004). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:42:25,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1506.0 (TID 1004) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:25,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1506.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1506 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:42:25,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 502 finished: print at Transform.scala:36, took 0.005678 s
   [druid] 2019-06-11 09:42:25,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217345000 ms.0 from job set of time 1560217345000 ms
   [druid] 2019-06-11 09:42:25,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560217345000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:42:25,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1751 from persistence list
   [druid] 2019-06-11 09:42:25,040 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1751
   [druid] 2019-06-11 09:42:25,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1746 from persistence list
   [druid] 2019-06-11 09:42:25,040 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1746
   [druid] 2019-06-11 09:42:25,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1745 from persistence list
   [druid] 2019-06-11 09:42:25,041 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1745
   [druid] 2019-06-11 09:42:25,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1745] at socketTextStream at Transform.scala:18 of time 1560217345000 ms
   [druid] 2019-06-11 09:42:25,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217335000 ms
   [druid] 2019-06-11 09:42:25,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217335000 ms
   [druid] 2019-06-11 09:42:30,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217350000 ms
   [druid] 2019-06-11 09:42:30,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217350000 ms.0 from job set of time 1560217350000 ms
   [druid] 2019-06-11 09:42:30,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1760 (map at Transform.scala:20)
   [druid] 2019-06-11 09:42:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:42:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 503 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1509 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1507, ShuffleMapStage 1508)
   [druid] 2019-06-11 09:42:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1508)
   [druid] 2019-06-11 09:42:30,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1508 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:42:30,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_754 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:30,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_754_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:42:30,012 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_754_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:30,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 754 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:30,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1508 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:42:30,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1508.0 with 2 tasks
   [druid] 2019-06-11 09:42:30,013 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1508.0 (TID 1005, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:30,013 [r for task 1005] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1508.0 (TID 1005)
   [druid] 2019-06-11 09:42:30,023 [r for task 1005] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1508.0 (TID 1005). 810 bytes result sent to driver
   [druid] 2019-06-11 09:42:30,024 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1508.0 (TID 1006, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:30,024 [r for task 1006] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1508.0 (TID 1006)
   [druid] 2019-06-11 09:42:30,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1508.0 (TID 1005) in 12 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:42:30,035 [r for task 1006] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1508.0 (TID 1006). 853 bytes result sent to driver
   [druid] 2019-06-11 09:42:30,035 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1508.0 (TID 1006) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:42:30,035 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1508.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:30,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1508 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:42:30,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:42:30,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:42:30,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1509)
   [druid] 2019-06-11 09:42:30,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:42:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1509 (MapPartitionsRDD[1765] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:30,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_755 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:30,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_755_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:30,041 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_755_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:30,041 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_745_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:30,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 755 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:30,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1509 (MapPartitionsRDD[1765] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:42:30,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1509.0 with 1 tasks
   [druid] 2019-06-11 09:42:30,043 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_746_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:30,043 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1509.0 (TID 1007, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:30,043 [r for task 1007] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1509.0 (TID 1007)
   [druid] 2019-06-11 09:42:30,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 497
   [druid] 2019-06-11 09:42:30,045 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_750_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:30,045 [r for task 1007] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:30,045 [r for task 1007] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:30,045 [r for task 1007] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:30,045 [r for task 1007] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:30,045 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_748_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:30,046 [r for task 1007] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1509.0 (TID 1007). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:42:30,046 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1509.0 (TID 1007) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:30,046 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1509.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:30,046 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_753_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:30,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1509 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:42:30,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 503 finished: print at Transform.scala:36, took 0.037296 s
   [druid] 2019-06-11 09:42:30,046 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 494
   [druid] 2019-06-11 09:42:30,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 495
   [druid] 2019-06-11 09:42:30,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 498
   [druid] 2019-06-11 09:42:30,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 492
   [druid] 2019-06-11 09:42:30,048 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 496
   [druid] 2019-06-11 09:42:30,049 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_744_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:30,049 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 499
   [druid] 2019-06-11 09:42:30,050 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_751_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:30,052 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_752_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:30,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 493
   [druid] 2019-06-11 09:42:30,054 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:30,055 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 502 is 83 bytes
   [druid] 2019-06-11 09:42:30,055 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_747_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:30,055 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 503 is 160 bytes
   [druid] 2019-06-11 09:42:30,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 504 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:30,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1512 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:30,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1511, ShuffleMapStage 1510)
   [druid] 2019-06-11 09:42:30,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:42:30,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1512 (MapPartitionsRDD[1765] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:30,056 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_756 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:30,056 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_749_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:30,058 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_756_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:30,058 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_756_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:30,058 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 756 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:30,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1512 (MapPartitionsRDD[1765] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:42:30,058 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1512.0 with 1 tasks
   [druid] 2019-06-11 09:42:30,059 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1512.0 (TID 1008, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:30,059 [r for task 1008] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1512.0 (TID 1008)
   [druid] 2019-06-11 09:42:30,060 [r for task 1008] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:30,060 [r for task 1008] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:30,060 [r for task 1008] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:30,060 [r for task 1008] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:30,061 [r for task 1008] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1512.0 (TID 1008). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:42:30,061 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1512.0 (TID 1008) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:30,061 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1512.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:30,061 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1512 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:30,062 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 504 finished: print at Transform.scala:36, took 0.007195 s
   [druid] 2019-06-11 09:42:30,062 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217350000 ms.0 from job set of time 1560217350000 ms
   [druid] 2019-06-11 09:42:30,062 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.062 s for time 1560217350000 ms (execution: 0.055 s)
   [druid] 2019-06-11 09:42:30,062 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1758 from persistence list
   [druid] 2019-06-11 09:42:30,062 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1758
   [druid] 2019-06-11 09:42:30,062 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1753 from persistence list
   [druid] 2019-06-11 09:42:30,062 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1753
   [druid] 2019-06-11 09:42:30,062 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1752 from persistence list
   [druid] 2019-06-11 09:42:30,063 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1752
   [druid] 2019-06-11 09:42:30,063 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1752] at socketTextStream at Transform.scala:18 of time 1560217350000 ms
   [druid] 2019-06-11 09:42:30,063 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217340000 ms
   [druid] 2019-06-11 09:42:30,063 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217340000 ms
   [druid] 2019-06-11 09:42:35,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217355000 ms
   [druid] 2019-06-11 09:42:35,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217355000 ms.0 from job set of time 1560217355000 ms
   [druid] 2019-06-11 09:42:35,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:42:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1767 (map at Transform.scala:20)
   [druid] 2019-06-11 09:42:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 505 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1515 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1513, ShuffleMapStage 1514)
   [druid] 2019-06-11 09:42:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1513)
   [druid] 2019-06-11 09:42:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1513 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:42:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_757 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:35,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_757_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:42:35,010 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_757_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:35,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 757 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:35,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1513 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:42:35,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1513.0 with 2 tasks
   [druid] 2019-06-11 09:42:35,011 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1513.0 (TID 1009, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:35,011 [r for task 1009] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1513.0 (TID 1009)
   [druid] 2019-06-11 09:42:35,021 [r for task 1009] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1513.0 (TID 1009). 810 bytes result sent to driver
   [druid] 2019-06-11 09:42:35,021 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1513.0 (TID 1010, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:35,021 [r for task 1010] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1513.0 (TID 1010)
   [druid] 2019-06-11 09:42:35,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1513.0 (TID 1009) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:42:35,029 [r for task 1010] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1513.0 (TID 1010). 767 bytes result sent to driver
   [druid] 2019-06-11 09:42:35,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1513.0 (TID 1010) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:42:35,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1513.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:35,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1513 (parallelize at Transform.scala:16) finished in 0.019 s
   [druid] 2019-06-11 09:42:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:42:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:42:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1515)
   [druid] 2019-06-11 09:42:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:42:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1515 (MapPartitionsRDD[1772] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:35,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_758 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:35,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_758_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:35,031 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_758_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:35,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 758 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:35,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1515 (MapPartitionsRDD[1772] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:42:35,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1515.0 with 1 tasks
   [druid] 2019-06-11 09:42:35,032 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1515.0 (TID 1011, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:35,032 [r for task 1011] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1515.0 (TID 1011)
   [druid] 2019-06-11 09:42:35,032 [r for task 1011] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:35,032 [r for task 1011] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:35,032 [r for task 1011] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:35,032 [r for task 1011] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:35,033 [r for task 1011] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1515.0 (TID 1011). 966 bytes result sent to driver
   [druid] 2019-06-11 09:42:35,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1515.0 (TID 1011) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:35,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1515.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:35,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1515 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:42:35,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 505 finished: print at Transform.scala:36, took 0.025166 s
   [druid] 2019-06-11 09:42:35,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:35,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 505 is 160 bytes
   [druid] 2019-06-11 09:42:35,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 504 is 83 bytes
   [druid] 2019-06-11 09:42:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 506 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1518 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1516, ShuffleMapStage 1517)
   [druid] 2019-06-11 09:42:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:42:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1518 (MapPartitionsRDD[1772] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:35,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_759 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:35,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_759_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:35,037 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_759_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:35,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 759 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:35,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1518 (MapPartitionsRDD[1772] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:42:35,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1518.0 with 1 tasks
   [druid] 2019-06-11 09:42:35,038 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1518.0 (TID 1012, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:35,038 [r for task 1012] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1518.0 (TID 1012)
   [druid] 2019-06-11 09:42:35,039 [r for task 1012] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:35,039 [r for task 1012] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:35,039 [r for task 1012] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:35,039 [r for task 1012] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:35,039 [r for task 1012] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1518.0 (TID 1012). 966 bytes result sent to driver
   [druid] 2019-06-11 09:42:35,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1518.0 (TID 1012) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:35,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1518.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:35,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1518 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:35,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 506 finished: print at Transform.scala:36, took 0.004635 s
   [druid] 2019-06-11 09:42:35,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217355000 ms.0 from job set of time 1560217355000 ms
   [druid] 2019-06-11 09:42:35,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560217355000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:42:35,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1765 from persistence list
   [druid] 2019-06-11 09:42:35,041 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1765
   [druid] 2019-06-11 09:42:35,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1760 from persistence list
   [druid] 2019-06-11 09:42:35,041 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1760
   [druid] 2019-06-11 09:42:35,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1759 from persistence list
   [druid] 2019-06-11 09:42:35,041 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1759
   [druid] 2019-06-11 09:42:35,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1759] at socketTextStream at Transform.scala:18 of time 1560217355000 ms
   [druid] 2019-06-11 09:42:35,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217345000 ms
   [druid] 2019-06-11 09:42:35,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217345000 ms
   [druid] 2019-06-11 09:42:40,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217360000 ms
   [druid] 2019-06-11 09:42:40,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217360000 ms.0 from job set of time 1560217360000 ms
   [druid] 2019-06-11 09:42:40,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1774 (map at Transform.scala:20)
   [druid] 2019-06-11 09:42:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:42:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 507 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1521 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1519, ShuffleMapStage 1520)
   [druid] 2019-06-11 09:42:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1520)
   [druid] 2019-06-11 09:42:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1520 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:42:40,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_760 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:40,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_760_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:42:40,010 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_760_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:40,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 760 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1520 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:42:40,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1520.0 with 2 tasks
   [druid] 2019-06-11 09:42:40,010 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1520.0 (TID 1013, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:40,011 [r for task 1013] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1520.0 (TID 1013)
   [druid] 2019-06-11 09:42:40,019 [r for task 1013] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1520.0 (TID 1013). 810 bytes result sent to driver
   [druid] 2019-06-11 09:42:40,019 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1520.0 (TID 1014, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:40,019 [r for task 1014] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1520.0 (TID 1014)
   [druid] 2019-06-11 09:42:40,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1520.0 (TID 1013) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:42:40,027 [r for task 1014] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1520.0 (TID 1014). 810 bytes result sent to driver
   [druid] 2019-06-11 09:42:40,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1520.0 (TID 1014) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:42:40,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1520.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1520 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:42:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:42:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:42:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1521)
   [druid] 2019-06-11 09:42:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:42:40,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1521 (MapPartitionsRDD[1779] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:40,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_761 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:40,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_761_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:40,029 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_761_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:40,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 761 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1521 (MapPartitionsRDD[1779] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:42:40,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1521.0 with 1 tasks
   [druid] 2019-06-11 09:42:40,030 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1521.0 (TID 1015, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:40,030 [r for task 1015] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1521.0 (TID 1015)
   [druid] 2019-06-11 09:42:40,031 [r for task 1015] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:40,031 [r for task 1015] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:40,031 [r for task 1015] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:40,031 [r for task 1015] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:40,031 [r for task 1015] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1521.0 (TID 1015). 966 bytes result sent to driver
   [druid] 2019-06-11 09:42:40,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1521.0 (TID 1015) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:40,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1521.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:40,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1521 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:40,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 507 finished: print at Transform.scala:36, took 0.024358 s
   [druid] 2019-06-11 09:42:40,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:40,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 506 is 83 bytes
   [druid] 2019-06-11 09:42:40,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 507 is 160 bytes
   [druid] 2019-06-11 09:42:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 508 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1524 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1522, ShuffleMapStage 1523)
   [druid] 2019-06-11 09:42:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:42:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1524 (MapPartitionsRDD[1779] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:40,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_762 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:40,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_762_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:40,038 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_762_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:40,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 762 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1524 (MapPartitionsRDD[1779] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:42:40,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1524.0 with 1 tasks
   [druid] 2019-06-11 09:42:40,038 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1524.0 (TID 1016, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:40,038 [r for task 1016] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1524.0 (TID 1016)
   [druid] 2019-06-11 09:42:40,039 [r for task 1016] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:40,039 [r for task 1016] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:40,039 [r for task 1016] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:40,039 [r for task 1016] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:40,040 [r for task 1016] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1524.0 (TID 1016). 966 bytes result sent to driver
   [druid] 2019-06-11 09:42:40,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1524.0 (TID 1016) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:40,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1524.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:40,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1524 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:40,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 508 finished: print at Transform.scala:36, took 0.004230 s
   [druid] 2019-06-11 09:42:40,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217360000 ms.0 from job set of time 1560217360000 ms
   [druid] 2019-06-11 09:42:40,040 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.040 s for time 1560217360000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:42:40,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1772 from persistence list
   [druid] 2019-06-11 09:42:40,040 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1772
   [druid] 2019-06-11 09:42:40,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1767 from persistence list
   [druid] 2019-06-11 09:42:40,041 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1767
   [druid] 2019-06-11 09:42:40,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1766 from persistence list
   [druid] 2019-06-11 09:42:40,041 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1766
   [druid] 2019-06-11 09:42:40,041 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1766] at socketTextStream at Transform.scala:18 of time 1560217360000 ms
   [druid] 2019-06-11 09:42:40,041 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217350000 ms
   [druid] 2019-06-11 09:42:40,041 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217350000 ms
   [druid] 2019-06-11 09:42:45,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217365000 ms
   [druid] 2019-06-11 09:42:45,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217365000 ms.0 from job set of time 1560217365000 ms
   [druid] 2019-06-11 09:42:45,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1781 (map at Transform.scala:20)
   [druid] 2019-06-11 09:42:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:42:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 509 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1527 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:45,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1525, ShuffleMapStage 1526)
   [druid] 2019-06-11 09:42:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1526)
   [druid] 2019-06-11 09:42:45,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1526 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:42:45,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_763 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:45,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_763_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:42:45,009 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_763_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:45,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 763 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:45,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1526 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:42:45,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1526.0 with 2 tasks
   [druid] 2019-06-11 09:42:45,009 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1526.0 (TID 1017, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:45,010 [r for task 1017] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1526.0 (TID 1017)
   [druid] 2019-06-11 09:42:45,018 [r for task 1017] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1526.0 (TID 1017). 810 bytes result sent to driver
   [druid] 2019-06-11 09:42:45,019 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1526.0 (TID 1018, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:45,019 [r for task 1018] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1526.0 (TID 1018)
   [druid] 2019-06-11 09:42:45,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1526.0 (TID 1017) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:42:45,026 [r for task 1018] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1526.0 (TID 1018). 767 bytes result sent to driver
   [druid] 2019-06-11 09:42:45,026 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1526.0 (TID 1018) in 7 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:42:45,026 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1526.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:45,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1526 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:42:45,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:42:45,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:42:45,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1527)
   [druid] 2019-06-11 09:42:45,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:42:45,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1527 (MapPartitionsRDD[1786] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:45,027 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_764 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:45,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_764_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:45,028 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_764_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:45,028 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 764 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:45,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1527 (MapPartitionsRDD[1786] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:42:45,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1527.0 with 1 tasks
   [druid] 2019-06-11 09:42:45,029 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1527.0 (TID 1019, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:45,029 [r for task 1019] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1527.0 (TID 1019)
   [druid] 2019-06-11 09:42:45,029 [r for task 1019] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:45,029 [r for task 1019] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:45,029 [r for task 1019] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:45,029 [r for task 1019] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:45,030 [r for task 1019] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1527.0 (TID 1019). 966 bytes result sent to driver
   [druid] 2019-06-11 09:42:45,030 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1527.0 (TID 1019) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:45,030 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1527.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:45,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1527 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:42:45,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 509 finished: print at Transform.scala:36, took 0.023254 s
   [druid] 2019-06-11 09:42:45,033 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:45,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 508 is 83 bytes
   [druid] 2019-06-11 09:42:45,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 509 is 160 bytes
   [druid] 2019-06-11 09:42:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 510 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1530 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1529, ShuffleMapStage 1528)
   [druid] 2019-06-11 09:42:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:42:45,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1530 (MapPartitionsRDD[1786] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:45,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_765 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:45,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_765_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:45,035 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_765_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:45,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 765 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1530 (MapPartitionsRDD[1786] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:42:45,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1530.0 with 1 tasks
   [druid] 2019-06-11 09:42:45,035 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1530.0 (TID 1020, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:45,035 [r for task 1020] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1530.0 (TID 1020)
   [druid] 2019-06-11 09:42:45,036 [r for task 1020] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:45,036 [r for task 1020] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:45,036 [r for task 1020] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:45,036 [r for task 1020] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:45,037 [r for task 1020] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1530.0 (TID 1020). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:42:45,037 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1530.0 (TID 1020) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:45,037 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1530.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:45,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1530 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:45,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 510 finished: print at Transform.scala:36, took 0.004728 s
   [druid] 2019-06-11 09:42:45,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217365000 ms.0 from job set of time 1560217365000 ms
   [druid] 2019-06-11 09:42:45,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.038 s for time 1560217365000 ms (execution: 0.033 s)
   [druid] 2019-06-11 09:42:45,038 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1779 from persistence list
   [druid] 2019-06-11 09:42:45,038 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1779
   [druid] 2019-06-11 09:42:45,038 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1774 from persistence list
   [druid] 2019-06-11 09:42:45,038 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1774
   [druid] 2019-06-11 09:42:45,038 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1773 from persistence list
   [druid] 2019-06-11 09:42:45,038 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1773
   [druid] 2019-06-11 09:42:45,038 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1773] at socketTextStream at Transform.scala:18 of time 1560217365000 ms
   [druid] 2019-06-11 09:42:45,039 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217355000 ms
   [druid] 2019-06-11 09:42:45,039 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217355000 ms
   [druid] 2019-06-11 09:42:50,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217370000 ms
   [druid] 2019-06-11 09:42:50,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217370000 ms.0 from job set of time 1560217370000 ms
   [druid] 2019-06-11 09:42:50,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1788 (map at Transform.scala:20)
   [druid] 2019-06-11 09:42:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:42:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 511 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1533 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1531, ShuffleMapStage 1532)
   [druid] 2019-06-11 09:42:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1532)
   [druid] 2019-06-11 09:42:50,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1532 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:42:50,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_766 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:42:50,013 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_766_piece0 stored as bytes in memory (estimated size 1311.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:42:50,014 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_766_piece0 in memory on 192.168.91.1:63518 (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,014 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_761_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,014 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 766 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:50,014 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1532 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:42:50,014 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1532.0 with 2 tasks
   [druid] 2019-06-11 09:42:50,015 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_763_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,015 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1532.0 (TID 1021, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:50,015 [r for task 1021] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1532.0 (TID 1021)
   [druid] 2019-06-11 09:42:50,016 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 506
   [druid] 2019-06-11 09:42:50,017 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_760_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,018 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_765_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,019 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_755_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,019 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 502
   [druid] 2019-06-11 09:42:50,019 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 504
   [druid] 2019-06-11 09:42:50,020 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_754_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,020 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 500
   [druid] 2019-06-11 09:42:50,021 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 505
   [druid] 2019-06-11 09:42:50,021 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_762_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,021 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 507
   [druid] 2019-06-11 09:42:50,022 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_758_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,022 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 501
   [druid] 2019-06-11 09:42:50,023 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_764_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,023 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_757_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,024 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_756_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,024 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_759_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,025 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 503
   [druid] 2019-06-11 09:42:50,028 [r for task 1021] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1532.0 (TID 1021). 853 bytes result sent to driver
   [druid] 2019-06-11 09:42:50,028 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1532.0 (TID 1022, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:50,028 [r for task 1022] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1532.0 (TID 1022)
   [druid] 2019-06-11 09:42:50,028 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1532.0 (TID 1021) in 13 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:42:50,039 [r for task 1022] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1532.0 (TID 1022). 810 bytes result sent to driver
   [druid] 2019-06-11 09:42:50,039 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1532.0 (TID 1022) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:42:50,039 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1532.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1532 (parallelize at Transform.scala:16) finished in 0.024 s
   [druid] 2019-06-11 09:42:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:42:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:42:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1533)
   [druid] 2019-06-11 09:42:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:42:50,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1533 (MapPartitionsRDD[1793] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:50,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_767 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:50,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_767_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:50,042 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_767_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 767 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:50,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1533 (MapPartitionsRDD[1793] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:42:50,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1533.0 with 1 tasks
   [druid] 2019-06-11 09:42:50,042 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1533.0 (TID 1023, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:50,042 [r for task 1023] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1533.0 (TID 1023)
   [druid] 2019-06-11 09:42:50,044 [r for task 1023] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:50,044 [r for task 1023] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:50,044 [r for task 1023] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:50,044 [r for task 1023] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:50,045 [r for task 1023] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1533.0 (TID 1023). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:42:50,045 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1533.0 (TID 1023) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:50,045 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1533.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:50,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1533 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:42:50,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 511 finished: print at Transform.scala:36, took 0.037603 s
   [druid] 2019-06-11 09:42:50,048 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:50,049 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 510 is 83 bytes
   [druid] 2019-06-11 09:42:50,049 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 511 is 160 bytes
   [druid] 2019-06-11 09:42:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 512 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1536 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1534, ShuffleMapStage 1535)
   [druid] 2019-06-11 09:42:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:42:50,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1536 (MapPartitionsRDD[1793] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:50,050 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_768 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:50,051 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_768_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:50,052 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_768_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:50,052 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 768 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:50,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1536 (MapPartitionsRDD[1793] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:42:50,052 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1536.0 with 1 tasks
   [druid] 2019-06-11 09:42:50,052 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1536.0 (TID 1024, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:50,052 [r for task 1024] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1536.0 (TID 1024)
   [druid] 2019-06-11 09:42:50,053 [r for task 1024] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:50,053 [r for task 1024] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:50,053 [r for task 1024] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:50,054 [r for task 1024] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:42:50,055 [r for task 1024] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1536.0 (TID 1024). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:42:50,055 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1536.0 (TID 1024) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:50,055 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1536.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:50,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1536 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:42:50,055 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 512 finished: print at Transform.scala:36, took 0.007125 s
   [druid] 2019-06-11 09:42:50,056 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217370000 ms.0 from job set of time 1560217370000 ms
   [druid] 2019-06-11 09:42:50,056 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.056 s for time 1560217370000 ms (execution: 0.050 s)
   [druid] 2019-06-11 09:42:50,056 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1786 from persistence list
   [druid] 2019-06-11 09:42:50,056 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1786
   [druid] 2019-06-11 09:42:50,056 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1781 from persistence list
   [druid] 2019-06-11 09:42:50,056 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1781
   [druid] 2019-06-11 09:42:50,056 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1780 from persistence list
   [druid] 2019-06-11 09:42:50,057 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1780
   [druid] 2019-06-11 09:42:50,057 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1780] at socketTextStream at Transform.scala:18 of time 1560217370000 ms
   [druid] 2019-06-11 09:42:50,057 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217360000 ms
   [druid] 2019-06-11 09:42:50,057 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217360000 ms
   [druid] 2019-06-11 09:42:55,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217375000 ms
   [druid] 2019-06-11 09:42:55,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217375000 ms.0 from job set of time 1560217375000 ms
   [druid] 2019-06-11 09:42:55,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:42:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1795 (map at Transform.scala:20)
   [druid] 2019-06-11 09:42:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 513 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1539 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1537, ShuffleMapStage 1538)
   [druid] 2019-06-11 09:42:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1537)
   [druid] 2019-06-11 09:42:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1537 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:42:55,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_769 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:55,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_769_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:42:55,011 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_769_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:55,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 769 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:55,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1537 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:42:55,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1537.0 with 2 tasks
   [druid] 2019-06-11 09:42:55,011 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1537.0 (TID 1025, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:55,011 [r for task 1025] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1537.0 (TID 1025)
   [druid] 2019-06-11 09:42:55,019 [r for task 1025] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1537.0 (TID 1025). 810 bytes result sent to driver
   [druid] 2019-06-11 09:42:55,020 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1537.0 (TID 1026, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:42:55,020 [r for task 1026] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1537.0 (TID 1026)
   [druid] 2019-06-11 09:42:55,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1537.0 (TID 1025) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:42:55,027 [r for task 1026] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1537.0 (TID 1026). 767 bytes result sent to driver
   [druid] 2019-06-11 09:42:55,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1537.0 (TID 1026) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:42:55,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1537.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1537 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:42:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:42:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:42:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1539)
   [druid] 2019-06-11 09:42:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:42:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1539 (MapPartitionsRDD[1800] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:55,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_770 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:55,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_770_piece0 stored as bytes in memory (estimated size 2040.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:42:55,029 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_770_piece0 in memory on 192.168.91.1:63518 (size: 2040.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:55,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 770 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:55,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1539 (MapPartitionsRDD[1800] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:42:55,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1539.0 with 1 tasks
   [druid] 2019-06-11 09:42:55,029 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1539.0 (TID 1027, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:55,029 [r for task 1027] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1539.0 (TID 1027)
   [druid] 2019-06-11 09:42:55,030 [r for task 1027] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:55,030 [r for task 1027] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:55,030 [r for task 1027] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:55,031 [r for task 1027] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:55,031 [r for task 1027] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1539.0 (TID 1027). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:42:55,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1539.0 (TID 1027) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:55,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1539.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:55,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1539 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:42:55,032 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 513 finished: print at Transform.scala:36, took 0.023392 s
   [druid] 2019-06-11 09:42:55,033 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:42:55,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 513 is 160 bytes
   [druid] 2019-06-11 09:42:55,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 512 is 83 bytes
   [druid] 2019-06-11 09:42:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 514 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:42:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1542 (print at Transform.scala:36)
   [druid] 2019-06-11 09:42:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1540, ShuffleMapStage 1541)
   [druid] 2019-06-11 09:42:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:42:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1542 (MapPartitionsRDD[1800] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:42:55,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_771 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:42:55,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_771_piece0 stored as bytes in memory (estimated size 2040.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:42:55,035 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_771_piece0 in memory on 192.168.91.1:63518 (size: 2040.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:42:55,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 771 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:42:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1542 (MapPartitionsRDD[1800] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:42:55,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1542.0 with 1 tasks
   [druid] 2019-06-11 09:42:55,036 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1542.0 (TID 1028, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:42:55,036 [r for task 1028] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1542.0 (TID 1028)
   [druid] 2019-06-11 09:42:55,037 [r for task 1028] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:42:55,037 [r for task 1028] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:55,037 [r for task 1028] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:42:55,037 [r for task 1028] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:42:55,037 [r for task 1028] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1542.0 (TID 1028). 966 bytes result sent to driver
   [druid] 2019-06-11 09:42:55,037 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1542.0 (TID 1028) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:42:55,038 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1542.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:42:55,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1542 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:42:55,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 514 finished: print at Transform.scala:36, took 0.004319 s
   [druid] 2019-06-11 09:42:55,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217375000 ms.0 from job set of time 1560217375000 ms
   [druid] 2019-06-11 09:42:55,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.038 s for time 1560217375000 ms (execution: 0.032 s)
   [druid] 2019-06-11 09:42:55,038 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1793 from persistence list
   [druid] 2019-06-11 09:42:55,039 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1793
   [druid] 2019-06-11 09:42:55,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1788 from persistence list
   [druid] 2019-06-11 09:42:55,039 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1788
   [druid] 2019-06-11 09:42:55,039 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1787 from persistence list
   [druid] 2019-06-11 09:42:55,039 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1787
   [druid] 2019-06-11 09:42:55,039 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1787] at socketTextStream at Transform.scala:18 of time 1560217375000 ms
   [druid] 2019-06-11 09:42:55,039 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217365000 ms
   [druid] 2019-06-11 09:42:55,039 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217365000 ms
   [druid] 2019-06-11 09:43:00,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217380000 ms
   [druid] 2019-06-11 09:43:00,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217380000 ms.0 from job set of time 1560217380000 ms
   [druid] 2019-06-11 09:43:00,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:43:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1802 (map at Transform.scala:20)
   [druid] 2019-06-11 09:43:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 515 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1545 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1543, ShuffleMapStage 1544)
   [druid] 2019-06-11 09:43:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1543)
   [druid] 2019-06-11 09:43:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1543 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:43:00,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_772 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:00,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_772_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:43:00,011 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_772_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:00,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 772 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:00,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1543 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:43:00,012 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1543.0 with 2 tasks
   [druid] 2019-06-11 09:43:00,012 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1543.0 (TID 1029, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:00,012 [r for task 1029] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1543.0 (TID 1029)
   [druid] 2019-06-11 09:43:00,021 [r for task 1029] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1543.0 (TID 1029). 853 bytes result sent to driver
   [druid] 2019-06-11 09:43:00,021 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1543.0 (TID 1030, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:00,021 [r for task 1030] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1543.0 (TID 1030)
   [druid] 2019-06-11 09:43:00,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1543.0 (TID 1029) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:43:00,029 [r for task 1030] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1543.0 (TID 1030). 810 bytes result sent to driver
   [druid] 2019-06-11 09:43:00,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1543.0 (TID 1030) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:43:00,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1543.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:00,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1543 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:43:00,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:43:00,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:43:00,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1545)
   [druid] 2019-06-11 09:43:00,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:43:00,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1545 (MapPartitionsRDD[1807] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:00,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_773 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:00,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_773_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:00,031 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_773_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:00,031 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 773 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:00,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1545 (MapPartitionsRDD[1807] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:43:00,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1545.0 with 1 tasks
   [druid] 2019-06-11 09:43:00,032 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1545.0 (TID 1031, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:00,032 [r for task 1031] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1545.0 (TID 1031)
   [druid] 2019-06-11 09:43:00,033 [r for task 1031] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:00,033 [r for task 1031] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:00,033 [r for task 1031] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:00,033 [r for task 1031] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:00,034 [r for task 1031] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1545.0 (TID 1031). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:43:00,034 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1545.0 (TID 1031) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:00,034 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1545.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:00,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1545 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:00,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 515 finished: print at Transform.scala:36, took 0.024984 s
   [druid] 2019-06-11 09:43:00,036 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:00,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 515 is 160 bytes
   [druid] 2019-06-11 09:43:00,037 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 514 is 83 bytes
   [druid] 2019-06-11 09:43:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 516 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1548 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1547, ShuffleMapStage 1546)
   [druid] 2019-06-11 09:43:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:43:00,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1548 (MapPartitionsRDD[1807] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:00,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_774 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:00,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_774_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:00,038 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_774_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:00,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 774 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:00,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1548 (MapPartitionsRDD[1807] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:43:00,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1548.0 with 1 tasks
   [druid] 2019-06-11 09:43:00,039 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1548.0 (TID 1032, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:00,039 [r for task 1032] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1548.0 (TID 1032)
   [druid] 2019-06-11 09:43:00,039 [r for task 1032] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:00,039 [r for task 1032] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:00,039 [r for task 1032] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:00,039 [r for task 1032] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:00,040 [r for task 1032] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1548.0 (TID 1032). 966 bytes result sent to driver
   [druid] 2019-06-11 09:43:00,040 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1548.0 (TID 1032) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:00,040 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1548.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:00,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1548 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:00,040 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 516 finished: print at Transform.scala:36, took 0.004125 s
   [druid] 2019-06-11 09:43:00,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217380000 ms.0 from job set of time 1560217380000 ms
   [druid] 2019-06-11 09:43:00,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560217380000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:43:00,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1800 from persistence list
   [druid] 2019-06-11 09:43:00,041 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1800
   [druid] 2019-06-11 09:43:00,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1795 from persistence list
   [druid] 2019-06-11 09:43:00,041 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1795
   [druid] 2019-06-11 09:43:00,041 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1794 from persistence list
   [druid] 2019-06-11 09:43:00,042 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1794
   [druid] 2019-06-11 09:43:00,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1794] at socketTextStream at Transform.scala:18 of time 1560217380000 ms
   [druid] 2019-06-11 09:43:00,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217370000 ms
   [druid] 2019-06-11 09:43:00,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217370000 ms
   [druid] 2019-06-11 09:43:05,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217385000 ms
   [druid] 2019-06-11 09:43:05,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217385000 ms.0 from job set of time 1560217385000 ms
   [druid] 2019-06-11 09:43:05,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:43:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1809 (map at Transform.scala:20)
   [druid] 2019-06-11 09:43:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 517 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1551 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1549, ShuffleMapStage 1550)
   [druid] 2019-06-11 09:43:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1549)
   [druid] 2019-06-11 09:43:05,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1549 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:43:05,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_775 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:05,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_775_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:43:05,010 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_775_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 775 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:05,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1549 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:43:05,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1549.0 with 2 tasks
   [druid] 2019-06-11 09:43:05,010 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1549.0 (TID 1033, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:05,010 [r for task 1033] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1549.0 (TID 1033)
   [druid] 2019-06-11 09:43:05,019 [r for task 1033] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1549.0 (TID 1033). 810 bytes result sent to driver
   [druid] 2019-06-11 09:43:05,019 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1549.0 (TID 1034, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:05,019 [r for task 1034] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1549.0 (TID 1034)
   [druid] 2019-06-11 09:43:05,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1549.0 (TID 1033) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:43:05,027 [r for task 1034] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1549.0 (TID 1034). 767 bytes result sent to driver
   [druid] 2019-06-11 09:43:05,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1549.0 (TID 1034) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:43:05,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1549.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1549 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:43:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:43:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:43:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1551)
   [druid] 2019-06-11 09:43:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:43:05,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1551 (MapPartitionsRDD[1814] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:05,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_776 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:05,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_776_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:05,029 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_776_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,029 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 776 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:05,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1551 (MapPartitionsRDD[1814] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:43:05,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1551.0 with 1 tasks
   [druid] 2019-06-11 09:43:05,029 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1551.0 (TID 1035, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:05,030 [r for task 1035] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1551.0 (TID 1035)
   [druid] 2019-06-11 09:43:05,030 [r for task 1035] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:05,030 [r for task 1035] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:05,030 [r for task 1035] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:05,030 [r for task 1035] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:05,031 [r for task 1035] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1551.0 (TID 1035). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:43:05,031 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1551.0 (TID 1035) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:05,031 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1551.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:05,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1551 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:05,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 517 finished: print at Transform.scala:36, took 0.023738 s
   [druid] 2019-06-11 09:43:05,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:05,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 517 is 160 bytes
   [druid] 2019-06-11 09:43:05,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 516 is 83 bytes
   [druid] 2019-06-11 09:43:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 518 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1554 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1552, ShuffleMapStage 1553)
   [druid] 2019-06-11 09:43:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:43:05,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1554 (MapPartitionsRDD[1814] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:05,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_777 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:05,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_777_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:05,036 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_777_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 777 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:05,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1554 (MapPartitionsRDD[1814] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:43:05,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1554.0 with 1 tasks
   [druid] 2019-06-11 09:43:05,037 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1554.0 (TID 1036, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:05,037 [r for task 1036] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1554.0 (TID 1036)
   [druid] 2019-06-11 09:43:05,042 [r for task 1036] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:05,042 [r for task 1036] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:05,042 [r for task 1036] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:05,042 [r for task 1036] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:05,042 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_768_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,043 [r for task 1036] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1554.0 (TID 1036). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:43:05,043 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1554.0 (TID 1036) in 6 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:05,043 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1554.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:05,043 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_770_piece0 on 192.168.91.1:63518 in memory (size: 2040.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1554 (print at Transform.scala:36) finished in 0.006 s
   [druid] 2019-06-11 09:43:05,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 518 finished: print at Transform.scala:36, took 0.008816 s
   [druid] 2019-06-11 09:43:05,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217385000 ms.0 from job set of time 1560217385000 ms
   [druid] 2019-06-11 09:43:05,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.044 s for time 1560217385000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:43:05,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1807 from persistence list
   [druid] 2019-06-11 09:43:05,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1802 from persistence list
   [druid] 2019-06-11 09:43:05,044 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_769_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,044 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1807
   [druid] 2019-06-11 09:43:05,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1801 from persistence list
   [druid] 2019-06-11 09:43:05,044 [-thread-pool-28] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1802
   [druid] 2019-06-11 09:43:05,045 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1801
   [druid] 2019-06-11 09:43:05,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1801] at socketTextStream at Transform.scala:18 of time 1560217385000 ms
   [druid] 2019-06-11 09:43:05,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217375000 ms
   [druid] 2019-06-11 09:43:05,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217375000 ms
   [druid] 2019-06-11 09:43:05,045 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_772_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,046 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_771_piece0 on 192.168.91.1:63518 in memory (size: 2040.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,046 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 511
   [druid] 2019-06-11 09:43:05,047 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_767_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 510
   [druid] 2019-06-11 09:43:05,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 509
   [druid] 2019-06-11 09:43:05,047 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_766_piece0 on 192.168.91.1:63518 in memory (size: 1311.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,048 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_776_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,049 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_773_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,050 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_774_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,050 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_775_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:05,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 512
   [druid] 2019-06-11 09:43:05,051 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 508
   [druid] 2019-06-11 09:43:05,052 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 513
   [druid] 2019-06-11 09:43:10,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217390000 ms
   [druid] 2019-06-11 09:43:10,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217390000 ms.0 from job set of time 1560217390000 ms
   [druid] 2019-06-11 09:43:10,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:43:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1816 (map at Transform.scala:20)
   [druid] 2019-06-11 09:43:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 519 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1557 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1555, ShuffleMapStage 1556)
   [druid] 2019-06-11 09:43:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1555)
   [druid] 2019-06-11 09:43:10,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1555 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:43:10,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_778 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:10,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_778_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:43:10,010 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_778_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:10,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 778 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:10,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1555 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:43:10,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1555.0 with 2 tasks
   [druid] 2019-06-11 09:43:10,011 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1555.0 (TID 1037, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:10,011 [r for task 1037] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1555.0 (TID 1037)
   [druid] 2019-06-11 09:43:10,020 [r for task 1037] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1555.0 (TID 1037). 810 bytes result sent to driver
   [druid] 2019-06-11 09:43:10,020 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1555.0 (TID 1038, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:10,020 [r for task 1038] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1555.0 (TID 1038)
   [druid] 2019-06-11 09:43:10,021 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1555.0 (TID 1037) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:43:10,042 [r for task 1038] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1555.0 (TID 1038). 853 bytes result sent to driver
   [druid] 2019-06-11 09:43:10,042 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1555.0 (TID 1038) in 22 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:43:10,042 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1555.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1555 (parallelize at Transform.scala:16) finished in 0.032 s
   [druid] 2019-06-11 09:43:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:43:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:43:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1557)
   [druid] 2019-06-11 09:43:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:43:10,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1557 (MapPartitionsRDD[1821] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:10,043 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_779 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:10,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_779_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:10,044 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_779_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:10,044 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 779 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:10,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1557 (MapPartitionsRDD[1821] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:43:10,044 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1557.0 with 1 tasks
   [druid] 2019-06-11 09:43:10,045 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1557.0 (TID 1039, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:10,045 [r for task 1039] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1557.0 (TID 1039)
   [druid] 2019-06-11 09:43:10,045 [r for task 1039] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:10,045 [r for task 1039] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:10,045 [r for task 1039] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:10,045 [r for task 1039] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:10,046 [r for task 1039] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1557.0 (TID 1039). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:43:10,046 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1557.0 (TID 1039) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:10,046 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1557.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:10,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1557 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:10,046 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 519 finished: print at Transform.scala:36, took 0.038030 s
   [druid] 2019-06-11 09:43:10,050 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:10,050 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 519 is 160 bytes
   [druid] 2019-06-11 09:43:10,050 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 518 is 83 bytes
   [druid] 2019-06-11 09:43:10,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 520 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:10,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1560 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:10,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1558, ShuffleMapStage 1559)
   [druid] 2019-06-11 09:43:10,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:43:10,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1560 (MapPartitionsRDD[1821] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:10,051 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_780 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:10,051 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_780_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:10,052 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_780_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:10,052 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 780 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:10,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1560 (MapPartitionsRDD[1821] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:43:10,052 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1560.0 with 1 tasks
   [druid] 2019-06-11 09:43:10,052 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1560.0 (TID 1040, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:10,052 [r for task 1040] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1560.0 (TID 1040)
   [druid] 2019-06-11 09:43:10,053 [r for task 1040] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:10,053 [r for task 1040] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:10,053 [r for task 1040] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:10,053 [r for task 1040] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:10,054 [r for task 1040] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1560.0 (TID 1040). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:43:10,054 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1560.0 (TID 1040) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:10,054 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1560.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:10,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1560 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:10,054 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 520 finished: print at Transform.scala:36, took 0.004384 s
   [druid] 2019-06-11 09:43:10,054 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217390000 ms.0 from job set of time 1560217390000 ms
   [druid] 2019-06-11 09:43:10,054 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.054 s for time 1560217390000 ms (execution: 0.048 s)
   [druid] 2019-06-11 09:43:10,054 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1814 from persistence list
   [druid] 2019-06-11 09:43:10,055 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1814
   [druid] 2019-06-11 09:43:10,055 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1809 from persistence list
   [druid] 2019-06-11 09:43:10,055 [-thread-pool-28] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1809
   [druid] 2019-06-11 09:43:10,055 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1808 from persistence list
   [druid] 2019-06-11 09:43:10,055 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1808
   [druid] 2019-06-11 09:43:10,055 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1808] at socketTextStream at Transform.scala:18 of time 1560217390000 ms
   [druid] 2019-06-11 09:43:10,055 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217380000 ms
   [druid] 2019-06-11 09:43:10,055 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217380000 ms
   [druid] 2019-06-11 09:43:15,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217395000 ms
   [druid] 2019-06-11 09:43:15,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217395000 ms.0 from job set of time 1560217395000 ms
   [druid] 2019-06-11 09:43:15,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1823 (map at Transform.scala:20)
   [druid] 2019-06-11 09:43:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:43:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 521 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1563 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1561, ShuffleMapStage 1562)
   [druid] 2019-06-11 09:43:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1562)
   [druid] 2019-06-11 09:43:15,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1562 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:43:15,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_781 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:15,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_781_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:43:15,009 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_781_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:15,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 781 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:15,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1562 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:43:15,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1562.0 with 2 tasks
   [druid] 2019-06-11 09:43:15,014 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1562.0 (TID 1041, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:15,015 [r for task 1041] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1562.0 (TID 1041)
   [druid] 2019-06-11 09:43:15,024 [r for task 1041] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1562.0 (TID 1041). 853 bytes result sent to driver
   [druid] 2019-06-11 09:43:15,024 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1562.0 (TID 1042, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:15,024 [r for task 1042] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1562.0 (TID 1042)
   [druid] 2019-06-11 09:43:15,024 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1562.0 (TID 1041) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:43:15,032 [r for task 1042] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1562.0 (TID 1042). 810 bytes result sent to driver
   [druid] 2019-06-11 09:43:15,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1562.0 (TID 1042) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:43:15,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1562.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1562 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:43:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:43:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:43:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1563)
   [druid] 2019-06-11 09:43:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:43:15,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1563 (MapPartitionsRDD[1828] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:15,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_782 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:15,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_782_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:15,034 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_782_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:15,034 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 782 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:15,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1563 (MapPartitionsRDD[1828] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:43:15,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1563.0 with 1 tasks
   [druid] 2019-06-11 09:43:15,035 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1563.0 (TID 1043, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:15,035 [r for task 1043] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1563.0 (TID 1043)
   [druid] 2019-06-11 09:43:15,035 [r for task 1043] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:15,036 [r for task 1043] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:43:15,036 [r for task 1043] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:15,036 [r for task 1043] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:15,036 [r for task 1043] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1563.0 (TID 1043). 966 bytes result sent to driver
   [druid] 2019-06-11 09:43:15,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1563.0 (TID 1043) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:15,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1563.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:15,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1563 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:15,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 521 finished: print at Transform.scala:36, took 0.028826 s
   [druid] 2019-06-11 09:43:15,039 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:15,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 520 is 83 bytes
   [druid] 2019-06-11 09:43:15,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 521 is 160 bytes
   [druid] 2019-06-11 09:43:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 522 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1566 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1565, ShuffleMapStage 1564)
   [druid] 2019-06-11 09:43:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:43:15,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1566 (MapPartitionsRDD[1828] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:15,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_783 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:15,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_783_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:15,041 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_783_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:15,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 783 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:15,042 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1566 (MapPartitionsRDD[1828] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:43:15,042 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1566.0 with 1 tasks
   [druid] 2019-06-11 09:43:15,042 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1566.0 (TID 1044, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:15,042 [r for task 1044] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1566.0 (TID 1044)
   [druid] 2019-06-11 09:43:15,043 [r for task 1044] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:15,043 [r for task 1044] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:15,043 [r for task 1044] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:15,043 [r for task 1044] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:15,043 [r for task 1044] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1566.0 (TID 1044). 966 bytes result sent to driver
   [druid] 2019-06-11 09:43:15,043 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1566.0 (TID 1044) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:15,043 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1566.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:15,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1566 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:15,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 522 finished: print at Transform.scala:36, took 0.004290 s
   [druid] 2019-06-11 09:43:15,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217395000 ms.0 from job set of time 1560217395000 ms
   [druid] 2019-06-11 09:43:15,044 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.044 s for time 1560217395000 ms (execution: 0.038 s)
   [druid] 2019-06-11 09:43:15,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1821 from persistence list
   [druid] 2019-06-11 09:43:15,044 [-thread-pool-28] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1821
   [druid] 2019-06-11 09:43:15,044 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1816 from persistence list
   [druid] 2019-06-11 09:43:15,044 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1816
   [druid] 2019-06-11 09:43:15,044 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1815 from persistence list
   [druid] 2019-06-11 09:43:15,045 [-thread-pool-28] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1815
   [druid] 2019-06-11 09:43:15,045 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1815] at socketTextStream at Transform.scala:18 of time 1560217395000 ms
   [druid] 2019-06-11 09:43:15,045 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217385000 ms
   [druid] 2019-06-11 09:43:15,045 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217385000 ms
   [druid] 2019-06-11 09:43:20,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217400000 ms
   [druid] 2019-06-11 09:43:20,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217400000 ms.0 from job set of time 1560217400000 ms
   [druid] 2019-06-11 09:43:20,010 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1830 (map at Transform.scala:20)
   [druid] 2019-06-11 09:43:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:43:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 523 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1569 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1567, ShuffleMapStage 1568)
   [druid] 2019-06-11 09:43:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1568)
   [druid] 2019-06-11 09:43:20,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1568 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:43:20,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_784 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:20,012 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_784_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:43:20,012 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_784_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:20,012 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 784 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:20,012 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1568 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:43:20,013 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1568.0 with 2 tasks
   [druid] 2019-06-11 09:43:20,013 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1568.0 (TID 1045, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:20,013 [r for task 1045] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1568.0 (TID 1045)
   [druid] 2019-06-11 09:43:20,022 [r for task 1045] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1568.0 (TID 1045). 853 bytes result sent to driver
   [druid] 2019-06-11 09:43:20,022 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1568.0 (TID 1046, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:20,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1568.0 (TID 1045) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:43:20,022 [r for task 1046] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1568.0 (TID 1046)
   [druid] 2019-06-11 09:43:20,030 [r for task 1046] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1568.0 (TID 1046). 767 bytes result sent to driver
   [druid] 2019-06-11 09:43:20,030 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1568.0 (TID 1046) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:43:20,030 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1568.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1568 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:43:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:43:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:43:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1569)
   [druid] 2019-06-11 09:43:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:43:20,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1569 (MapPartitionsRDD[1835] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:20,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_785 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:20,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_785_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:20,031 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_785_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:20,032 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 785 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:20,032 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1569 (MapPartitionsRDD[1835] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:43:20,032 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1569.0 with 1 tasks
   [druid] 2019-06-11 09:43:20,032 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1569.0 (TID 1047, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:20,032 [r for task 1047] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1569.0 (TID 1047)
   [druid] 2019-06-11 09:43:20,033 [r for task 1047] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:20,033 [r for task 1047] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:20,033 [r for task 1047] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:20,033 [r for task 1047] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:20,033 [r for task 1047] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1569.0 (TID 1047). 966 bytes result sent to driver
   [druid] 2019-06-11 09:43:20,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1569.0 (TID 1047) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:20,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1569.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:20,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1569 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:43:20,034 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 523 finished: print at Transform.scala:36, took 0.023707 s
   [druid] 2019-06-11 09:43:20,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:20,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 522 is 83 bytes
   [druid] 2019-06-11 09:43:20,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 523 is 160 bytes
   [druid] 2019-06-11 09:43:20,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 524 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:20,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1572 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:20,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1570, ShuffleMapStage 1571)
   [druid] 2019-06-11 09:43:20,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:43:20,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1572 (MapPartitionsRDD[1835] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:20,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_786 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:20,038 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_786_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:20,038 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_786_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:20,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 786 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:20,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1572 (MapPartitionsRDD[1835] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:43:20,039 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1572.0 with 1 tasks
   [druid] 2019-06-11 09:43:20,039 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1572.0 (TID 1048, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:20,039 [r for task 1048] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1572.0 (TID 1048)
   [druid] 2019-06-11 09:43:20,040 [r for task 1048] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:20,040 [r for task 1048] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:20,040 [r for task 1048] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:20,040 [r for task 1048] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:20,040 [r for task 1048] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1572.0 (TID 1048). 966 bytes result sent to driver
   [druid] 2019-06-11 09:43:20,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1572.0 (TID 1048) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:20,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1572.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:20,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1572 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:20,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 524 finished: print at Transform.scala:36, took 0.005408 s
   [druid] 2019-06-11 09:43:20,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217400000 ms.0 from job set of time 1560217400000 ms
   [druid] 2019-06-11 09:43:20,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560217400000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:43:20,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1828 from persistence list
   [druid] 2019-06-11 09:43:20,041 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1828
   [druid] 2019-06-11 09:43:20,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1823 from persistence list
   [druid] 2019-06-11 09:43:20,042 [-thread-pool-28] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1823
   [druid] 2019-06-11 09:43:20,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1822 from persistence list
   [druid] 2019-06-11 09:43:20,042 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1822
   [druid] 2019-06-11 09:43:20,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1822] at socketTextStream at Transform.scala:18 of time 1560217400000 ms
   [druid] 2019-06-11 09:43:20,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217390000 ms
   [druid] 2019-06-11 09:43:20,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217390000 ms
   [druid] 2019-06-11 09:43:25,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217405000 ms
   [druid] 2019-06-11 09:43:25,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217405000 ms.0 from job set of time 1560217405000 ms
   [druid] 2019-06-11 09:43:25,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1837 (map at Transform.scala:20)
   [druid] 2019-06-11 09:43:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:43:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 525 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1575 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1573, ShuffleMapStage 1574)
   [druid] 2019-06-11 09:43:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1574)
   [druid] 2019-06-11 09:43:25,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1574 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:43:25,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_787 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:25,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_787_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:43:25,010 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_787_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 787 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:25,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1574 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:43:25,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1574.0 with 2 tasks
   [druid] 2019-06-11 09:43:25,010 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1574.0 (TID 1049, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:25,010 [r for task 1049] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1574.0 (TID 1049)
   [druid] 2019-06-11 09:43:25,019 [r for task 1049] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1574.0 (TID 1049). 853 bytes result sent to driver
   [druid] 2019-06-11 09:43:25,019 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1574.0 (TID 1050, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:25,020 [r for task 1050] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1574.0 (TID 1050)
   [druid] 2019-06-11 09:43:25,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1574.0 (TID 1049) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:43:25,033 [r for task 1050] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1574.0 (TID 1050). 853 bytes result sent to driver
   [druid] 2019-06-11 09:43:25,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1574.0 (TID 1050) in 14 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:43:25,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1574.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:25,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1574 (parallelize at Transform.scala:16) finished in 0.023 s
   [druid] 2019-06-11 09:43:25,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:43:25,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:43:25,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1575)
   [druid] 2019-06-11 09:43:25,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:43:25,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1575 (MapPartitionsRDD[1842] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:25,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_788 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:25,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_788_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:25,034 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_788_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,035 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 788 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:25,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1575 (MapPartitionsRDD[1842] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:43:25,035 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1575.0 with 1 tasks
   [druid] 2019-06-11 09:43:25,035 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1575.0 (TID 1051, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:25,035 [r for task 1051] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1575.0 (TID 1051)
   [druid] 2019-06-11 09:43:25,036 [r for task 1051] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:25,036 [r for task 1051] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:25,036 [r for task 1051] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:25,036 [r for task 1051] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:25,037 [r for task 1051] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1575.0 (TID 1051). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:43:25,037 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1575.0 (TID 1051) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:25,037 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1575.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:25,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1575 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:25,037 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 525 finished: print at Transform.scala:36, took 0.029913 s
   [druid] 2019-06-11 09:43:25,040 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:25,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 524 is 83 bytes
   [druid] 2019-06-11 09:43:25,040 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 525 is 160 bytes
   [druid] 2019-06-11 09:43:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 526 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1578 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1576, ShuffleMapStage 1577)
   [druid] 2019-06-11 09:43:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:43:25,040 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1578 (MapPartitionsRDD[1842] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:25,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_789 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:25,045 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 514
   [druid] 2019-06-11 09:43:25,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_789_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:25,046 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_789_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,046 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_788_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,046 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 789 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:25,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1578 (MapPartitionsRDD[1842] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:43:25,047 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1578.0 with 1 tasks
   [druid] 2019-06-11 09:43:25,047 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1578.0 (TID 1052, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:25,047 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_786_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,047 [r for task 1052] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1578.0 (TID 1052)
   [druid] 2019-06-11 09:43:25,048 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_784_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,048 [r for task 1052] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:25,048 [r for task 1052] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:25,048 [r for task 1052] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:25,049 [r for task 1052] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:43:25,049 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_779_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,050 [r for task 1052] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1578.0 (TID 1052). 966 bytes result sent to driver
   [druid] 2019-06-11 09:43:25,050 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_787_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,050 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1578.0 (TID 1052) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:25,050 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1578.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:25,050 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1578 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:43:25,050 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 526 finished: print at Transform.scala:36, took 0.010224 s
   [druid] 2019-06-11 09:43:25,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217405000 ms.0 from job set of time 1560217405000 ms
   [druid] 2019-06-11 09:43:25,050 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.050 s for time 1560217405000 ms (execution: 0.044 s)
   [druid] 2019-06-11 09:43:25,050 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1835 from persistence list
   [druid] 2019-06-11 09:43:25,051 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1835
   [druid] 2019-06-11 09:43:25,051 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1830 from persistence list
   [druid] 2019-06-11 09:43:25,051 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_778_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,051 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1830
   [druid] 2019-06-11 09:43:25,051 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1829 from persistence list
   [druid] 2019-06-11 09:43:25,051 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1829
   [druid] 2019-06-11 09:43:25,051 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1829] at socketTextStream at Transform.scala:18 of time 1560217405000 ms
   [druid] 2019-06-11 09:43:25,051 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217395000 ms
   [druid] 2019-06-11 09:43:25,051 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217395000 ms
   [druid] 2019-06-11 09:43:25,051 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_777_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,052 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_782_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,053 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_781_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 520
   [druid] 2019-06-11 09:43:25,053 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 516
   [druid] 2019-06-11 09:43:25,054 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_783_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,054 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_780_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 515
   [druid] 2019-06-11 09:43:25,055 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 517
   [druid] 2019-06-11 09:43:25,056 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_785_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:25,056 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 521
   [druid] 2019-06-11 09:43:25,056 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 519
   [druid] 2019-06-11 09:43:25,056 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 518
   [druid] 2019-06-11 09:43:30,008 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217410000 ms
   [druid] 2019-06-11 09:43:30,008 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217410000 ms.0 from job set of time 1560217410000 ms
   [druid] 2019-06-11 09:43:30,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:43:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1844 (map at Transform.scala:20)
   [druid] 2019-06-11 09:43:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 527 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1581 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1579, ShuffleMapStage 1580)
   [druid] 2019-06-11 09:43:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1579)
   [druid] 2019-06-11 09:43:30,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1579 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:43:30,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_790 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:30,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_790_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:43:30,010 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_790_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:30,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 790 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:30,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1579 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:43:30,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1579.0 with 2 tasks
   [druid] 2019-06-11 09:43:30,011 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1579.0 (TID 1053, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:30,011 [r for task 1053] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1579.0 (TID 1053)
   [druid] 2019-06-11 09:43:30,020 [r for task 1053] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1579.0 (TID 1053). 853 bytes result sent to driver
   [druid] 2019-06-11 09:43:30,020 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1579.0 (TID 1054, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:30,020 [r for task 1054] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1579.0 (TID 1054)
   [druid] 2019-06-11 09:43:30,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1579.0 (TID 1053) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:43:30,028 [r for task 1054] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1579.0 (TID 1054). 810 bytes result sent to driver
   [druid] 2019-06-11 09:43:30,029 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1579.0 (TID 1054) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:43:30,029 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1579.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1579 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:43:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:43:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:43:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1581)
   [druid] 2019-06-11 09:43:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:43:30,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1581 (MapPartitionsRDD[1849] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:30,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_791 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:30,030 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_791_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:30,030 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_791_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:30,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 791 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:30,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1581 (MapPartitionsRDD[1849] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:43:30,031 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1581.0 with 1 tasks
   [druid] 2019-06-11 09:43:30,031 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1581.0 (TID 1055, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:30,032 [r for task 1055] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1581.0 (TID 1055)
   [druid] 2019-06-11 09:43:30,032 [r for task 1055] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:30,032 [r for task 1055] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:30,032 [r for task 1055] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:30,032 [r for task 1055] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:30,033 [r for task 1055] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1581.0 (TID 1055). 966 bytes result sent to driver
   [druid] 2019-06-11 09:43:30,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1581.0 (TID 1055) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:30,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1581.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:30,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1581 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:30,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 527 finished: print at Transform.scala:36, took 0.024383 s
   [druid] 2019-06-11 09:43:30,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:30,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 527 is 160 bytes
   [druid] 2019-06-11 09:43:30,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 526 is 83 bytes
   [druid] 2019-06-11 09:43:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 528 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1584 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1582, ShuffleMapStage 1583)
   [druid] 2019-06-11 09:43:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:43:30,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1584 (MapPartitionsRDD[1849] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:30,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_792 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:30,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_792_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:30,037 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_792_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:30,038 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 792 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:30,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1584 (MapPartitionsRDD[1849] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:43:30,038 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1584.0 with 1 tasks
   [druid] 2019-06-11 09:43:30,038 [er-event-loop-2] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1584.0 (TID 1056, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:30,038 [r for task 1056] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1584.0 (TID 1056)
   [druid] 2019-06-11 09:43:30,039 [r for task 1056] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:30,039 [r for task 1056] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:30,039 [r for task 1056] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:30,039 [r for task 1056] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:30,041 [r for task 1056] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1584.0 (TID 1056). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:43:30,041 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1584.0 (TID 1056) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:30,041 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1584.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:30,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1584 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:43:30,041 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 528 finished: print at Transform.scala:36, took 0.005879 s
   [druid] 2019-06-11 09:43:30,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217410000 ms.0 from job set of time 1560217410000 ms
   [druid] 2019-06-11 09:43:30,041 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.041 s for time 1560217410000 ms (execution: 0.033 s)
   [druid] 2019-06-11 09:43:30,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1842 from persistence list
   [druid] 2019-06-11 09:43:30,041 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1842
   [druid] 2019-06-11 09:43:30,041 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1837 from persistence list
   [druid] 2019-06-11 09:43:30,042 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1837
   [druid] 2019-06-11 09:43:30,042 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1836 from persistence list
   [druid] 2019-06-11 09:43:30,042 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1836
   [druid] 2019-06-11 09:43:30,042 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1836] at socketTextStream at Transform.scala:18 of time 1560217410000 ms
   [druid] 2019-06-11 09:43:30,042 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217400000 ms
   [druid] 2019-06-11 09:43:30,042 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217400000 ms
   [druid] 2019-06-11 09:43:35,006 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217415000 ms
   [druid] 2019-06-11 09:43:35,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217415000 ms.0 from job set of time 1560217415000 ms
   [druid] 2019-06-11 09:43:35,008 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:43:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1851 (map at Transform.scala:20)
   [druid] 2019-06-11 09:43:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 529 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1587 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1585, ShuffleMapStage 1586)
   [druid] 2019-06-11 09:43:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1585)
   [druid] 2019-06-11 09:43:35,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1585 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:43:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_793 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:35,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_793_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:43:35,010 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_793_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:35,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 793 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:35,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1585 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:43:35,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1585.0 with 2 tasks
   [druid] 2019-06-11 09:43:35,010 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1585.0 (TID 1057, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:35,010 [r for task 1057] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1585.0 (TID 1057)
   [druid] 2019-06-11 09:43:35,020 [r for task 1057] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1585.0 (TID 1057). 853 bytes result sent to driver
   [druid] 2019-06-11 09:43:35,020 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1585.0 (TID 1058, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:35,020 [r for task 1058] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1585.0 (TID 1058)
   [druid] 2019-06-11 09:43:35,020 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1585.0 (TID 1057) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:43:35,028 [r for task 1058] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1585.0 (TID 1058). 767 bytes result sent to driver
   [druid] 2019-06-11 09:43:35,028 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1585.0 (TID 1058) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:43:35,028 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1585.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1585 (parallelize at Transform.scala:16) finished in 0.018 s
   [druid] 2019-06-11 09:43:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:43:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:43:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1587)
   [druid] 2019-06-11 09:43:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:43:35,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1587 (MapPartitionsRDD[1856] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:35,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_794 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:35,029 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_794_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:35,030 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_794_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:35,030 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 794 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:35,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1587 (MapPartitionsRDD[1856] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:43:35,030 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1587.0 with 1 tasks
   [druid] 2019-06-11 09:43:35,030 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1587.0 (TID 1059, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:35,030 [r for task 1059] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1587.0 (TID 1059)
   [druid] 2019-06-11 09:43:35,031 [r for task 1059] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:35,031 [r for task 1059] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:35,031 [r for task 1059] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:35,032 [r for task 1059] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:43:35,032 [r for task 1059] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1587.0 (TID 1059). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:43:35,033 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1587.0 (TID 1059) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:35,033 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1587.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:35,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1587 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:43:35,033 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 529 finished: print at Transform.scala:36, took 0.024930 s
   [druid] 2019-06-11 09:43:35,035 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:35,035 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 529 is 160 bytes
   [druid] 2019-06-11 09:43:35,036 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 528 is 83 bytes
   [druid] 2019-06-11 09:43:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 530 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1590 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1588, ShuffleMapStage 1589)
   [druid] 2019-06-11 09:43:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:43:35,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1590 (MapPartitionsRDD[1856] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:35,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_795 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:35,037 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_795_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:35,037 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_795_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:35,037 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 795 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:35,037 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1590 (MapPartitionsRDD[1856] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:43:35,037 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1590.0 with 1 tasks
   [druid] 2019-06-11 09:43:35,037 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1590.0 (TID 1060, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:35,038 [r for task 1060] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1590.0 (TID 1060)
   [druid] 2019-06-11 09:43:35,038 [r for task 1060] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:35,038 [r for task 1060] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:35,038 [r for task 1060] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:35,038 [r for task 1060] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:35,039 [r for task 1060] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1590.0 (TID 1060). 966 bytes result sent to driver
   [druid] 2019-06-11 09:43:35,039 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1590.0 (TID 1060) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:35,039 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1590.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:35,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1590 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:35,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 530 finished: print at Transform.scala:36, took 0.004015 s
   [druid] 2019-06-11 09:43:35,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217415000 ms.0 from job set of time 1560217415000 ms
   [druid] 2019-06-11 09:43:35,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.039 s for time 1560217415000 ms (execution: 0.033 s)
   [druid] 2019-06-11 09:43:35,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1849 from persistence list
   [druid] 2019-06-11 09:43:35,040 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1849
   [druid] 2019-06-11 09:43:35,040 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1844 from persistence list
   [druid] 2019-06-11 09:43:35,040 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1844
   [druid] 2019-06-11 09:43:35,040 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1843 from persistence list
   [druid] 2019-06-11 09:43:35,040 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1843
   [druid] 2019-06-11 09:43:35,040 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1843] at socketTextStream at Transform.scala:18 of time 1560217415000 ms
   [druid] 2019-06-11 09:43:35,040 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217405000 ms
   [druid] 2019-06-11 09:43:35,040 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217405000 ms
   [druid] 2019-06-11 09:43:40,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217420000 ms
   [druid] 2019-06-11 09:43:40,006 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217420000 ms.0 from job set of time 1560217420000 ms
   [druid] 2019-06-11 09:43:40,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:43:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1858 (map at Transform.scala:20)
   [druid] 2019-06-11 09:43:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 531 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1593 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1591, ShuffleMapStage 1592)
   [druid] 2019-06-11 09:43:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1591)
   [druid] 2019-06-11 09:43:40,008 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1591 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:43:40,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_796 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:40,009 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_796_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:43:40,010 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_796_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:40,010 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 796 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:40,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1591 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:43:40,010 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1591.0 with 2 tasks
   [druid] 2019-06-11 09:43:40,010 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1591.0 (TID 1061, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:40,010 [r for task 1061] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1591.0 (TID 1061)
   [druid] 2019-06-11 09:43:40,018 [r for task 1061] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1591.0 (TID 1061). 853 bytes result sent to driver
   [druid] 2019-06-11 09:43:40,019 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1591.0 (TID 1062, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:40,019 [r for task 1062] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1591.0 (TID 1062)
   [druid] 2019-06-11 09:43:40,019 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1591.0 (TID 1061) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:43:40,027 [r for task 1062] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1591.0 (TID 1062). 767 bytes result sent to driver
   [druid] 2019-06-11 09:43:40,027 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1591.0 (TID 1062) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:43:40,027 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1591.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:40,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1591 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:43:40,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:43:40,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:43:40,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1593)
   [druid] 2019-06-11 09:43:40,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:43:40,027 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1593 (MapPartitionsRDD[1863] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:40,027 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_797 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:40,028 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_797_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:40,028 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_797_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:40,028 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 797 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:40,029 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1593 (MapPartitionsRDD[1863] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:43:40,029 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1593.0 with 1 tasks
   [druid] 2019-06-11 09:43:40,029 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1593.0 (TID 1063, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:40,029 [r for task 1063] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1593.0 (TID 1063)
   [druid] 2019-06-11 09:43:40,029 [r for task 1063] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:40,030 [r for task 1063] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:43:40,030 [r for task 1063] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:40,030 [r for task 1063] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:40,030 [r for task 1063] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1593.0 (TID 1063). 966 bytes result sent to driver
   [druid] 2019-06-11 09:43:40,030 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1593.0 (TID 1063) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:40,030 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1593.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:40,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1593 (print at Transform.scala:36) finished in 0.001 s
   [druid] 2019-06-11 09:43:40,030 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 531 finished: print at Transform.scala:36, took 0.023101 s
   [druid] 2019-06-11 09:43:40,034 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:40,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 531 is 160 bytes
   [druid] 2019-06-11 09:43:40,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 530 is 83 bytes
   [druid] 2019-06-11 09:43:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 532 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1596 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1594, ShuffleMapStage 1595)
   [druid] 2019-06-11 09:43:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:43:40,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1596 (MapPartitionsRDD[1863] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:40,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_798 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:40,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_798_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:40,036 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_798_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:40,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 798 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:40,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1596 (MapPartitionsRDD[1863] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:43:40,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1596.0 with 1 tasks
   [druid] 2019-06-11 09:43:40,036 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1596.0 (TID 1064, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:40,037 [r for task 1064] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1596.0 (TID 1064)
   [druid] 2019-06-11 09:43:40,037 [r for task 1064] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:40,037 [r for task 1064] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:40,037 [r for task 1064] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:40,037 [r for task 1064] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:40,038 [r for task 1064] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1596.0 (TID 1064). 966 bytes result sent to driver
   [druid] 2019-06-11 09:43:40,038 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1596.0 (TID 1064) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:40,038 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1596.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:40,038 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1596 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:40,038 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 532 finished: print at Transform.scala:36, took 0.004557 s
   [druid] 2019-06-11 09:43:40,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217420000 ms.0 from job set of time 1560217420000 ms
   [druid] 2019-06-11 09:43:40,038 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.038 s for time 1560217420000 ms (execution: 0.032 s)
   [druid] 2019-06-11 09:43:40,038 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1856 from persistence list
   [druid] 2019-06-11 09:43:40,039 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1856
   [druid] 2019-06-11 09:43:40,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1851 from persistence list
   [druid] 2019-06-11 09:43:40,039 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1851
   [druid] 2019-06-11 09:43:40,039 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1850 from persistence list
   [druid] 2019-06-11 09:43:40,039 [c-thread-pool-1] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1850
   [druid] 2019-06-11 09:43:40,039 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1850] at socketTextStream at Transform.scala:18 of time 1560217420000 ms
   [druid] 2019-06-11 09:43:40,039 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217410000 ms
   [druid] 2019-06-11 09:43:40,039 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217410000 ms
   [druid] 2019-06-11 09:43:45,012 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217425000 ms
   [druid] 2019-06-11 09:43:45,012 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217425000 ms.0 from job set of time 1560217425000 ms
   [druid] 2019-06-11 09:43:45,014 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:45,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:43:45,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1865 (map at Transform.scala:20)
   [druid] 2019-06-11 09:43:45,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 533 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:45,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1599 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:45,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1598, ShuffleMapStage 1597)
   [druid] 2019-06-11 09:43:45,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1597)
   [druid] 2019-06-11 09:43:45,015 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1597 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:43:45,015 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_799 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:45,017 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_799_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:43:45,017 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_799_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:45,017 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 799 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:45,017 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1597 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:43:45,017 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1597.0 with 2 tasks
   [druid] 2019-06-11 09:43:45,018 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1597.0 (TID 1065, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:45,018 [r for task 1065] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1597.0 (TID 1065)
   [druid] 2019-06-11 09:43:45,026 [r for task 1065] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1597.0 (TID 1065). 767 bytes result sent to driver
   [druid] 2019-06-11 09:43:45,026 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1597.0 (TID 1066, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:45,026 [r for task 1066] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1597.0 (TID 1066)
   [druid] 2019-06-11 09:43:45,026 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1597.0 (TID 1065) in 8 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:43:45,034 [r for task 1066] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1597.0 (TID 1066). 853 bytes result sent to driver
   [druid] 2019-06-11 09:43:45,034 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1597.0 (TID 1066) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:43:45,034 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1597.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1597 (parallelize at Transform.scala:16) finished in 0.016 s
   [druid] 2019-06-11 09:43:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:43:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:43:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1599)
   [druid] 2019-06-11 09:43:45,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:43:45,035 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1599 (MapPartitionsRDD[1870] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:45,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_800 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:45,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_800_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:43:45,040 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_800_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:45,040 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_795_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:45,041 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 800 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:45,041 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1599 (MapPartitionsRDD[1870] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:43:45,041 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1599.0 with 1 tasks
   [druid] 2019-06-11 09:43:45,041 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1599.0 (TID 1067, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:45,041 [r for task 1067] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1599.0 (TID 1067)
   [druid] 2019-06-11 09:43:45,041 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_789_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:45,042 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 528
   [druid] 2019-06-11 09:43:45,042 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 526
   [druid] 2019-06-11 09:43:45,042 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 523
   [druid] 2019-06-11 09:43:45,042 [r for task 1067] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:45,042 [r for task 1067] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:45,043 [r for task 1067] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:45,043 [r for task 1067] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:45,043 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_793_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:45,043 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 525
   [druid] 2019-06-11 09:43:45,043 [r for task 1067] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1599.0 (TID 1067). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:43:45,044 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_794_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:45,044 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1599.0 (TID 1067) in 3 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:45,044 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1599.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:45,044 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 527
   [druid] 2019-06-11 09:43:45,044 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1599 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:43:45,044 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 533 finished: print at Transform.scala:36, took 0.029869 s
   [druid] 2019-06-11 09:43:45,044 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_798_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:45,045 [er-event-loop-3] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_792_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:45,046 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_790_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:45,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 524
   [druid] 2019-06-11 09:43:45,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 529
   [druid] 2019-06-11 09:43:45,047 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 522
   [druid] 2019-06-11 09:43:45,048 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_791_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:45,048 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_797_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:45,049 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_796_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:45,051 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:45,052 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 533 is 160 bytes
   [druid] 2019-06-11 09:43:45,052 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 532 is 83 bytes
   [druid] 2019-06-11 09:43:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 534 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1602 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1600, ShuffleMapStage 1601)
   [druid] 2019-06-11 09:43:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:43:45,052 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1602 (MapPartitionsRDD[1870] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:45,053 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_801 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:45,054 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_801_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:45,054 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_801_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:45,054 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 801 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:45,055 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1602 (MapPartitionsRDD[1870] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:43:45,055 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1602.0 with 1 tasks
   [druid] 2019-06-11 09:43:45,055 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1602.0 (TID 1068, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:45,055 [r for task 1068] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1602.0 (TID 1068)
   [druid] 2019-06-11 09:43:45,055 [r for task 1068] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:45,056 [r for task 1068] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 1 ms
   [druid] 2019-06-11 09:43:45,056 [r for task 1068] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:45,056 [r for task 1068] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:45,056 [r for task 1068] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1602.0 (TID 1068). 966 bytes result sent to driver
   [druid] 2019-06-11 09:43:45,056 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1602.0 (TID 1068) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:45,056 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1602.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:45,057 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1602 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:45,057 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 534 finished: print at Transform.scala:36, took 0.005608 s
   [druid] 2019-06-11 09:43:45,057 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217425000 ms.0 from job set of time 1560217425000 ms
   [druid] 2019-06-11 09:43:45,057 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.057 s for time 1560217425000 ms (execution: 0.045 s)
   [druid] 2019-06-11 09:43:45,057 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1863 from persistence list
   [druid] 2019-06-11 09:43:45,057 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1863
   [druid] 2019-06-11 09:43:45,057 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1858 from persistence list
   [druid] 2019-06-11 09:43:45,058 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1858
   [druid] 2019-06-11 09:43:45,058 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1857 from persistence list
   [druid] 2019-06-11 09:43:45,058 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1857
   [druid] 2019-06-11 09:43:45,058 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1857] at socketTextStream at Transform.scala:18 of time 1560217425000 ms
   [druid] 2019-06-11 09:43:45,058 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217415000 ms
   [druid] 2019-06-11 09:43:45,058 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217415000 ms
   [druid] 2019-06-11 09:43:50,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217430000 ms
   [druid] 2019-06-11 09:43:50,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217430000 ms.0 from job set of time 1560217430000 ms
   [druid] 2019-06-11 09:43:50,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1872 (map at Transform.scala:20)
   [druid] 2019-06-11 09:43:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:43:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 535 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1605 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1603, ShuffleMapStage 1604)
   [druid] 2019-06-11 09:43:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1604)
   [druid] 2019-06-11 09:43:50,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1604 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:43:50,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_802 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:50,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_802_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:43:50,011 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_802_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:50,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 802 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:50,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1604 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:43:50,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1604.0 with 2 tasks
   [druid] 2019-06-11 09:43:50,012 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1604.0 (TID 1069, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:50,012 [r for task 1069] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1604.0 (TID 1069)
   [druid] 2019-06-11 09:43:50,021 [r for task 1069] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1604.0 (TID 1069). 810 bytes result sent to driver
   [druid] 2019-06-11 09:43:50,022 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1604.0 (TID 1070, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:50,022 [r for task 1070] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1604.0 (TID 1070)
   [druid] 2019-06-11 09:43:50,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1604.0 (TID 1069) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:43:50,030 [r for task 1070] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1604.0 (TID 1070). 767 bytes result sent to driver
   [druid] 2019-06-11 09:43:50,031 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1604.0 (TID 1070) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:43:50,031 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1604.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1604 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:43:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:43:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:43:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1605)
   [druid] 2019-06-11 09:43:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:43:50,031 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1605 (MapPartitionsRDD[1877] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:50,031 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_803 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:50,033 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_803_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:50,033 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_803_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:50,033 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 803 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:50,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1605 (MapPartitionsRDD[1877] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:43:50,034 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1605.0 with 1 tasks
   [druid] 2019-06-11 09:43:50,034 [er-event-loop-4] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1605.0 (TID 1071, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:50,034 [r for task 1071] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1605.0 (TID 1071)
   [druid] 2019-06-11 09:43:50,035 [r for task 1071] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:50,035 [r for task 1071] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:50,035 [r for task 1071] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:50,035 [r for task 1071] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:50,036 [r for task 1071] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1605.0 (TID 1071). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:43:50,036 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1605.0 (TID 1071) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:50,036 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1605.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:50,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1605 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:50,036 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 535 finished: print at Transform.scala:36, took 0.027279 s
   [druid] 2019-06-11 09:43:50,038 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:50,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 534 is 83 bytes
   [druid] 2019-06-11 09:43:50,039 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 535 is 160 bytes
   [druid] 2019-06-11 09:43:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 536 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1608 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1606, ShuffleMapStage 1607)
   [druid] 2019-06-11 09:43:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:43:50,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1608 (MapPartitionsRDD[1877] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:50,040 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_804 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:50,041 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_804_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:50,042 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_804_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:50,042 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 804 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:50,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1608 (MapPartitionsRDD[1877] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:43:50,043 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1608.0 with 1 tasks
   [druid] 2019-06-11 09:43:50,043 [er-event-loop-6] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1608.0 (TID 1072, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:50,043 [r for task 1072] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1608.0 (TID 1072)
   [druid] 2019-06-11 09:43:50,044 [r for task 1072] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:50,044 [r for task 1072] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:50,044 [r for task 1072] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:50,044 [r for task 1072] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:50,045 [r for task 1072] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1608.0 (TID 1072). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:43:50,045 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1608.0 (TID 1072) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:50,045 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1608.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:50,045 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1608 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:50,045 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 536 finished: print at Transform.scala:36, took 0.006799 s
   [druid] 2019-06-11 09:43:50,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217430000 ms.0 from job set of time 1560217430000 ms
   [druid] 2019-06-11 09:43:50,046 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.046 s for time 1560217430000 ms (execution: 0.039 s)
   [druid] 2019-06-11 09:43:50,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1870 from persistence list
   [druid] 2019-06-11 09:43:50,046 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1870
   [druid] 2019-06-11 09:43:50,046 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1865 from persistence list
   [druid] 2019-06-11 09:43:50,046 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1865
   [druid] 2019-06-11 09:43:50,046 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1864 from persistence list
   [druid] 2019-06-11 09:43:50,046 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1864
   [druid] 2019-06-11 09:43:50,046 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1864] at socketTextStream at Transform.scala:18 of time 1560217430000 ms
   [druid] 2019-06-11 09:43:50,046 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217420000 ms
   [druid] 2019-06-11 09:43:50,046 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217420000 ms
   [druid] 2019-06-11 09:43:55,005 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217435000 ms
   [druid] 2019-06-11 09:43:55,005 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217435000 ms.0 from job set of time 1560217435000 ms
   [druid] 2019-06-11 09:43:55,007 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:43:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1879 (map at Transform.scala:20)
   [druid] 2019-06-11 09:43:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 537 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1611 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1609, ShuffleMapStage 1610)
   [druid] 2019-06-11 09:43:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1609)
   [druid] 2019-06-11 09:43:55,007 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1609 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:43:55,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_805 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:55,008 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_805_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:43:55,009 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_805_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:55,009 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 805 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:55,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1609 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:43:55,009 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1609.0 with 2 tasks
   [druid] 2019-06-11 09:43:55,009 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1609.0 (TID 1073, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:55,009 [r for task 1073] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1609.0 (TID 1073)
   [druid] 2019-06-11 09:43:55,017 [r for task 1073] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1609.0 (TID 1073). 853 bytes result sent to driver
   [druid] 2019-06-11 09:43:55,018 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1609.0 (TID 1074, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:43:55,018 [r for task 1074] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1609.0 (TID 1074)
   [druid] 2019-06-11 09:43:55,018 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1609.0 (TID 1073) in 9 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:43:55,025 [r for task 1074] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1609.0 (TID 1074). 767 bytes result sent to driver
   [druid] 2019-06-11 09:43:55,026 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1609.0 (TID 1074) in 9 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:43:55,026 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1609.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:55,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1609 (parallelize at Transform.scala:16) finished in 0.017 s
   [druid] 2019-06-11 09:43:55,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:43:55,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:43:55,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1611)
   [druid] 2019-06-11 09:43:55,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:43:55,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1611 (MapPartitionsRDD[1884] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:55,026 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_806 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:55,027 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_806_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:55,027 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_806_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:55,028 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 806 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:55,028 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1611 (MapPartitionsRDD[1884] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:43:55,028 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1611.0 with 1 tasks
   [druid] 2019-06-11 09:43:55,028 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1611.0 (TID 1075, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:55,028 [r for task 1075] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1611.0 (TID 1075)
   [druid] 2019-06-11 09:43:55,029 [r for task 1075] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:55,029 [r for task 1075] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:55,029 [r for task 1075] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:55,029 [r for task 1075] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:55,030 [r for task 1075] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1611.0 (TID 1075). 966 bytes result sent to driver
   [druid] 2019-06-11 09:43:55,030 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1611.0 (TID 1075) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:55,030 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1611.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:55,030 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1611 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:55,031 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 537 finished: print at Transform.scala:36, took 0.023515 s
   [druid] 2019-06-11 09:43:55,033 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:43:55,033 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 537 is 160 bytes
   [druid] 2019-06-11 09:43:55,034 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 536 is 83 bytes
   [druid] 2019-06-11 09:43:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 538 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:43:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1614 (print at Transform.scala:36)
   [druid] 2019-06-11 09:43:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1612, ShuffleMapStage 1613)
   [druid] 2019-06-11 09:43:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:43:55,034 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1614 (MapPartitionsRDD[1884] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:43:55,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_807 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:55,036 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_807_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:43:55,036 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_807_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:43:55,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 807 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:43:55,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1614 (MapPartitionsRDD[1884] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:43:55,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1614.0 with 1 tasks
   [druid] 2019-06-11 09:43:55,037 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1614.0 (TID 1076, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:43:55,037 [r for task 1076] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1614.0 (TID 1076)
   [druid] 2019-06-11 09:43:55,037 [r for task 1076] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:43:55,037 [r for task 1076] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:55,037 [r for task 1076] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:43:55,037 [r for task 1076] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:43:55,038 [r for task 1076] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1614.0 (TID 1076). 966 bytes result sent to driver
   [druid] 2019-06-11 09:43:55,038 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1614.0 (TID 1076) in 1 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:43:55,038 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1614.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:43:55,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1614 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:43:55,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 538 finished: print at Transform.scala:36, took 0.005649 s
   [druid] 2019-06-11 09:43:55,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217435000 ms.0 from job set of time 1560217435000 ms
   [druid] 2019-06-11 09:43:55,039 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.039 s for time 1560217435000 ms (execution: 0.034 s)
   [druid] 2019-06-11 09:43:55,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1877 from persistence list
   [druid] 2019-06-11 09:43:55,039 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1872 from persistence list
   [druid] 2019-06-11 09:43:55,039 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1877
   [druid] 2019-06-11 09:43:55,039 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1872
   [druid] 2019-06-11 09:43:55,039 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1871 from persistence list
   [druid] 2019-06-11 09:43:55,039 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1871
   [druid] 2019-06-11 09:43:55,039 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1871] at socketTextStream at Transform.scala:18 of time 1560217435000 ms
   [druid] 2019-06-11 09:43:55,040 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217425000 ms
   [druid] 2019-06-11 09:43:55,040 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217425000 ms
   [druid] 2019-06-11 09:44:00,007 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217440000 ms
   [druid] 2019-06-11 09:44:00,007 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217440000 ms.0 from job set of time 1560217440000 ms
   [druid] 2019-06-11 09:44:00,009 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:44:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1886 (map at Transform.scala:20)
   [druid] 2019-06-11 09:44:00,009 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:44:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 539 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:44:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1617 (print at Transform.scala:36)
   [druid] 2019-06-11 09:44:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1616, ShuffleMapStage 1615)
   [druid] 2019-06-11 09:44:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1616)
   [druid] 2019-06-11 09:44:00,010 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1616 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:44:00,010 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_808 stored as values in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:44:00,011 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_808_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.3 MB)
   [druid] 2019-06-11 09:44:00,011 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_808_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:00,011 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 808 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:44:00,011 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1616 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:44:00,011 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1616.0 with 2 tasks
   [druid] 2019-06-11 09:44:00,012 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1616.0 (TID 1077, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:44:00,012 [r for task 1077] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1616.0 (TID 1077)
   [druid] 2019-06-11 09:44:00,022 [r for task 1077] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1616.0 (TID 1077). 810 bytes result sent to driver
   [druid] 2019-06-11 09:44:00,022 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1616.0 (TID 1078, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:44:00,022 [r for task 1078] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1616.0 (TID 1078)
   [druid] 2019-06-11 09:44:00,022 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1616.0 (TID 1077) in 10 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:44:00,033 [r for task 1078] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1616.0 (TID 1078). 853 bytes result sent to driver
   [druid] 2019-06-11 09:44:00,033 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1616.0 (TID 1078) in 11 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:44:00,033 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1616.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:44:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1616 (parallelize at Transform.scala:16) finished in 0.021 s
   [druid] 2019-06-11 09:44:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:44:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:44:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1617)
   [druid] 2019-06-11 09:44:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:44:00,033 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1617 (MapPartitionsRDD[1891] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:44:00,034 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_809 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:44:00,035 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_809_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:44:00,035 [er-event-loop-2] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_809_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:00,036 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 809 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:44:00,036 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1617 (MapPartitionsRDD[1891] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:44:00,036 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1617.0 with 1 tasks
   [druid] 2019-06-11 09:44:00,036 [er-event-loop-3] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1617.0 (TID 1079, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:44:00,036 [r for task 1079] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1617.0 (TID 1079)
   [druid] 2019-06-11 09:44:00,037 [r for task 1079] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:44:00,037 [r for task 1079] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:44:00,037 [r for task 1079] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:44:00,037 [r for task 1079] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:44:00,038 [r for task 1079] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1617.0 (TID 1079). 1009 bytes result sent to driver
   [druid] 2019-06-11 09:44:00,038 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1617.0 (TID 1079) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:44:00,038 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1617.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:44:00,039 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1617 (print at Transform.scala:36) finished in 0.003 s
   [druid] 2019-06-11 09:44:00,039 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 539 finished: print at Transform.scala:36, took 0.030051 s
   [druid] 2019-06-11 09:44:00,042 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:44:00,042 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 538 is 83 bytes
   [druid] 2019-06-11 09:44:00,043 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 539 is 160 bytes
   [druid] 2019-06-11 09:44:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 540 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:44:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1620 (print at Transform.scala:36)
   [druid] 2019-06-11 09:44:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1618, ShuffleMapStage 1619)
   [druid] 2019-06-11 09:44:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:44:00,043 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1620 (MapPartitionsRDD[1891] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:44:00,044 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_810 stored as values in memory (estimated size 3.7 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:44:00,045 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_810_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.3 MB)
   [druid] 2019-06-11 09:44:00,045 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_810_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:00,045 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 810 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:44:00,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1620 (MapPartitionsRDD[1891] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:44:00,046 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1620.0 with 1 tasks
   [druid] 2019-06-11 09:44:00,046 [er-event-loop-7] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1620.0 (TID 1080, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:44:00,046 [r for task 1080] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1620.0 (TID 1080)
   [druid] 2019-06-11 09:44:00,047 [r for task 1080] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:44:00,047 [r for task 1080] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:44:00,047 [r for task 1080] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:44:00,047 [r for task 1080] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:44:00,048 [r for task 1080] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1620.0 (TID 1080). 1052 bytes result sent to driver
   [druid] 2019-06-11 09:44:00,048 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1620.0 (TID 1080) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:44:00,048 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1620.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:44:00,049 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1620 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:44:00,049 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 540 finished: print at Transform.scala:36, took 0.006583 s
   [druid] 2019-06-11 09:44:00,049 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217440000 ms.0 from job set of time 1560217440000 ms
   [druid] 2019-06-11 09:44:00,049 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.049 s for time 1560217440000 ms (execution: 0.042 s)
   [druid] 2019-06-11 09:44:00,049 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1884 from persistence list
   [druid] 2019-06-11 09:44:00,049 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1879 from persistence list
   [druid] 2019-06-11 09:44:00,049 [-thread-pool-26] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1884
   [druid] 2019-06-11 09:44:00,050 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1879
   [druid] 2019-06-11 09:44:00,050 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1878 from persistence list
   [druid] 2019-06-11 09:44:00,050 [-thread-pool-25] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1878
   [druid] 2019-06-11 09:44:00,050 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1878] at socketTextStream at Transform.scala:18 of time 1560217440000 ms
   [druid] 2019-06-11 09:44:00,050 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217430000 ms
   [druid] 2019-06-11 09:44:00,050 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217430000 ms
   [druid] 2019-06-11 09:44:05,017 [JobGenerator   ] INFO  reaming.scheduler.JobScheduler {1} - Added jobs for time 1560217445000 ms
   [druid] 2019-06-11 09:44:05,017 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 530
   [druid] 2019-06-11 09:44:05,017 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 533
   [druid] 2019-06-11 09:44:05,017 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Starting job streaming job 1560217445000 ms.0 from job set of time 1560217445000 ms
   [druid] 2019-06-11 09:44:05,017 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 531
   [druid] 2019-06-11 09:44:05,017 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 536
   [druid] 2019-06-11 09:44:05,018 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 537
   [druid] 2019-06-11 09:44:05,018 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_808_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,019 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 532
   [druid] 2019-06-11 09:44:05,020 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_805_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,022 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_802_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,023 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_801_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,023 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:44:05,023 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 534
   [druid] 2019-06-11 09:44:05,023 [Context Cleaner] INFO  rg.apache.spark.ContextCleaner {1} - Cleaned shuffle 535
   [druid] 2019-06-11 09:44:05,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 0 (parallelize at Transform.scala:16)
   [druid] 2019-06-11 09:44:05,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Registering RDD 1893 (map at Transform.scala:20)
   [druid] 2019-06-11 09:44:05,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 541 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:44:05,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1623 (print at Transform.scala:36)
   [druid] 2019-06-11 09:44:05,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1621, ShuffleMapStage 1622)
   [druid] 2019-06-11 09:44:05,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List(ShuffleMapStage 1621)
   [druid] 2019-06-11 09:44:05,023 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ShuffleMapStage 1621 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16), which has no missing parents
   [druid] 2019-06-11 09:44:05,024 [er-event-loop-5] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_809_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,024 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_811 stored as values in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:44:05,024 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_803_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,025 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_811_piece0 stored as bytes in memory (estimated size 1307.0 B, free 1989.4 MB)
   [druid] 2019-06-11 09:44:05,025 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_811_piece0 in memory on 192.168.91.1:63518 (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,026 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 811 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:44:05,026 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_799_piece0 on 192.168.91.1:63518 in memory (size: 1307.0 B, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,026 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 2 missing tasks from ShuffleMapStage 1621 (ParallelCollectionRDD[0] at parallelize at Transform.scala:16) (first 15 tasks are for partitions Vector(0, 1))
   [druid] 2019-06-11 09:44:05,026 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1621.0 with 2 tasks
   [druid] 2019-06-11 09:44:05,026 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_806_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,026 [er-event-loop-0] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1621.0 (TID 1081, localhost, executor driver, partition 0, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:44:05,026 [r for task 1081] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1621.0 (TID 1081)
   [druid] 2019-06-11 09:44:05,027 [er-event-loop-6] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_810_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,028 [er-event-loop-4] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_800_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,028 [er-event-loop-1] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_807_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,029 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Removed broadcast_804_piece0 on 192.168.91.1:63518 in memory (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,037 [r for task 1081] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1621.0 (TID 1081). 810 bytes result sent to driver
   [druid] 2019-06-11 09:44:05,037 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 1.0 in stage 1621.0 (TID 1082, localhost, executor driver, partition 1, PROCESS_LOCAL, 4911 bytes)
   [druid] 2019-06-11 09:44:05,037 [r for task 1082] INFO  apache.spark.executor.Executor {1} - Running task 1.0 in stage 1621.0 (TID 1082)
   [druid] 2019-06-11 09:44:05,037 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1621.0 (TID 1081) in 11 ms on localhost (executor driver) (1/2)
   [druid] 2019-06-11 09:44:05,045 [r for task 1082] INFO  apache.spark.executor.Executor {1} - Finished task 1.0 in stage 1621.0 (TID 1082). 810 bytes result sent to driver
   [druid] 2019-06-11 09:44:05,046 [result-getter-1] INFO  spark.scheduler.TaskSetManager {1} - Finished task 1.0 in stage 1621.0 (TID 1082) in 8 ms on localhost (executor driver) (2/2)
   [druid] 2019-06-11 09:44:05,046 [result-getter-1] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1621.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:44:05,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ShuffleMapStage 1621 (parallelize at Transform.scala:16) finished in 0.020 s
   [druid] 2019-06-11 09:44:05,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - looking for newly runnable stages
   [druid] 2019-06-11 09:44:05,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - running: Set(ResultStage 0)
   [druid] 2019-06-11 09:44:05,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - waiting: Set(ResultStage 1623)
   [druid] 2019-06-11 09:44:05,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - failed: Set()
   [druid] 2019-06-11 09:44:05,046 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1623 (MapPartitionsRDD[1898] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:44:05,046 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_812 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:44:05,048 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_812_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:44:05,048 [er-event-loop-0] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_812_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,048 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 812 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:44:05,048 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1623 (MapPartitionsRDD[1898] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(0))
   [druid] 2019-06-11 09:44:05,049 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1623.0 with 1 tasks
   [druid] 2019-06-11 09:44:05,049 [er-event-loop-1] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1623.0 (TID 1083, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:44:05,049 [r for task 1083] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1623.0 (TID 1083)
   [druid] 2019-06-11 09:44:05,049 [r for task 1083] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:44:05,049 [r for task 1083] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:44:05,050 [r for task 1083] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:44:05,050 [r for task 1083] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:44:05,050 [r for task 1083] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1623.0 (TID 1083). 966 bytes result sent to driver
   [druid] 2019-06-11 09:44:05,051 [result-getter-2] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1623.0 (TID 1083) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:44:05,051 [result-getter-2] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1623.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:44:05,051 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1623 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:44:05,051 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 541 finished: print at Transform.scala:36, took 0.027799 s
   [druid] 2019-06-11 09:44:05,054 [-job-executor-0] INFO  org.apache.spark.SparkContext  {1} - Starting job: print at Transform.scala:36
   [druid] 2019-06-11 09:44:05,054 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 541 is 160 bytes
   [druid] 2019-06-11 09:44:05,054 [uler-event-loop] INFO  e.spark.MapOutputTrackerMaster {1} - Size of output statuses for shuffle 540 is 83 bytes
   [druid] 2019-06-11 09:44:05,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Got job 542 (print at Transform.scala:36) with 1 output partitions
   [druid] 2019-06-11 09:44:05,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Final stage: ResultStage 1626 (print at Transform.scala:36)
   [druid] 2019-06-11 09:44:05,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Parents of final stage: List(ShuffleMapStage 1624, ShuffleMapStage 1625)
   [druid] 2019-06-11 09:44:05,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Missing parents: List()
   [druid] 2019-06-11 09:44:05,054 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting ResultStage 1626 (MapPartitionsRDD[1898] at map at Transform.scala:32), which has no missing parents
   [druid] 2019-06-11 09:44:05,055 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_813 stored as values in memory (estimated size 3.7 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:44:05,055 [uler-event-loop] INFO  ark.storage.memory.MemoryStore {1} - Block broadcast_813_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1989.4 MB)
   [druid] 2019-06-11 09:44:05,056 [er-event-loop-7] INFO  spark.storage.BlockManagerInfo {1} - Added broadcast_813_piece0 in memory on 192.168.91.1:63518 (size: 2.0 KB, free: 1989.5 MB)
   [druid] 2019-06-11 09:44:05,056 [uler-event-loop] INFO  org.apache.spark.SparkContext  {1} - Created broadcast 813 from broadcast at DAGScheduler.scala:1006
   [druid] 2019-06-11 09:44:05,056 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - Submitting 1 missing tasks from ResultStage 1626 (MapPartitionsRDD[1898] at map at Transform.scala:32) (first 15 tasks are for partitions Vector(1))
   [druid] 2019-06-11 09:44:05,056 [uler-event-loop] INFO  rk.scheduler.TaskSchedulerImpl {1} - Adding task set 1626.0 with 1 tasks
   [druid] 2019-06-11 09:44:05,056 [er-event-loop-5] INFO  spark.scheduler.TaskSetManager {1} - Starting task 0.0 in stage 1626.0 (TID 1084, localhost, executor driver, partition 1, PROCESS_LOCAL, 4684 bytes)
   [druid] 2019-06-11 09:44:05,056 [r for task 1084] INFO  apache.spark.executor.Executor {1} - Running task 0.0 in stage 1626.0 (TID 1084)
   [druid] 2019-06-11 09:44:05,057 [r for task 1084] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 0 non-empty blocks out of 0 blocks
   [druid] 2019-06-11 09:44:05,057 [r for task 1084] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:44:05,057 [r for task 1084] INFO  ge.ShuffleBlockFetcherIterator {1} - Getting 1 non-empty blocks out of 2 blocks
   [druid] 2019-06-11 09:44:05,057 [r for task 1084] INFO  ge.ShuffleBlockFetcherIterator {1} - Started 0 remote fetches in 0 ms
   [druid] 2019-06-11 09:44:05,057 [r for task 1084] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 1626.0 (TID 1084). 966 bytes result sent to driver
   [druid] 2019-06-11 09:44:05,058 [result-getter-3] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 1626.0 (TID 1084) in 2 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:44:05,058 [result-getter-3] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 1626.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:44:05,058 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 1626 (print at Transform.scala:36) finished in 0.002 s
   [druid] 2019-06-11 09:44:05,058 [-job-executor-0] INFO  e.spark.scheduler.DAGScheduler {1} - Job 542 finished: print at Transform.scala:36, took 0.004143 s
   [druid] 2019-06-11 09:44:05,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Finished job streaming job 1560217445000 ms.0 from job set of time 1560217445000 ms
   [druid] 2019-06-11 09:44:05,058 [JobScheduler   ] INFO  reaming.scheduler.JobScheduler {1} - Total delay: 0.058 s for time 1560217445000 ms (execution: 0.041 s)
   [druid] 2019-06-11 09:44:05,058 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1891 from persistence list
   [druid] 2019-06-11 09:44:05,058 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1891
   [druid] 2019-06-11 09:44:05,059 [JobGenerator   ] INFO  che.spark.rdd.MapPartitionsRDD {1} - Removing RDD 1886 from persistence list
   [druid] 2019-06-11 09:44:05,059 [-thread-pool-18] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1886
   [druid] 2019-06-11 09:44:05,059 [JobGenerator   ] INFO  org.apache.spark.rdd.BlockRDD  {1} - Removing RDD 1885 from persistence list
   [druid] 2019-06-11 09:44:05,059 [-thread-pool-27] INFO  che.spark.storage.BlockManager {1} - Removing RDD 1885
   [druid] 2019-06-11 09:44:05,059 [JobGenerator   ] INFO  ing.dstream.SocketInputDStream {1} - Removing blocks of RDD BlockRDD[1885] at socketTextStream at Transform.scala:18 of time 1560217445000 ms
   [druid] 2019-06-11 09:44:05,059 [JobGenerator   ] INFO  scheduler.ReceivedBlockTracker {1} - Deleting batches: 1560217435000 ms
   [druid] 2019-06-11 09:44:05,059 [JobGenerator   ] INFO  ing.scheduler.InputInfoTracker {1} - remove old batch metadata: 1560217435000 ms
   [druid] 2019-06-11 09:44:10,019 [JobScheduler   ] ERROR reaming.scheduler.JobScheduler {1} - Error generating jobs for time 1560217450000 ms
   java.io.IOException: Class not found
	at org.apache.xbean.asm5.ClassReader.a(Unknown Source)
	at org.apache.xbean.asm5.ClassReader.<init>(Unknown Source)
	at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:42)
	at org.apache.spark.util.ClosureCleaner$.getInnerClosureClasses(ClosureCleaner.scala:83)
	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:173)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:2287)
	at org.apache.spark.rdd.RDD$$anonfun$filter$1.apply(RDD.scala:387)
	at org.apache.spark.rdd.RDD$$anonfun$filter$1.apply(RDD.scala:386)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.filter(RDD.scala:386)
	at StreamDemo.Transform$$anonfun$2.apply(Transform.scala:27)
	at StreamDemo.Transform$$anonfun$2.apply(Transform.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$transform$1$$anonfun$apply$21.apply(DStream.scala:667)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$transform$1$$anonfun$apply$21.apply(DStream.scala:667)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$transform$2$$anonfun$5.apply(DStream.scala:681)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$transform$2$$anonfun$5.apply(DStream.scala:679)
	at org.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:46)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[druid] 2019-06-11 09:44:10,123 [Thread-1       ] INFO  ark.streaming.StreamingContext {1} - Invoking stop(stopGracefully=false) from shutdown hook
   [druid] 2019-06-11 09:44:10,129 [er-event-loop-0] INFO  ming.scheduler.ReceiverTracker {1} - Sent stop signal to all 1 receivers
   [druid] 2019-06-11 09:44:10,151 [er-event-loop-1] INFO  eceiver.ReceiverSupervisorImpl {1} - Received stop signal
   [druid] 2019-06-11 09:44:10,155 [er-event-loop-1] INFO  eceiver.ReceiverSupervisorImpl {1} - Stopping receiver with message: Stopped by driver: 
   [druid] 2019-06-11 09:44:10,156 [er-event-loop-1] INFO  reaming.dstream.SocketReceiver {1} - Closed socket to hadoop01:8989
   [druid] 2019-06-11 09:44:10,171 [er-event-loop-1] INFO  eceiver.ReceiverSupervisorImpl {1} - Called receiver onStop
   [druid] 2019-06-11 09:44:10,171 [Socket Receiver] WARN  reaming.dstream.SocketReceiver {1} - Error receiving data
   java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
	at java.net.SocketInputStream.read(SocketInputStream.java:171)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:121)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:119)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:72)
[druid] 2019-06-11 09:44:10,190 [er-event-loop-1] INFO  eceiver.ReceiverSupervisorImpl {1} - Deregistering receiver 0
   [druid] 2019-06-11 09:44:10,192 [rvisor-future-0] WARN  eceiver.ReceiverSupervisorImpl {1} - Restarting receiver with delay 2000 ms: Error receiving data
   java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
	at java.net.SocketInputStream.read(SocketInputStream.java:171)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:121)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:119)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:72)
[druid] 2019-06-11 09:44:10,193 [er-event-loop-6] ERROR ming.scheduler.ReceiverTracker {1} - Deregistered receiver for stream 0: Stopped by driver
   [druid] 2019-06-11 09:44:10,195 [er-event-loop-1] INFO  eceiver.ReceiverSupervisorImpl {1} - Stopped receiver 0
   [druid] 2019-06-11 09:44:10,195 [rvisor-future-0] INFO  eceiver.ReceiverSupervisorImpl {1} - Stopping receiver with message: Restarting receiver with delay 2000ms: Error receiving data: java.net.SocketException: Socket closed
   [druid] 2019-06-11 09:44:10,196 [rvisor-future-0] WARN  eceiver.ReceiverSupervisorImpl {1} - Receiver has been stopped
   [druid] 2019-06-11 09:44:10,200 [er-event-loop-1] INFO  eaming.receiver.BlockGenerator {1} - Stopping BlockGenerator
   [druid] 2019-06-11 09:44:10,402 [er-event-loop-1] INFO  .streaming.util.RecurringTimer {1} - Stopped timer for BlockGenerator after time 1560217450400
   [druid] 2019-06-11 09:44:10,402 [er-event-loop-1] INFO  eaming.receiver.BlockGenerator {1} - Waiting for block pushing thread to terminate
   [druid] 2019-06-11 09:44:10,408 [Thread-12      ] INFO  eaming.receiver.BlockGenerator {1} - Pushing out the last 0 blocks
   [druid] 2019-06-11 09:44:10,409 [Thread-12      ] INFO  eaming.receiver.BlockGenerator {1} - Stopped block pushing thread
   [druid] 2019-06-11 09:44:10,410 [er-event-loop-1] INFO  eaming.receiver.BlockGenerator {1} - Stopped BlockGenerator
   [druid] 2019-06-11 09:44:10,411 [rker for task 0] INFO  eceiver.ReceiverSupervisorImpl {1} - Stopped receiver without error
   [druid] 2019-06-11 09:44:10,412 [rker for task 0] INFO  apache.spark.executor.Executor {1} - Finished task 0.0 in stage 0.0 (TID 0). 751 bytes result sent to driver
   [druid] 2019-06-11 09:44:10,413 [result-getter-0] INFO  spark.scheduler.TaskSetManager {1} - Finished task 0.0 in stage 0.0 (TID 0) in 1356302 ms on localhost (executor driver) (1/1)
   [druid] 2019-06-11 09:44:10,413 [result-getter-0] INFO  rk.scheduler.TaskSchedulerImpl {1} - Removed TaskSet 0.0, whose tasks have all completed, from pool 
   [druid] 2019-06-11 09:44:10,413 [uler-event-loop] INFO  e.spark.scheduler.DAGScheduler {1} - ResultStage 0 (start at Transform.scala:37) finished in 1356.312 s
   [druid] 2019-06-11 09:44:10,563 [Thread-1       ] INFO  ming.scheduler.ReceiverTracker {1} - All of the receivers have deregistered successfully
   [druid] 2019-06-11 09:44:10,564 [Thread-1       ] INFO  ming.scheduler.ReceiverTracker {1} - ReceiverTracker stopped
   [druid] 2019-06-11 09:44:10,565 [Thread-1       ] INFO  reaming.scheduler.JobGenerator {1} - Stopping JobGenerator immediately
   [druid] 2019-06-11 09:44:10,565 [Thread-1       ] INFO  .streaming.util.RecurringTimer {1} - Stopped timer for JobGenerator after time 1560217450000
   [druid] 2019-06-11 09:44:10,566 [Thread-1       ] INFO  reaming.scheduler.JobGenerator {1} - Stopped JobGenerator
   [druid] 2019-06-11 09:44:10,568 [Thread-1       ] INFO  reaming.scheduler.JobScheduler {1} - Stopped JobScheduler
   [druid] 2019-06-11 09:44:10,573 [Thread-1       ] INFO  .server.handler.ContextHandler {1} - Stopped o.s.j.s.ServletContextHandler@5cd61783{/streaming,null,UNAVAILABLE,@Spark}
   [druid] 2019-06-11 09:44:10,574 [Thread-1       ] INFO  .server.handler.ContextHandler {1} - Stopped o.s.j.s.ServletContextHandler@769d513{/streaming/batch,null,UNAVAILABLE,@Spark}
   [druid] 2019-06-11 09:44:10,575 [Thread-1       ] INFO  .server.handler.ContextHandler {1} - Stopped o.s.j.s.ServletContextHandler@6be25526{/static/streaming,null,UNAVAILABLE,@Spark}
   [druid] 2019-06-11 09:44:10,576 [Thread-1       ] INFO  ark.streaming.StreamingContext {1} - StreamingContext stopped successfully
   [druid] 2019-06-11 09:44:10,577 [Thread-1       ] INFO  org.apache.spark.SparkContext  {1} - Invoking stop() from shutdown hook
   [druid] 2019-06-11 09:44:10,630 [Thread-1       ] INFO  jetty.server.AbstractConnector {1} - Stopped Spark@58e6d4b8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
   [druid] 2019-06-11 09:44:10,632 [Thread-1       ] INFO  org.apache.spark.ui.SparkUI    {1} - Stopped Spark web UI at http://192.168.91.1:4040
   [druid] 2019-06-11 09:44:10,642 [er-event-loop-6] INFO  MapOutputTrackerMasterEndpoint {1} - MapOutputTrackerMasterEndpoint stopped!
   [druid] 2019-06-11 09:44:11,047 [Thread-1       ] INFO  ark.storage.memory.MemoryStore {1} - MemoryStore cleared
   [druid] 2019-06-11 09:44:11,049 [Thread-1       ] INFO  che.spark.storage.BlockManager {1} - BlockManager stopped
   [druid] 2019-06-11 09:44:11,051 [Thread-1       ] INFO  ark.storage.BlockManagerMaster {1} - BlockManagerMaster stopped
   [druid] 2019-06-11 09:44:11,062 [er-event-loop-5] INFO  utputCommitCoordinatorEndpoint {1} - OutputCommitCoordinator stopped!
   [druid] 2019-06-11 09:44:11,067 [Thread-1       ] INFO  org.apache.spark.SparkContext  {1} - Successfully stopped SparkContext
   [druid] 2019-06-11 09:44:11,068 [Thread-1       ] INFO  spark.util.ShutdownHookManager {1} - Shutdown hook called
   [druid] 2019-06-11 09:44:11,068 [Thread-1       ] INFO  spark.util.ShutdownHookManager {1} - Deleting directory C:\Users\于羊\AppData\Local\Temp\spark-ff6fc0ad-cc9e-44ee-90d7-991d56a45fa6
   